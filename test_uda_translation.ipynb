{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /Users/lunli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Generator\n",
    "from peft import LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset, random_split, RandomSampler, TensorDataset\n",
    "# transformer\n",
    "from transformers.optimization import AdamW, get_scheduler, SchedulerType\n",
    "# native\n",
    "from NlpAnalytics import *\n",
    "\n",
    "PATH = '/Users/lunli/Library/CloudStorage/GoogleDrive-yaojn19880525@gmail.com/My Drive/Colab Notebooks/'\n",
    "DATASET_NAME = 'amazon'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load valid, test, unsup\n",
    "df_train = pd.read_csv(os.path.join(PATH, f'data/{DATASET_NAME}/sup_data_intent.csv'))\n",
    "df_valid = pd.read_csv(os.path.join(PATH, f'data/{DATASET_NAME}/test_intent.csv'))\n",
    "df_valid['intent']= df_valid['intent'].astype('category')\n",
    "df_valid['intent'] = df_valid['intent'].cat.codes\n",
    "# df_test = pd.read_csv(os.path.join(PATH, f'data/{DATASET_NAME}/amazon_test.csv'))\n",
    "df_unsup_1 = pd.read_csv(os.path.join(PATH, f'data/{DATASET_NAME}/unsup_data_intent_1.csv'))\n",
    "df_unsup_2 = pd.read_csv(os.path.join(PATH, f'data/{DATASET_NAME}/unsup_data_intent_2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ori_text</th>\n",
       "      <th>aug_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add the artist choclair to la mejor música de bso</td>\n",
       "      <td>'add artist to la mejor de bso'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rate this book four stars out of 6</td>\n",
       "      <td>'rate this book stars of'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add tune to punk español</td>\n",
       "      <td>'add music to'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>play deezer form 2010 tune by dave grohl</td>\n",
       "      <td>'Deezer shape air dave grohl'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i d like to go to a halal restaurant in twenty...</td>\n",
       "      <td>'I like to go to a halal restaurant in twenty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27270</th>\n",
       "      <td>play some music from the thirties</td>\n",
       "      <td>'a part of the music of the'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27271</th>\n",
       "      <td>can you get me seating for a party of 4</td>\n",
       "      <td>'can you me seating a party of'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27272</th>\n",
       "      <td>rate the previous textbook a 4 out of 6</td>\n",
       "      <td>'evaluate the previous manual has 4 out of 6'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27273</th>\n",
       "      <td>add this artist to the this is dirty projector...</td>\n",
       "      <td>'add this artist to the list of dirty project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27274</th>\n",
       "      <td>my sons and i want to dine at the water club i...</td>\n",
       "      <td>'My sons and I want to have dinner at the wat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ori_text  \\\n",
       "0      add the artist choclair to la mejor música de bso   \n",
       "1                     rate this book four stars out of 6   \n",
       "2                               add tune to punk español   \n",
       "3               play deezer form 2010 tune by dave grohl   \n",
       "4      i d like to go to a halal restaurant in twenty...   \n",
       "...                                                  ...   \n",
       "27270                  play some music from the thirties   \n",
       "27271            can you get me seating for a party of 4   \n",
       "27272            rate the previous textbook a 4 out of 6   \n",
       "27273  add this artist to the this is dirty projector...   \n",
       "27274  my sons and i want to dine at the water club i...   \n",
       "\n",
       "                                                aug_text  \n",
       "0                        'add artist to la mejor de bso'  \n",
       "1                              'rate this book stars of'  \n",
       "2                                         'add music to'  \n",
       "3                          'Deezer shape air dave grohl'  \n",
       "4      'I like to go to a halal restaurant in twenty ...  \n",
       "...                                                  ...  \n",
       "27270                       'a part of the music of the'  \n",
       "27271                    'can you me seating a party of'  \n",
       "27272      'evaluate the previous manual has 4 out of 6'  \n",
       "27273   'add this artist to the list of dirty project...  \n",
       "27274   'My sons and I want to have dinner at the wat...  \n",
       "\n",
       "[27275 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine unsup data\n",
    "df_unsup_1.columns = ['ori_text', 'aug_text_2']\n",
    "df_unsup = pd.concat([df_unsup_1, df_unsup_2])\n",
    "# drop duplicates\n",
    "df_unsup.drop_duplicates(inplace=True)\n",
    "# reset index and drop index column\n",
    "df_unsup.reset_index(inplace=True)\n",
    "df_unsup = df_unsup.drop('index', axis = 1)\n",
    "# rename cols\n",
    "df_unsup.columns = ['ori_text', 'aug_text']\n",
    "df_unsup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>add the lady bunny album to décadas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>add blag dahlia to pura vida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>add animal stories to maryanne s by per yngve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>can you add something by gregori chad petree t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>add this song onto hip hop gaming playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6</td>\n",
       "      <td>in the neighborhood find movie times for movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6</td>\n",
       "      <td>show me movie times at my local theater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6</td>\n",
       "      <td>what films are scheduled around here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6</td>\n",
       "      <td>what is the movie schedules at consolidated th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6</td>\n",
       "      <td>tell me what movies are at amc theatres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    intent                                               text\n",
       "0        0                add the lady bunny album to décadas\n",
       "1        0                       add blag dahlia to pura vida\n",
       "2        0  add animal stories to maryanne s by per yngve ...\n",
       "3        0  can you add something by gregori chad petree t...\n",
       "4        0         add this song onto hip hop gaming playlist\n",
       "..     ...                                                ...\n",
       "72       6    in the neighborhood find movie times for movies\n",
       "73       6            show me movie times at my local theater\n",
       "74       6               what films are scheduled around here\n",
       "75       6  what is the movie schedules at consolidated th...\n",
       "76       6            tell me what movies are at amc theatres\n",
       "\n",
       "[77 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop extra column in df_sup\n",
    "df_train = df_train.drop('level_1', axis = 1)\n",
    "df_train['intent']= df_train['intent'].astype('category')\n",
    "df_train['intent'] = df_train['intent'].cat.codes\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokennize sup and unsup toether\n",
    "sup_size = len(df_train)\n",
    "unsup_size = len(df_unsup)\n",
    "\n",
    "#\n",
    " ### Load tokenizer\n",
    "tokenizer = BertLoader(load_tokenizer=True).tokenizer\n",
    "sup_unsup = pd.DataFrame(df_train['text'].tolist() + df_unsup['ori_text'].tolist() + \n",
    "                        df_unsup['aug_text'].tolist())\n",
    "sup_unsup.columns = ['text']\n",
    "sup_unsup_  = DatasetNLP(input_df=sup_unsup, \n",
    "                    tokenizer=tokenizer,\n",
    "                    cols_to_tokenize=['text'],  \n",
    "                    cols_label=[] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spilt to up and unsup\n",
    "\n",
    "\n",
    "def split_sup_unsup(sup_unsup_, sup_size, unsup_size):\n",
    "    count = 0\n",
    "    input_ids_sup = []\n",
    "    mask_sup = []\n",
    "    input_ids_unsup_ori = []\n",
    "    mask_unsup_ori = []\n",
    "    input_ids_unsup_aug = []\n",
    "    mask_unsup_aug = []\n",
    "    for each in sup_unsup_:\n",
    "        if count < sup_size:\n",
    "            count += 1\n",
    "            input_ids_sup.append(each[0])\n",
    "            mask_sup.append(each[1])\n",
    "        elif sup_size <= count < unsup_size + sup_size:\n",
    "            count += 1\n",
    "            input_ids_unsup_ori.append(each[0])\n",
    "            mask_unsup_ori.append(each[1])\n",
    "        else:\n",
    "            input_ids_unsup_aug.append(each[0])\n",
    "            mask_unsup_aug.append(each[1])\n",
    "\n",
    "    return input_ids_sup, mask_sup, input_ids_unsup_ori, mask_unsup_ori, input_ids_unsup_aug, mask_unsup_aug\n",
    "\n",
    "input_ids_sup, mask_sup, input_ids_unsup_ori, mask_unsup_ori,input_ids_unsup_aug, mask_unsup_aug = split_sup_unsup(sup_unsup_, sup_size, unsup_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load tokenizer\n",
    "tokenizer = BertLoader(load_tokenizer=True).tokenizer\n",
    "\n",
    "# use DatasetNLP tokenize valid and test\n",
    "\n",
    "\n",
    "valid_data = DatasetNLP(input_df=df_valid, \n",
    "                    tokenizer=tokenizer,  \n",
    "                    cols_to_tokenize=['text'],  \n",
    "                    cols_label=['intent'] )\n",
    "# test_data = DatasetNLP(input_df=df_test, \n",
    "#                     tokenizer=tokenizer,  \n",
    "#                     cols_to_tokenize=['text'],  \n",
    "#                     cols_label=['label'] )\n",
    "\n",
    "# make new dataserNLP fpr sup and unsup:\n",
    "train_sup_data_ = TensorDataset(torch.stack(input_ids_sup), torch.stack(mask_sup), torch.LongTensor(df_train['intent'].tolist()))\n",
    "train_unsup_data = TensorDataset(torch.stack(input_ids_unsup_ori), torch.stack(mask_unsup_ori),\n",
    "                                 torch.stack(input_ids_unsup_aug), torch.stack(mask_unsup_aug))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_unsup_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To dataloader\n",
    "# to dataloader\n",
    "generator = Generator().manual_seed(42)\n",
    "train_sup_dataloader = DataLoader(train_sup_data_, sampler=RandomSampler(train_sup_data_, generator=generator), batch_size=8)\n",
    "train_unsup_dataloader = DataLoader(train_unsup_data, sampler=RandomSampler(train_unsup_data, generator=generator), batch_size=32)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=RandomSampler(valid_data, generator=generator), batch_size=32)\n",
    "test_dataloader = DataLoader(valid_data, sampler=RandomSampler(valid_data, generator=generator), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 100, the training loss is 0.24158695784397424.\n",
      "At step 200, the training loss is 0.1620826621935703.\n",
      "At step 300, the training loss is 0.12302737587752442.\n",
      "At step 400, the training loss is 0.09842588788131251.\n",
      "At step 500, the training loss is 0.08394889936130494.\n",
      "At step 600, the training loss is 0.07986806775831307.\n",
      "At step 700, the training loss is 0.08585280282489423.\n",
      "At step 800, the training loss is 0.08192687783681322.\n",
      "For epoch 1, the mean sup loss is: 0.07900058039041521, and accuracy is: 0.7657177448272705.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 1/3 [10:05<20:10, 605.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8942857384681702.\n",
      "\n",
      "At step 100, the training loss is 0.02577314996859059.\n",
      "At step 200, the training loss is 0.02551072683534585.\n",
      "At step 300, the training loss is 0.03127870161474372.\n",
      "At step 400, the training loss is 0.02820278826111462.\n",
      "At step 500, the training loss is 0.03176299004023895.\n",
      "At step 600, the training loss is 0.03705374830053188.\n",
      "At step 700, the training loss is 0.03809330270326297.\n",
      "At step 800, the training loss is 0.037112645038578196.\n",
      "For epoch 2, the mean sup loss is: 0.03657620057713673, and accuracy is: 0.982036828994751.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 2/3 [20:31<10:17, 617.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.9285714030265808.\n",
      "\n",
      "At step 100, the training loss is 0.02405446336604655.\n",
      "At step 200, the training loss is 0.023642566931666806.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 2/3 [23:31<11:45, 705.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m##### lora #####\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# lora_config = LoraConfig(task_type=TaskType.SEQ_CLS,target_modules=[\"query\", \"key\", \"value\"], r=1, lora_alpha=1, lora_dropout=0.1)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# optimizer = AdamNLP.newNLPAdam_LORA(loader.model, lora_config)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# model = optimizer.get_model_transformed()\u001b[39;00m\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TrainerUDA(model, datamodeler, loss_dict, optimizer)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSchedulerType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINVERSE_SQRT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/NLP-Proejct-Classification-main/NlpAnalytics/trainers/standard_trainer.py:101\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, schedule_type, num_warmup_steps, override_schedule, save_model_freq, save_loc, model_name)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# gather results and report if needed\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresMgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msup_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msup_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresMgr\u001b[38;5;241m.\u001b[39mreport(\u001b[38;5;28miter\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# summary training results\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/NLP-Proejct-Classification-main/NlpAnalytics/trainers/trainer_utilities.py:90\u001b[0m, in \u001b[0;36mResultsMgr.step\u001b[0;34m(self, model_output, labels, loss, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_output : torch\u001b[38;5;241m.\u001b[39mTensor, labels : torch\u001b[38;5;241m.\u001b[39mTensor, loss : Union[\u001b[38;5;28;01mNone\u001b[39;00m, torch\u001b[38;5;241m.\u001b[39mTensor], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# gather results for this batch\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# list of tensors\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreds\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mappend(labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# list of numbers\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "### load HF BERT Classifier\n",
    "num_labels = len(df_train['intent'].unique())\n",
    "loader = BertClassifierLoader(ClassifierType.BERT_CLASSIFIER_HF, \"bert-base-uncased\", num_labels, 0.1, load_tokenizer=True)\n",
    "\n",
    "datamodeler = {DataLoaderType.TRAINING: train_sup_dataloader,DataLoaderType.VALIDATION: valid_dataloader,\n",
    "            DataLoaderType.TESTING:test_dataloader, DataLoaderType.TRAINING_UNLABELED:train_unsup_dataloader}\n",
    "\n",
    "loss_sup = get_loss_functions(LossFuncType.CROSS_ENTROPY)\n",
    "loss_unsup = get_loss_functions(LossFuncType.KL_DIV)\n",
    "\n",
    "loss_dict = {'sup':loss_sup, 'unsup':loss_unsup}\n",
    "##### no lora ####\n",
    "optimizer = AdamNLP.newNLPAdam(loader.model, {'embeddings':True, 'encoder': 9}, lr = 2e-4)\n",
    "model = optimizer.get_model_transformed()\n",
    "##### lora #####\n",
    "# lora_config = LoraConfig(task_type=TaskType.SEQ_CLS,target_modules=[\"query\", \"key\", \"value\"], r=1, lora_alpha=1, lora_dropout=0.1)\n",
    "# optimizer = AdamNLP.newNLPAdam_LORA(loader.model, lora_config)\n",
    "# model = optimizer.get_model_transformed()\n",
    "\n",
    "trainer = TrainerUDA(model, datamodeler, loss_dict, optimizer)\n",
    "trainer.train(3, schedule_type = SchedulerType.INVERSE_SQRT, save_model_freq=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
