{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Generator\n",
    "from peft import LoraConfig, TaskType\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "# transformer\n",
    "from transformers.optimization import AdamW, get_scheduler, SchedulerType\n",
    "# native\n",
    "from NlpAnalytics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### about files\n",
    "file_path = os.path.join(get_root_path(), \"data\\gan_bert_data\")\n",
    "labeled_file = \"labeled.tsv\"\n",
    "unlabeled_file = \"unlabeled.tsv\"\n",
    "test_filename = \"test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### some utilities just for this stupid data set\n",
    "def get_qc_examples(input_file):\n",
    "  \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "  examples = []\n",
    "\n",
    "  with open(input_file, 'r') as f:\n",
    "      contents = f.read()\n",
    "      file_as_list = contents.splitlines()\n",
    "      for line in file_as_list[1:]:\n",
    "          split = line.split(\" \")\n",
    "          question = ' '.join(split[1:])\n",
    "\n",
    "          text_a = question\n",
    "          inn_split = split[0].split(\":\")\n",
    "          label = inn_split[0] + \"_\" + inn_split[1]\n",
    "          examples.append((text_a, label))\n",
    "      f.close()\n",
    "\n",
    "  return examples\n",
    "\n",
    "def stich_train_data(df_labeled, df_unlabeled):\n",
    "    examples = []\n",
    "    train_label_mask = np.ones(len(df_labeled), dtype=bool)\n",
    "    train_unlabel_masks = np.zeros(len(df_unlabeled), dtype=bool)\n",
    "    train_label_masks = np.concatenate([train_label_mask, train_unlabel_masks])\n",
    "    df_all = pd.concat([df_labeled, df_unlabeled], axis=0)\n",
    "    label_mask_rate = len(df_labeled) / len(df_all)\n",
    "    balance = int(1 / label_mask_rate)\n",
    "    balance = int(math.log(balance, 2)) # not sure why\n",
    "    for index, row in df_all.iterrows(): \n",
    "        if label_mask_rate == 1:\n",
    "            examples.append([row.text, row.label, train_label_masks[index]])\n",
    "        else:\n",
    "            if train_label_masks[index]:\n",
    "                if balance < 1:\n",
    "                    balance = 1\n",
    "                for b in range(0, int(balance)):\n",
    "                    examples.append([row.text, row.label, train_label_masks[index]])\n",
    "            else:\n",
    "                examples.append([row.text, row.label, train_label_masks[index]])\n",
    "    return pd.DataFrame(examples, columns = ['text', 'label', 'mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the examples\n",
    "df_labeled = pd.DataFrame(get_qc_examples(os.path.join(file_path, labeled_file)), columns=['text', 'label'])\n",
    "df_unlabeled = pd.DataFrame(get_qc_examples(os.path.join(file_path, unlabeled_file)), columns=['text', 'label'])\n",
    "df_test = pd.DataFrame(get_qc_examples(os.path.join(file_path, test_filename)), columns=['text', 'label'])\n",
    "# exclude labels that have not been seen in labeled examples\n",
    "label_space = set(df_labeled.label.unique())\n",
    "df_test = df_test[df_test.label.apply(lambda x: x in label_space)]\n",
    "# piece together labeled and unlabeled data\n",
    "df_train = stich_train_data(df_labeled, df_unlabeled)\n",
    "# label conversion\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_labeled.label.unique())\n",
    "df_train['label_new'] = encoder.fit_transform(df_train['label'])\n",
    "df_test['label_new'] = encoder.fit_transform(df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "### create data loader\n",
    "generator = Generator().manual_seed(42)\n",
    "tokenizer = BertLoader(load_tokenizer=True).tokenizer\n",
    "ds_train = DatasetNLP(input_df=df_train, tokenizer=tokenizer, cols_to_tokenize=['text'], cols_label=['label_new'], bool_col=['mask'])\n",
    "ds_test = DatasetNLP(input_df=df_test, tokenizer=tokenizer,  cols_to_tokenize=['text'], cols_label=['label_new'] )\n",
    "train_dataloader = DataLoader(ds_train, sampler=RandomSampler(ds_train, generator=generator), batch_size=64)\n",
    "valid_dataloader = DataLoader(ds_test, sampler=RandomSampler(ds_test, generator=generator), batch_size=64)\n",
    "data_modeler = {DataLoaderType.TRAINING: train_dataloader, DataLoaderType.VALIDATION: valid_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Jay\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#### model / optimizer\n",
    "loader = BertClassifierLoader(ClassifierType.BERT_CLASSIFIER, \"bert-base-uncased\", num_labels=len(label_space), dropout=0.1)\n",
    "optimizer = AdamNLP.newNLPAdam(loader.model, {'embeddings':False, 'encoder': None})\n",
    "model = optimizer.get_model_transformed()\n",
    "# extra package for GAN\n",
    "gen_pckage = GANPackage(num_labels=len(label_space), gen_lr=1e-4, disc_lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:55<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15860\\2465073644.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### TRainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainerDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_pckage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_modeler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschedule_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSchedulerType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONSTANT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Jay\\Desktop\\NLP-Proejct-Classification-main\\NlpAnalytics\\trainers\\standard_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, schedule_type, num_warmup_steps, override_schedule, save_model_freq, save_loc, model_name)\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msup_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msup_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcualte_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jay\\Desktop\\NLP-Proejct-Classification-main\\NlpAnalytics\\trainers\\domain_adpt_trainer.py\u001b[0m in \u001b[0;36mcalcualte_loss\u001b[1;34m(self, batch_idx, batch)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# the discriminator provides an output for both labeled and unlabeled real data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;31m# we skip unlabeld via masking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mlabel2one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mper_example_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel2one_hot\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": [
    "### TRainer\n",
    "trainer = TrainerDA(model, gen_pckage, data_modeler, optimizer)\n",
    "trainer.train(3, schedule_type = SchedulerType.CONSTANT, save_model_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the percentage of labeled examples  \n",
    "examples = []\n",
    "train_label_mask = np.ones(len(df_labeled), dtype=bool)\n",
    "train_unlabel_masks = np.zeros(len(df_unlabeled), dtype=bool)\n",
    "train_label_masks = np.concatenate([train_label_mask, train_unlabel_masks])\n",
    "df_all = pd.concat([df_labeled, df_unlabeled], axis=0)\n",
    "label_mask_rate = len(df_labeled) / len(df_all)\n",
    "balance = int(1 / label_mask_rate)\n",
    "balance = int(math.log(balance, 2))\n",
    "# if required it applies the balance\n",
    "for index, row in df_all.iterrows(): \n",
    "    if label_mask_rate == 1:\n",
    "        examples.append(([row.text, row.label_new], train_label_masks[index]))\n",
    "    else:\n",
    "        if train_label_masks[index]:\n",
    "            if balance < 1:\n",
    "                balance = 1\n",
    "            for b in range(0, int(balance)):\n",
    "                examples.append(([row.text, row.label_new], train_label_masks[index]))\n",
    "        else:\n",
    "            examples.append(([row.text, row.label_new], train_label_masks[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance = int(1 / label_mask_rate)\n",
    "balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_label_masks) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.803669724770643"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_unlabeled) / 545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6324"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text         How many pitchers occupy the shelf beside the ...\n",
      "label                                                NUM_count\n",
      "label_new                                                   23\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df_all.iterrows():\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
