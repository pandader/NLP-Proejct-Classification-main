{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# torch\n",
    "import torch\n",
    "from torch import Generator\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset, random_split, RandomSampler\n",
    "# native\n",
    "from NlpAnalytics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "### for reproducibility\n",
    "generator = Generator().manual_seed(42)\n",
    "### Load tokenizer\n",
    "tokenizer = BertLoader(load_tokenizer=True).tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: DataFrame [text | label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101, 13360,   102,     0]), tensor([1, 1, 1, 0]), tensor(0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['text_input_ids', 'text_attention_mask', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_1 = \\\n",
    "[\n",
    "    ['aaa', 0],\n",
    "    ['bbb', 1],\n",
    "    ['aa', 0]\n",
    "]\n",
    "df_1 = pd.DataFrame(table_1, columns = ['text', 'label'])\n",
    "dataset_1 = DatasetNLP(input_df=df_1, tokenizer=tokenizer, cols_label=['label'])\n",
    "# # re-generate everything\n",
    "# dataset_1.run_all(ret_token_type_ids=True)\n",
    "# __getitem__\n",
    "display(dataset_1[0])\n",
    "# wanna check desccriptions? and tensors ? \n",
    "all_tensors , desc = dataset_1.export_tensors_with_desc()\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: DataFrame [text 1 | text 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101, 13360,   102,     0]),\n",
       " tensor([1, 1, 1, 0]),\n",
       " tensor([  101, 22861,  2497,   102]),\n",
       " tensor([1, 1, 1, 1]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['text 1_input_ids',\n",
       " 'text 1_attention_mask',\n",
       " 'text 2_input_ids',\n",
       " 'text 2_attention_mask']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2 = \\\n",
    "[\n",
    "    ['aaa', 'bbb'],\n",
    "    ['bbb', 'ccc'],\n",
    "    ['ccc', 'aaa']\n",
    "]\n",
    "df_2 = pd.DataFrame(table_2, columns = ['text 1', 'text 2'])\n",
    "dataset_2 = DatasetNLP(input_df=df_2, tokenizer=tokenizer)\n",
    "# __getitem__\n",
    "display(dataset_2[0])\n",
    "# wanna check desccriptions? and tensors ? \n",
    "all_tensors , desc = dataset_2.export_tensors_with_desc()\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: DataFrame [text 1 | text 2 | text 3 | label 1 | label 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101, 13360,   102,     0]),\n",
       " tensor([1, 1, 1, 0]),\n",
       " tensor([  101, 22861,  2497,   102]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor(2),\n",
       " tensor(0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['text 1_input_ids',\n",
       " 'text 1_attention_mask',\n",
       " 'text 2_input_ids',\n",
       " 'text 2_attention_mask',\n",
       " 'label 1',\n",
       " 'label 2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### double labelling, useful for hierachical labels\n",
    "table_3 = \\\n",
    "[\n",
    "    ['aaa', 'bbb', 'dd', 2 , 0],\n",
    "    ['bbb', 'ccc', '33', 3, 1],\n",
    "    ['ccc', 'aaa', '44', 4, 0]\n",
    "]\n",
    "df_3 = pd.DataFrame(table_3, columns = ['text 1', 'text 2', 'text 3', 'label 1', 'label 2'])\n",
    "dataset_3 = DatasetNLP(input_df=df_3, \n",
    "                       tokenizer=tokenizer, \n",
    "                       cols_to_tokenize=['text 1', 'text 2'], \n",
    "                       cols_label=['label 1', 'label 2'] )\n",
    "# __getitem__\n",
    "display(dataset_3[0])\n",
    "# wanna check desccriptions? and tensors ? \n",
    "all_tensors , desc = dataset_3.export_tensors_with_desc()\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make TensorDataset out of DatasetNlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is useful say when the tokenization takes a long time, and you just want to do once.\n",
    "### Once you're happy, you can export the tensordata, you can just load the dataset as standard TensorDataset\n",
    "root_path = \"./NlpAnalytics/data/dummy_data/\"\n",
    "_ = dataset_3.export_as_tensordataset(\"dummy_tensordata\", root_path) # the return value will be the tensordataaset, if you want see it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it back as tensordataset\n",
    "dataset_3_tsdata = torch.load(os.path.join(root_path, \"dummy_tensordata.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 1037,  102,    0,    0,    0]),\n",
       " tensor([1, 1, 1, 0, 0, 0]),\n",
       " tensor([ 101, 1058, 2615, 2615, 2615,  102]),\n",
       " tensor([1, 1, 1, 1, 1, 1]),\n",
       " tensor(62),\n",
       " tensor(0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 1037,  102,    0,    0,    0]),\n",
       " tensor([1, 1, 1, 0, 0, 0]),\n",
       " tensor([ 101, 1058, 2615, 2615, 2615,  102]),\n",
       " tensor([1, 1, 1, 1, 1, 1]),\n",
       " tensor(62),\n",
       " tensor(0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### if you have multiple dataset, and merge some of them\n",
    "table_4 = \\\n",
    "[\n",
    "    ['aaa', 'bbb', 'dd', 2 , 0],\n",
    "    ['bbb', 'ccc', '33', 3, 1],\n",
    "    ['ccc', 'aaa', '44', 4, 0]\n",
    "]\n",
    "df_4 = pd.DataFrame(table_4, columns = ['text 1', 'text 2', 'text 3', 'label 1', 'label 2'])\n",
    "dataset_4 = DatasetNLP(input_df=df_3, tokenizer=tokenizer, cols_to_tokenize=['text 1', 'text 2'], cols_label=['label 1', 'label 2'])\n",
    "table_5 = \\\n",
    "[\n",
    "    ['a', 'vvvv', 'd', 62 , 0],\n",
    "    ['bbb', 'c', '33', 13, 1],\n",
    "    ['111', 'aaa', '22', 5, 0]\n",
    "]\n",
    "df_5 = pd.DataFrame(table_5, columns = ['text 1', 'text 2', 'text 3', 'label 1', 'label 2'])\n",
    "dataset_5 = DatasetNLP(input_df=df_5, tokenizer=tokenizer, cols_to_tokenize=['text 1', 'text 2'], cols_label=['label 1', 'label 2'])\n",
    "# merge (method 1)\n",
    "dataset_6_1 = ConcatDataset([dataset_4, dataset_5])\n",
    "# merge (method 2)\n",
    "dataset_6_2 = dataset_4 + dataset_5\n",
    "# validate if the same ?\n",
    "display(dataset_6_2[3])\n",
    "display(dataset_5[0])\n",
    "# Remark: the __getitem__ function of ConcatDataset will delegate to the constituent Dataset class, which is DatasetNLP. So as long as we have our desired implementation of __getitem__ function in DatasetNLP, we don't need to worry\n",
    "# the same applies to __len__, which basically calls datatset_5.__len__() + dataset_6.__len__()\n",
    "len(dataset_6_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset or random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101, 22861,  2497,   102]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([  101, 10507,  2278,   102]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor(3),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create a subset of an existing dataset\n",
    "dataset_7 = Subset(dataset_6_1, [1, 3, 5])\n",
    "dataset_7[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### random split of a dataset => Subset\n",
    "dataset_7_2_1, dataset_7_2_2 = random_split(dataset_6_2, [2, 4], generator=generator)\n",
    "dataset_7_3_1, dataset_7_3_2 = random_split(dataset_6_2, [0.3, 0.7], generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataloader = DataLoader(dataset_3, sampler=RandomSampler(dataset_3, generator=generator), batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
