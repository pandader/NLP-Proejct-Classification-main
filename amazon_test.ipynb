{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /Users/lunli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Generator\n",
    "from peft import LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset, random_split, RandomSampler\n",
    "# transformer\n",
    "from transformers.optimization import AdamW, get_scheduler, SchedulerType\n",
    "# native\n",
    "from NlpAnalytics import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df_train = pd.read_csv('./dataset/amazon_train.csv')\n",
    "df_valid = pd.read_csv('./dataset/amazon_valid.csv')\n",
    "df_test = pd.read_csv('./dataset/amazon_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the id,label_text columns\n",
    "df_train = df_train.drop(['id','label_text'], axis = 1)\n",
    "df_valid = df_valid.drop(['id','label_text'], axis = 1)\n",
    "df_test = df_test.drop(['id','label_text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Load tokenizer\n",
    "tokenizer = BertLoader(load_tokenizer=True).tokenizer\n",
    "\n",
    "df_train_ = DatasetNLP(input_df=df_train, \n",
    "                    tokenizer=tokenizer,\n",
    "                    cols_to_tokenize=['text'],  \n",
    "                    cols_label=['label'] )\n",
    "df_valid_ = DatasetNLP(input_df=df_valid, \n",
    "                    tokenizer=tokenizer,  \n",
    "                    cols_to_tokenize=['text'],  \n",
    "                    cols_label=['label'] )\n",
    "df_test_ = DatasetNLP(input_df=df_test, \n",
    "                    tokenizer=tokenizer,  \n",
    "                    cols_to_tokenize=['text'],  \n",
    "                    cols_label=['label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To dataloader\n",
    "\n",
    "generator = Generator().manual_seed(42)\n",
    "train_dataloader = DataLoader(df_train_, sampler=RandomSampler(df_train_, generator=generator), batch_size=32)\n",
    "valid_dataloader = DataLoader(df_valid_, sampler=RandomSampler(df_valid_, generator=generator), batch_size=32)\n",
    "test_dataloader = DataLoader(df_test_, sampler=RandomSampler(df_test_, generator=generator), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### trainer ####\n",
    "### load HF BERT Classifier\n",
    "num_labels = len(df_train['label'].unique())\n",
    "loader = BertClassifierLoader(ClassifierType.BERT_CLASSIFIER, \"bert-base-uncased\", num_labels, 0.1, load_tokenizer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 100, the training loss is 2.4227107644081114.\n",
      "At step 200, the training loss is 1.8437885788083077.\n",
      "At step 300, the training loss is 1.5516526286800703.\n",
      "For epoch 1, the mean sup loss is: 1.4600541857381661, and accuracy is: 0.6208094358444214.\n",
      "Validation accuracy is: 0.7840629816055298.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 1/5 [00:44<02:57, 44.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 100, the training loss is 0.743461574614048.\n",
      "At step 200, the training loss is 0.7599235337972641.\n",
      "At step 300, the training loss is 0.7798667089144389.\n",
      "For epoch 2, the mean sup loss is: 0.7667223841779762, and accuracy is: 0.793034553527832.\n",
      "Validation accuracy is: 0.812100350856781.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [01:29<02:13, 44.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 100, the training loss is 0.6850616621971131.\n",
      "At step 200, the training loss is 0.6757959394901991.\n",
      "At step 300, the training loss is 0.6902452560762564.\n",
      "For epoch 3, the mean sup loss is: 0.6890239578568274, and accuracy is: 0.8064095973968506.\n",
      "Validation accuracy is: 0.8278406262397766.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [02:12<01:28, 44.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 100, the training loss is 0.5648182702064514.\n",
      "At step 200, the training loss is 0.5726697836816311.\n",
      "At step 300, the training loss is 0.5934335879981518.\n",
      "For epoch 4, the mean sup loss is: 0.59676102163891, and accuracy is: 0.8343755602836609.\n",
      "Validation accuracy is: 0.8057058453559875.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [02:56<00:43, 43.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 100, the training loss is 0.5744256457686424.\n",
      "At step 200, the training loss is 0.5780070266872644.\n",
      "At step 300, the training loss is 0.5575636709233125.\n",
      "For epoch 5, the mean sup loss is: 0.5617036698179112, and accuracy is: 0.8449713587760925.\n",
      "Validation accuracy is: 0.8273487687110901.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [03:39<00:00, 43.94s/it]\n"
     ]
    }
   ],
   "source": [
    "datamodeler = {DataLoaderType.TRAINING: train_dataloader,DataLoaderType.VALIDATION: valid_dataloader,DataLoaderType.TESTING:test_dataloader}\n",
    "my_loss_func = get_loss_functions(LossFuncType.CROSS_ENTROPY)\n",
    "##### no lora ####\n",
    "optimizer = AdamNLP.newNLPAdam(loader.model, {'embeddings':True, 'encoder': 9}, lr = 0.0005)\n",
    "model = optimizer.get_model_transformed()\n",
    "##### lora #####\n",
    "# lora_config = LoraConfig(task_type=TaskType.SEQ_CLS,target_modules=[\"query\", \"key\", \"value\"], r=1, lora_alpha=1, lora_dropout=0.1)\n",
    "# optimizer = AdamNLP.newNLPAdam_LORA(loader.model, lora_config)\n",
    "# model = optimizer.get_model_transformed()\n",
    "\n",
    "trainer = Trainer(model, datamodeler, my_loss_func, optimizer)\n",
    "trainer.train(5, schedule_type = SchedulerType.CONSTANT, save_model_freq=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
