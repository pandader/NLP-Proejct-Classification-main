{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "# peft\n",
    "from peft import LoraConfig, TaskType\n",
    "# native\n",
    "from NlpAnalytics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### global vars\n",
    "USE_LORA = True\n",
    "ROOT_PATH = 'NlpAnalytics/data/dummy_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "df_train = pd.read_csv(os.path.join(f'{ROOT_PATH}/amazon_train.csv')).drop(['id', 'label_text'], axis=1)\n",
    "df_valid = pd.read_csv(os.path.join(f'{ROOT_PATH}/amazon_valid.csv')).drop(['id', 'label_text'], axis=1)\n",
    "df_test = pd.read_csv(os.path.join(f'{ROOT_PATH}/amazon_test.csv')).drop(['id', 'label_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### assemble data modeler\n",
    "tokenizer = BertLoader(load_tokenizer=True).tokenizer\n",
    "train_dataset = DatasetNLP(input_df=df_train, tokenizer=tokenizer, cols_to_tokenize=['text'], cols_label=['label'])\n",
    "valid_dataset = DatasetNLP(input_df=df_valid, tokenizer=tokenizer, cols_to_tokenize=['text'], cols_label=['label'])\n",
    "test_dataset = DatasetNLP(input_df=df_test, tokenizer=tokenizer, cols_to_tokenize=['text'], cols_label=['label'])\n",
    "datamodeler = {\n",
    "    DataLoaderType.TRAINING: DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32), \n",
    "    DataLoaderType.VALIDATION: DataLoader(valid_dataset, sampler=RandomSampler(valid_dataset), batch_size=32),\n",
    "    DataLoaderType.TESTING: DataLoader(test_dataset, batch_size=32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load model\n",
    "num_labels = len(df_train['label'].unique())\n",
    "loader = BertClassifierLoader(ClassifierType.BERT_CLASSIFIER, 'bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### loss func & optimizer\n",
    "loss_func = get_loss_functions(LossFuncType.CROSS_ENTROPY)\n",
    "if not USE_LORA:\n",
    "    optimizer = AdamNLP.newNLPAdam(loader.model, {'embeddings' : True, 'encoder' : 9}, lr=2e-4)\n",
    "else:\n",
    "    lora_config = LoraConfig(task_type=TaskType.SEQ_CLS, target_modules=['query', 'key', 'value'], r=1, lora_alpha=1., lora_dropout=0.1)\n",
    "    optimizer = AdamNLP.newNLPAdam_LORA(loader.model, lora_config)\n",
    "model = optimizer.get_model_transformed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 100, the training (sup)loss is 3.6342701649665834.\n",
      "At step 200, the training (sup)loss is 3.160478878617287.\n",
      "At step 300, the training (sup)loss is 2.7086252494653067.\n",
      "For epoch 1, the mean sup loss is: 2.5007002628511854, and accuracy is: 0.35591453313827515.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Epoch:  20%|██        | 1/5 [01:05<04:20, 65.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.7068371772766113.\n",
      "\n",
      "At step 100, the training (sup)loss is 1.1758556139469147.\n",
      "At step 200, the training (sup)loss is 1.0946638363599777.\n",
      "At step 300, the training (sup)loss is 1.0428383094072342.\n",
      "For epoch 2, the mean sup loss is: 1.0173466237054931, and accuracy is: 0.7344971299171448.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [02:10<03:15, 65.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8140679001808167.\n",
      "\n",
      "At step 100, the training (sup)loss is 0.779270369708538.\n",
      "At step 200, the training (sup)loss is 0.7670558834075928.\n",
      "At step 300, the training (sup)loss is 0.7602354889114697.\n",
      "For epoch 3, the mean sup loss is: 0.753211014635033, and accuracy is: 0.7978113889694214.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [03:15<02:10, 65.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.829316258430481.\n",
      "\n",
      "At step 100, the training (sup)loss is 0.6606304344534873.\n",
      "At step 200, the training (sup)loss is 0.6334922241419554.\n",
      "At step 300, the training (sup)loss is 0.6410610896845659.\n",
      "For epoch 4, the mean sup loss is: 0.6377354768001371, and accuracy is: 0.8277748823165894.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [04:20<01:04, 64.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.84456467628479.\n",
      "\n",
      "At step 100, the training (sup)loss is 0.548561694920063.\n",
      "At step 200, the training (sup)loss is 0.5736738024652004.\n",
      "At step 300, the training (sup)loss is 0.5663673825562.\n",
      "For epoch 5, the mean sup loss is: 0.5670469277434879, and accuracy is: 0.84392911195755.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [05:25<00:00, 65.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.84800785779953.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### start training\n",
    "trainer = Trainer(model, datamodeler, loss_func, optimizer)\n",
    "trainer.train(5, schedule_type=SchedulerType.CONSTANT, save_model_freq=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
