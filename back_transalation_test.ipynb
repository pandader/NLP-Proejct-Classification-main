{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /Users/lunli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import Optional\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List\n",
    "from torch import Generator\n",
    "from peft import LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset, random_split, RandomSampler\n",
    "# transformer\n",
    "from transformers.optimization import AdamW, get_scheduler, SchedulerType\n",
    "# native\n",
    "from NlpAnalytics import *\n",
    "\n",
    "MY_DEVICE = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "np.random.seed(1)\n",
    "PATH = '/Users/lunli/Library/CloudStorage/GoogleDrive-yaojn19880525@gmail.com/My Drive/Colab Notebooks/'\n",
    "DATASET_NAME = 'amazon'\n",
    "df_train = pd.read_csv(os.path.join(PATH, f'data/{DATASET_NAME}/amazon_train.csv'))\n",
    "df_train_ = pd.DataFrame(df_train.groupby('label')['text'].apply(lambda s: s.sample(4)))\n",
    "df_train_ = df_train_.reset_index()\n",
    "sup_index = list(df_train_['level_1'])\n",
    "df_train = df_train[~df_train.index.isin(sup_index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_.to_csv('sup_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load valid and test\n",
    "\n",
    "df_valid = pd.read_csv(os.path.join(PATH, f'data/{DATASET_NAME}/amazon_valid.csv'))\n",
    "df_test = pd.read_csv(os.path.join(PATH, f'data/{DATASET_NAME}/amazon_test.csv'))\n",
    "\n",
    "# remove the id,label_text columns\n",
    "df_train = df_train.drop(['id','label_text'], axis = 1)\n",
    "df_valid = df_valid.drop(['id','label_text'], axis = 1)\n",
    "df_test = df_test.drop(['id','label_text'], axis = 1)\n",
    "\n",
    " ### Load tokenizer\n",
    "tokenizer = BertLoader(load_tokenizer=True).tokenizer\n",
    "\n",
    "df_train_ = DatasetNLP(input_df=df_train, \n",
    "                    tokenizer=tokenizer,\n",
    "                    cols_to_tokenize=['text'],  \n",
    "                    cols_label=['label'] )\n",
    "df_valid_ = DatasetNLP(input_df=df_valid, \n",
    "                    tokenizer=tokenizer,  \n",
    "                    cols_to_tokenize=['text'],  \n",
    "                    cols_label=['label'] )\n",
    "df_test_ = DatasetNLP(input_df=df_test, \n",
    "                    tokenizer=tokenizer,  \n",
    "                    cols_to_tokenize=['text'],  \n",
    "                    cols_label=['label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the first model\n",
    "first_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "second_model_name = 'Helsinki-NLP/opus-mt-fr-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "### en -> fr\n",
    "# Get the tokenizer\n",
    "first_model_tkn = MarianTokenizer.from_pretrained(first_model_name)\n",
    "# Load the pretrained model based on the name\n",
    "first_model = MarianMTModel.from_pretrained(first_model_name)\n",
    "### fr -> en\n",
    "# Get the tokenizer\n",
    "second_model_tkn = MarianTokenizer.from_pretrained(second_model_name)\n",
    "# Load the pretrained model based on the name\n",
    "second_model = MarianMTModel.from_pretrained(second_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4061: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ### sample text\n",
    "# original_texts = [\n",
    "#     \"This article aims to perform the back translation for text data augmentation\",\n",
    "#     \"It is the 25th article by Zoumana on Medium. He loves to give back to the community\",\n",
    "#     \"The first model translates from English to French, which is a temporary process\",\n",
    "#     \"The second model finally translates back all the temporary french text into English\"\n",
    "# ]\n",
    "\n",
    "def chunks(text, chunk_size):\n",
    "    res = []\n",
    "    total_length = len(text)\n",
    "    for i in range(0, total_length, chunk_size):\n",
    "        tmp = text[i:i+chunk_size]\n",
    "        res.append(tmp)\n",
    "    return res\n",
    "\n",
    "# Generate translation using model\n",
    "train_text = list(df_train['text'].values)[:11000]\n",
    "translated = []\n",
    "for src_text in chunks(train_text,10):\n",
    "    batch = first_model_tkn.prepare_seq2seq_batch(src_text,return_tensors=\"pt\").to(MY_DEVICE)\n",
    "    generated = first_model.generate(**batch)\n",
    "    translation:List[str] = first_model_tkn.batch_decode(generated, skip_special_tokens=True)\n",
    "    translated.extend(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translated back\n",
    "\n",
    "translated_back = []\n",
    "for text in chunks(translated,10):\n",
    "    batch_ = second_model_tkn.prepare_seq2seq_batch(text,return_tensors=\"pt\").to(MY_DEVICE)\n",
    "    generated_back = second_model.generate(**batch_)\n",
    "    translation_back:List[str] = second_model_tkn.batch_decode(generated_back, skip_special_tokens=True)\n",
    "    translated_back.extend(translation_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_back = list(map(str.lower,translated_back))\n",
    "# generated augmentation data\n",
    "\n",
    "# def combine_texts(original_texts, back_translated_batch):\n",
    "#     return set(original_texts + back_translated_batch)\n",
    "\n",
    "train_text = [x.strip(' ') for x in train_text]\n",
    "\n",
    "# final_augmented = combine_texts(train_text, translated_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ori_text</th>\n",
       "      <th>aug_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wake me up at nine am on friday</td>\n",
       "      <td>wake me up at 9:00 a.m. on friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>olly quiet</td>\n",
       "      <td>it's very quiet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stop</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olly pause for ten seconds</td>\n",
       "      <td>full break for 10 seconds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>check my email for new emails during the last ...</td>\n",
       "      <td>check my email for new emails during the last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>send the following email to my sister</td>\n",
       "      <td>send the following email to my sister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10956</th>\n",
       "      <td>did i get an email from mike</td>\n",
       "      <td>did i get an email from mike?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957</th>\n",
       "      <td>has mike sent me an email</td>\n",
       "      <td>mike sent me an e-mail.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10958</th>\n",
       "      <td>hi do i have any new emails</td>\n",
       "      <td>hello, i have new e-mails</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10959 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ori_text  \\\n",
       "0                        wake me up at nine am on friday   \n",
       "1                    set an alarm for two hours from now   \n",
       "2                                             olly quiet   \n",
       "3                                                   stop   \n",
       "4                             olly pause for ten seconds   \n",
       "...                                                  ...   \n",
       "10954  check my email for new emails during the last ...   \n",
       "10955              send the following email to my sister   \n",
       "10956                       did i get an email from mike   \n",
       "10957                          has mike sent me an email   \n",
       "10958                        hi do i have any new emails   \n",
       "\n",
       "                                                aug_text  \n",
       "0                      wake me up at 9:00 a.m. on friday  \n",
       "1                    set an alarm for two hours from now  \n",
       "2                                       it's very quiet.  \n",
       "3                                                   stop  \n",
       "4                              full break for 10 seconds  \n",
       "...                                                  ...  \n",
       "10954  check my email for new emails during the last ...  \n",
       "10955              send the following email to my sister  \n",
       "10956                      did i get an email from mike?  \n",
       "10957                            mike sent me an e-mail.  \n",
       "10958                          hello, i have new e-mails  \n",
       "\n",
       "[10959 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train = dict(zip(train_text, translated_back))\n",
    "final_train = pd.DataFrame(final_train.items())\n",
    "final_train.columns = ['ori_text', 'aug_text']\n",
    "final_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train.to_csv('final_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_translation(\n",
    "        batch_texts : list, \n",
    "        model : MarianMTModel, \n",
    "        tokenizer : MarianTokenizer, \n",
    "        language : Optional[str]=\"fr\",\n",
    "        max_new_tokens : Optional[int]=128, \n",
    "        do_sample : Optional[bool]=True, \n",
    "        top_k : Optional[int]=50, \n",
    "        top_p : Optional[float]=0.9, \n",
    "        temperature : Optional[float]=0.9):\n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True),\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        do_sample=do_sample, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        temperature=temperature)\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]    \n",
    "    return translated_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_translation_beam(\n",
    "        batch_texts : list, \n",
    "        model : MarianMTModel, \n",
    "        tokenizer : MarianTokenizer, \n",
    "        language : Optional[str]=\"fr\",\n",
    "        max_new_tokens : Optional[int]=128):\n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True),\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        do_sample=False,\n",
    "        num_beam_groups=2,\n",
    "        num_beams=4,\n",
    "        diversity_penalty=100000.,\n",
    "        num_return_sequences=2)\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = [tokenizer.decode(translated[i], skip_special_tokens=True) for i in np.arange(1, len(translated), 2)]\n",
    "    return translated_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model translation from the original language (English) to French\n",
    "translated_texts = perform_translation_beam(original_texts, first_model, first_model_tkn)\n",
    "# Perform the translation back to English\n",
    "back_translated_texts  = perform_translation_beam(translated_texts, second_model, second_model_tkn)\n",
    "# print\n",
    "print('---------------------------------------------')\n",
    "for org, new in zip(original_texts, back_translated_texts):\n",
    "    print(org + '\\n')\n",
    "    print(new)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
