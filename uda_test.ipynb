{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# torch\n",
    "import torch\n",
    "from torch import Generator\n",
    "# local\n",
    "from uda_data import *\n",
    "from uda_model import *\n",
    "from uda_optimizer import *\n",
    "from uda_trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('mps')\n",
    "PATH = '/Users/lunli/Library/CloudStorage/GoogleDrive-yaojn19880525@gmail.com/My Drive/Colab Notebooks/'\n",
    "DATASET_NAME = 'uda_imdb_data_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load supervised and unsupervised data\n",
    "# sup\n",
    "train_sup_data = torch.load(os.path.join(PATH, f'data/{DATASET_NAME}/train_sup_data.pt'))\n",
    "# unsup\n",
    "train_unsup_data = torch.load(os.path.join(PATH, f'data/{DATASET_NAME}/train_unsup_data.pt'))\n",
    "train_unsup_data = TensorDataset(\n",
    "torch.cat([train_unsup_data.tensors[0][:20000], train_unsup_data.tensors[0][-20000:]]),\n",
    "torch.cat([train_unsup_data.tensors[1][:20000], train_unsup_data.tensors[1][-20000:]]),\n",
    "torch.cat([train_unsup_data.tensors[2][:20000], train_unsup_data.tensors[2][-20000:]]),\n",
    "torch.cat([train_unsup_data.tensors[3][:20000], train_unsup_data.tensors[3][-20000:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compile data loader\n",
    "# valid\n",
    "valid_data = torch.load(os.path.join(PATH, f'data/{DATASET_NAME}/val_data.pt'))\n",
    "# test\n",
    "test_data = torch.load(os.path.join(PATH, f'data/{DATASET_NAME}/test_data.pt'))\n",
    "# to dataloader\n",
    "generator = Generator().manual_seed(42)\n",
    "train_sup_dataloader = DataLoader(train_sup_data, sampler=RandomSampler(train_sup_data, generator=generator), batch_size=8)\n",
    "train_unsup_dataloader = DataLoader(train_unsup_data, sampler=RandomSampler(train_unsup_data, generator=generator), batch_size=24)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=RandomSampler(valid_data, generator=generator), batch_size=16)\n",
    "# organize the container\n",
    "datamodeler = {\n",
    "    DataLoaderType.TRAINING: train_sup_dataloader,\n",
    "    DataLoaderType.VALIDATION: valid_dataloader,\n",
    "    DataLoaderType.TRAINING_UNLABELED: train_unsup_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### set up neccessities for training\n",
    "# load model and tokenizer\n",
    "tokenizer, model = load_bert_model(\"bert-base-uncased\", num_labels=2, device=DEVICE)\n",
    "# load loss function for sup/unsup\n",
    "loss_sup = get_loss_functions(LossFuncType.CROSS_ENTROPY, reduce='none')\n",
    "loss_unsup = get_loss_functions(LossFuncType.KL_DIV, reduce='none')\n",
    "loss_dict = {'sup':loss_sup, 'unsup':loss_unsup}\n",
    "    \n",
    "# optimizer set up\n",
    "USE_LORA = False\n",
    "if not USE_LORA:\n",
    "    optimizer = AdamNLP.newNLPAdam(model, {'embeddings':True, 'encoder': 9}, lr = 2e-5)\n",
    "    model = optimizer.get_model_transformed()\n",
    "else:\n",
    "    lora_config = LoraConfig(task_type=TaskType.SEQ_CLS, target_modules=[\"query\", \"key\", \"value\"], r=1, lora_alpha=1, lora_dropout=0.1)\n",
    "    optimizer = AdamNLP.newNLPAdam_LORA(model, lora_config)\n",
    "    model = optimizer.get_model_transformed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 2, the training (sup)loss is 0.7823353409767151, the training unsup-loss is 0.010606745723634958.\n",
      "At step 4, the training (sup)loss is 0.7850031703710556, the training unsup-loss is 0.011880077654495835.\n",
      "At step 6, the training (sup)loss is 0.7866012851397196, the training unsup-loss is 0.012018722171584765.\n",
      "At step 8, the training (sup)loss is 0.7721334546804428, the training unsup-loss is 0.01082521805074066.\n",
      "At step 10, the training (sup)loss is 0.7727465212345124, the training unsup-loss is 0.010313795367255806.\n",
      "At step 12, the training (sup)loss is 0.7720537384351095, the training unsup-loss is 0.009726998900684217.\n",
      "At step 14, the training (sup)loss is 0.7719060012272426, the training unsup-loss is 0.009816831277151192.\n",
      "At step 16, the training (sup)loss is 0.7706480100750923, the training unsup-loss is 0.010173917369684204.\n",
      "At step 18, the training (sup)loss is 0.7268674870332082, the training unsup-loss is 0.009802090288657282.\n",
      "At step 20, the training (sup)loss is 0.7305454522371292, the training unsup-loss is 0.009754072618670761.\n",
      "At step 22, the training (sup)loss is 0.7325301224535162, the training unsup-loss is 0.010035148165612058.\n",
      "At step 24, the training (sup)loss is 0.7337066829204559, the training unsup-loss is 0.010035955385925869.\n",
      "At step 26, the training (sup)loss is 0.7326329992367671, the training unsup-loss is 0.010150948288635565.\n",
      "At step 28, the training (sup)loss is 0.7068943466459002, the training unsup-loss is 0.010101309096041535.\n",
      "At step 30, the training (sup)loss is 0.7066550751527151, the training unsup-loss is 0.01012402786873281.\n",
      "At step 32, the training (sup)loss is 0.7125281319022179, the training unsup-loss is 0.010036063395091332.\n",
      "At step 34, the training (sup)loss is 0.7140707724234637, the training unsup-loss is 0.009911365875536027.\n",
      "At step 36, the training (sup)loss is 0.6744001739554935, the training unsup-loss is 0.009903547237627208.\n",
      "At step 38, the training (sup)loss is 0.6580271971853155, the training unsup-loss is 0.01004111255813194.\n",
      "At step 40, the training (sup)loss is 0.6610178545117378, the training unsup-loss is 0.00990156086627394.\n",
      "At step 42, the training (sup)loss is 0.6295408138207027, the training unsup-loss is 0.009912945906675998.\n",
      "At step 44, the training (sup)loss is 0.6180479296229102, the training unsup-loss is 0.00991003177213398.\n",
      "At step 46, the training (sup)loss is 0.621646620657133, the training unsup-loss is 0.010089184076565763.\n",
      "At step 48, the training (sup)loss is 0.6102397379775842, the training unsup-loss is 0.010166715015657246.\n",
      "At step 50, the training (sup)loss is 0.6154598367214202, the training unsup-loss is 0.010161117669194936.\n",
      "At step 52, the training (sup)loss is 0.5917883045398272, the training unsup-loss is 0.010308729036926078.\n",
      "At step 54, the training (sup)loss is 0.5824621374960299, the training unsup-loss is 0.010588327646945362.\n",
      "At step 56, the training (sup)loss is 0.5616599182997432, the training unsup-loss is 0.0104409598861821.\n",
      "At step 58, the training (sup)loss is 0.5540031163856901, the training unsup-loss is 0.010444761245864732.\n",
      "At step 60, the training (sup)loss is 0.5355363458395004, the training unsup-loss is 0.010443264828063547.\n",
      "At step 62, the training (sup)loss is 0.5317902151615389, the training unsup-loss is 0.010313277621002447.\n",
      "At step 64, the training (sup)loss is 0.5275726765394211, the training unsup-loss is 0.010227158258203417.\n",
      "At step 66, the training (sup)loss is 0.5219289470802654, the training unsup-loss is 0.010179741936030261.\n",
      "At step 68, the training (sup)loss is 0.5065780956955517, the training unsup-loss is 0.010161188588587238.\n",
      "At step 70, the training (sup)loss is 0.5042228519916534, the training unsup-loss is 0.01023126470058092.\n",
      "At step 72, the training (sup)loss is 0.4999876477652126, the training unsup-loss is 0.01018156681675464.\n",
      "At step 74, the training (sup)loss is 0.50549747492816, the training unsup-loss is 0.010202721214374981.\n",
      "At step 76, the training (sup)loss is 0.5017479109136682, the training unsup-loss is 0.010223467405395288.\n",
      "At step 78, the training (sup)loss is 0.48888257986459976, the training unsup-loss is 0.010208210669075832.\n",
      "At step 80, the training (sup)loss is 0.48547705858945844, the training unsup-loss is 0.01016190998489037.\n",
      "At step 82, the training (sup)loss is 0.4824902066370336, the training unsup-loss is 0.010243102033599847.\n",
      "At step 84, the training (sup)loss is 0.4798522577399299, the training unsup-loss is 0.010298104690653937.\n",
      "At step 86, the training (sup)loss is 0.47709806367408397, the training unsup-loss is 0.010381106590462286.\n",
      "At step 88, the training (sup)loss is 0.46625492586330936, the training unsup-loss is 0.010386052493810315.\n",
      "At step 90, the training (sup)loss is 0.45589370528856915, the training unsup-loss is 0.010455354980917441.\n",
      "At step 92, the training (sup)loss is 0.4533339324204818, the training unsup-loss is 0.010486569483359546.\n",
      "At step 94, the training (sup)loss is 0.4516678249582331, the training unsup-loss is 0.010531418842877796.\n",
      "At step 96, the training (sup)loss is 0.4422580786049366, the training unsup-loss is 0.01055347895695983.\n",
      "At step 98, the training (sup)loss is 0.44771772014851474, the training unsup-loss is 0.010448563397310826.\n",
      "At step 100, the training (sup)loss is 0.4458197808265686, the training unsup-loss is 0.010470823682844639.\n",
      "At step 102, the training (sup)loss is 0.43707821649663586, the training unsup-loss is 0.010633762852818357.\n",
      "At step 104, the training (sup)loss is 0.4286728661793929, the training unsup-loss is 0.0106972036525034.\n",
      "At step 106, the training (sup)loss is 0.4205846988929893, the training unsup-loss is 0.010743878693934882.\n",
      "At step 108, the training (sup)loss is 0.4127960933579339, the training unsup-loss is 0.010780511712827892.\n",
      "At step 110, the training (sup)loss is 0.4052907098423351, the training unsup-loss is 0.010771296550096437.\n",
      "At step 112, the training (sup)loss is 0.41062745239053455, the training unsup-loss is 0.010740318123550554.\n",
      "At step 114, the training (sup)loss is 0.40342346199771817, the training unsup-loss is 0.01076828590757622.\n",
      "At step 116, the training (sup)loss is 0.396467885066723, the training unsup-loss is 0.010758200921278832.\n",
      "At step 118, the training (sup)loss is 0.38974809040457514, the training unsup-loss is 0.010848884695698903.\n",
      "At step 120, the training (sup)loss is 0.38325228889783225, the training unsup-loss is 0.01083275149964417.\n",
      "At step 122, the training (sup)loss is 0.37696946448967106, the training unsup-loss is 0.010837964679221393.\n",
      "At step 124, the training (sup)loss is 0.37088931183661183, the training unsup-loss is 0.010952069750806738.\n",
      "At step 126, the training (sup)loss is 0.37093906932406956, the training unsup-loss is 0.011041659998544862.\n",
      "At step 128, the training (sup)loss is 0.36514314636588097, the training unsup-loss is 0.011022204151231563.\n",
      "At step 130, the training (sup)loss is 0.35952555949871356, the training unsup-loss is 0.011053684378902499.\n",
      "At step 132, the training (sup)loss is 0.3540782025366118, the training unsup-loss is 0.011107026419433003.\n",
      "At step 134, the training (sup)loss is 0.34879345324502065, the training unsup-loss is 0.011118647402652832.\n",
      "At step 136, the training (sup)loss is 0.34366413775612326, the training unsup-loss is 0.011067907545058167.\n",
      "At step 138, the training (sup)loss is 0.3386834980784983, the training unsup-loss is 0.011107071242092745.\n",
      "At step 140, the training (sup)loss is 0.3338451623916626, the training unsup-loss is 0.011238694370591214.\n",
      "At step 142, the training (sup)loss is 0.329143117850935, the training unsup-loss is 0.011226096557794323.\n",
      "At step 144, the training (sup)loss is 0.32457168565856087, the training unsup-loss is 0.011223130794759426.\n",
      "At step 146, the training (sup)loss is 0.32555958179578387, the training unsup-loss is 0.011243856250474306.\n",
      "At step 148, the training (sup)loss is 0.32116012798773275, the training unsup-loss is 0.011229622108559753.\n",
      "At step 150, the training (sup)loss is 0.3168779929478963, the training unsup-loss is 0.01124121913065513.\n",
      "At step 152, the training (sup)loss is 0.3127085456722661, the training unsup-loss is 0.011279057668472984.\n",
      "At step 154, the training (sup)loss is 0.30864739572847044, the training unsup-loss is 0.011300951363397883.\n",
      "At step 156, the training (sup)loss is 0.3046903778345157, the training unsup-loss is 0.011293288293437889.\n",
      "At step 158, the training (sup)loss is 0.30083353760876236, the training unsup-loss is 0.011277985900546177.\n",
      "At step 160, the training (sup)loss is 0.2970731183886528, the training unsup-loss is 0.011271654587471858.\n",
      "At step 162, the training (sup)loss is 0.29340554902582994, the training unsup-loss is 0.011243745578268979.\n",
      "At step 164, the training (sup)loss is 0.29408881758771294, the training unsup-loss is 0.011232604071066328.\n",
      "At step 166, the training (sup)loss is 0.2905455788215959, the training unsup-loss is 0.011222801774358714.\n",
      "At step 168, the training (sup)loss is 0.28708670288324356, the training unsup-loss is 0.011209378936438866.\n",
      "At step 170, the training (sup)loss is 0.28370921226108775, the training unsup-loss is 0.011212443059091182.\n",
      "At step 172, the training (sup)loss is 0.2804102679324705, the training unsup-loss is 0.011185054439878049.\n",
      "At step 174, the training (sup)loss is 0.27718716140451105, the training unsup-loss is 0.011207750061762401.\n",
      "At step 176, the training (sup)loss is 0.2740373072976416, the training unsup-loss is 0.011199399770703167.\n",
      "At step 178, the training (sup)loss is 0.2745803614680687, the training unsup-loss is 0.011146947914169411.\n",
      "At step 180, the training (sup)loss is 0.2715294685628679, the training unsup-loss is 0.011106135799653.\n",
      "At step 182, the training (sup)loss is 0.26854562824899025, the training unsup-loss is 0.01109884174254078.\n",
      "At step 184, the training (sup)loss is 0.2656266540288925, the training unsup-loss is 0.011095937741312968.\n",
      "At step 186, the training (sup)loss is 0.2627704534479367, the training unsup-loss is 0.01109187027078963.\n",
      "At step 188, the training (sup)loss is 0.2634189696388042, the training unsup-loss is 0.011047569556815668.\n",
      "At step 190, the training (sup)loss is 0.26064613837944833, the training unsup-loss is 0.011034817638267812.\n",
      "At step 192, the training (sup)loss is 0.25793107443799573, the training unsup-loss is 0.011031869187718257.\n",
      "At step 194, the training (sup)loss is 0.25527199119636695, the training unsup-loss is 0.010995455849531693.\n",
      "At step 196, the training (sup)loss is 0.2526671749596693, the training unsup-loss is 0.011005219787226192.\n",
      "At step 198, the training (sup)loss is 0.250114981273208, the training unsup-loss is 0.011031152151355689.\n",
      "At step 200, the training (sup)loss is 0.2476138314604759, the training unsup-loss is 0.011015890778508038.\n",
      "At step 202, the training (sup)loss is 0.24838725735645484, the training unsup-loss is 0.01107484608252376.\n",
      "At step 204, the training (sup)loss is 0.24595208816668568, the training unsup-loss is 0.011060091258282317.\n",
      "At step 206, the training (sup)loss is 0.2435642038155528, the training unsup-loss is 0.01105660529020559.\n",
      "At step 208, the training (sup)loss is 0.24122224031732634, the training unsup-loss is 0.011043136744634606.\n",
      "At step 210, the training (sup)loss is 0.2389248856476375, the training unsup-loss is 0.011048676075768612.\n",
      "At step 212, the training (sup)loss is 0.2366708772924711, the training unsup-loss is 0.011053880120908736.\n",
      "At step 214, the training (sup)loss is 0.23445899993459754, the training unsup-loss is 0.01105455080021138.\n",
      "At step 216, the training (sup)loss is 0.23228808326853645, the training unsup-loss is 0.011053444470572113.\n",
      "At step 218, the training (sup)loss is 0.2301569999357976, the training unsup-loss is 0.011035537622749395.\n",
      "At step 220, the training (sup)loss is 0.22806466357274488, the training unsup-loss is 0.011044186652130025.\n",
      "At step 222, the training (sup)loss is 0.22601002696398143, the training unsup-loss is 0.011042361907023299.\n",
      "At step 224, the training (sup)loss is 0.22399208029466017, the training unsup-loss is 0.011070535925682634.\n",
      "At step 226, the training (sup)loss is 0.22200984949559238, the training unsup-loss is 0.01108337339607224.\n",
      "At step 228, the training (sup)loss is 0.2200623946754556, the training unsup-loss is 0.01108241465408355.\n",
      "At step 230, the training (sup)loss is 0.22092863450879635, the training unsup-loss is 0.011096395708053656.\n",
      "At step 232, the training (sup)loss is 0.219024077314755, the training unsup-loss is 0.011076067642565688.\n",
      "At step 234, the training (sup)loss is 0.21715207665394515, the training unsup-loss is 0.011081985548202299.\n",
      "At step 236, the training (sup)loss is 0.21531180481789475, the training unsup-loss is 0.011058978787702272.\n",
      "At step 238, the training (sup)loss is 0.2135024619202654, the training unsup-loss is 0.01107750495723566.\n",
      "At step 240, the training (sup)loss is 0.2117232747375965, the training unsup-loss is 0.011098024939807752.\n",
      "At step 242, the training (sup)loss is 0.20997349560753373, the training unsup-loss is 0.011086580747630725.\n",
      "At step 244, the training (sup)loss is 0.20825240138124246, the training unsup-loss is 0.01107729325590075.\n",
      "At step 246, the training (sup)loss is 0.20925034909713558, the training unsup-loss is 0.011096743132343621.\n",
      "At step 248, the training (sup)loss is 0.2075628462818361, the training unsup-loss is 0.011136669353882392.\n",
      "At step 250, the training (sup)loss is 0.20590234351158143, the training unsup-loss is 0.01112663472443819.\n",
      "At step 252, the training (sup)loss is 0.20426819792815618, the training unsup-loss is 0.011114024682827885.\n",
      "At step 254, the training (sup)loss is 0.20265978692084785, the training unsup-loss is 0.011112579657864852.\n",
      "At step 256, the training (sup)loss is 0.20107650733552873, the training unsup-loss is 0.011126720535685308.\n",
      "At step 258, the training (sup)loss is 0.19951777472052462, the training unsup-loss is 0.011140557572736527.\n",
      "At step 260, the training (sup)loss is 0.19798302260728984, the training unsup-loss is 0.011121382234761348.\n",
      "At step 262, the training (sup)loss is 0.19887885269317918, the training unsup-loss is 0.011092336818163517.\n",
      "At step 264, the training (sup)loss is 0.19737219471823086, the training unsup-loss is 0.011083085112852243.\n",
      "At step 266, the training (sup)loss is 0.195888193254184, the training unsup-loss is 0.011081538910634424.\n",
      "At step 268, the training (sup)loss is 0.19442634106571996, the training unsup-loss is 0.011063336504192384.\n",
      "At step 270, the training (sup)loss is 0.19298614594671462, the training unsup-loss is 0.011039339911399616.\n",
      "At step 272, the training (sup)loss is 0.19156713016769467, the training unsup-loss is 0.011054978627191089.\n",
      "At step 274, the training (sup)loss is 0.1901688299474925, the training unsup-loss is 0.011056334871363684.\n",
      "At step 276, the training (sup)loss is 0.19112860206244647, the training unsup-loss is 0.011037828913633373.\n",
      "At step 278, the training (sup)loss is 0.18975357614832816, the training unsup-loss is 0.011026293568634729.\n",
      "At step 280, the training (sup)loss is 0.1883981934615544, the training unsup-loss is 0.011024276489791061.\n",
      "At step 282, the training (sup)loss is 0.18706203606111785, the training unsup-loss is 0.011015742122202266.\n",
      "At step 284, the training (sup)loss is 0.1857446977789973, the training unsup-loss is 0.011022511518783343.\n",
      "At step 286, the training (sup)loss is 0.18444578380851478, the training unsup-loss is 0.011038966942578554.\n",
      "At step 288, the training (sup)loss is 0.1853683845450481, the training unsup-loss is 0.011044325003038265.\n",
      "At step 290, the training (sup)loss is 0.18408998189301326, the training unsup-loss is 0.011049928487246407.\n",
      "At step 292, the training (sup)loss is 0.18282909160607483, the training unsup-loss is 0.011058404237950502.\n",
      "At step 294, the training (sup)loss is 0.1815853562890267, the training unsup-loss is 0.011052351774407082.\n",
      "At step 296, the training (sup)loss is 0.18035842820599274, the training unsup-loss is 0.011029098607675248.\n",
      "At step 298, the training (sup)loss is 0.17914796895628807, the training unsup-loss is 0.011025537674209015.\n",
      "At step 300, the training (sup)loss is 0.17795364916324616, the training unsup-loss is 0.01102452383376658.\n",
      "At step 302, the training (sup)loss is 0.1767751481754101, the training unsup-loss is 0.011013227996003154.\n",
      "At step 304, the training (sup)loss is 0.17762199671644913, the training unsup-loss is 0.01100680922881063.\n",
      "At step 306, the training (sup)loss is 0.17646106863333508, the training unsup-loss is 0.01098133206787501.\n",
      "At step 308, the training (sup)loss is 0.17531521753831344, the training unsup-loss is 0.010974817215344058.\n",
      "At step 310, the training (sup)loss is 0.1741841516187114, the training unsup-loss is 0.011019197967083704.\n",
      "At step 312, the training (sup)loss is 0.1730675865442325, the training unsup-loss is 0.011030496887635821.\n",
      "At step 314, the training (sup)loss is 0.17196524522866413, the training unsup-loss is 0.011020379600702387.\n",
      "At step 316, the training (sup)loss is 0.17087685760063462, the training unsup-loss is 0.01100247308812281.\n",
      "At step 318, the training (sup)loss is 0.16980216038302057, the training unsup-loss is 0.01099738022078227.\n",
      "At step 320, the training (sup)loss is 0.16874089688062668, the training unsup-loss is 0.01100102197669912.\n",
      "At step 322, the training (sup)loss is 0.16769281677577808, the training unsup-loss is 0.011005472380274571.\n",
      "At step 324, the training (sup)loss is 0.16665767593148315, the training unsup-loss is 0.010970397677019607.\n",
      "At step 326, the training (sup)loss is 0.16563523620184215, the training unsup-loss is 0.010993883473672932.\n",
      "At step 328, the training (sup)loss is 0.1646252652493919, the training unsup-loss is 0.01098833994360686.\n",
      "At step 330, the training (sup)loss is 0.16362753636909252, the training unsup-loss is 0.010974586529262137.\n",
      "At step 332, the training (sup)loss is 0.16264182831867632, the training unsup-loss is 0.010985898869448757.\n",
      "At step 334, the training (sup)loss is 0.16166792515509143, the training unsup-loss is 0.010989024086359019.\n",
      "At step 336, the training (sup)loss is 0.1626575587406045, the training unsup-loss is 0.010978637465519742.\n",
      "At step 338, the training (sup)loss is 0.16169508797882576, the training unsup-loss is 0.010972506126951007.\n",
      "At step 340, the training (sup)loss is 0.16074394040247975, the training unsup-loss is 0.010985327347674791.\n",
      "At step 342, the training (sup)loss is 0.1598039173591904, the training unsup-loss is 0.010966168644385989.\n",
      "At step 344, the training (sup)loss is 0.1588748248164044, the training unsup-loss is 0.010978101992297398.\n",
      "At step 346, the training (sup)loss is 0.15975479066716453, the training unsup-loss is 0.01096216434844355.\n",
      "At step 348, the training (sup)loss is 0.15883665968631877, the training unsup-loss is 0.010972593799929252.\n",
      "At step 350, the training (sup)loss is 0.15975252083369665, the training unsup-loss is 0.010982504818322403.\n",
      "At step 352, the training (sup)loss is 0.15884483605623245, the training unsup-loss is 0.010968537966784259.\n",
      "At step 354, the training (sup)loss is 0.15966894330277956, the training unsup-loss is 0.010959148217560881.\n",
      "At step 356, the training (sup)loss is 0.16054338719067948, the training unsup-loss is 0.011006108922046724.\n",
      "At step 358, the training (sup)loss is 0.15964649675944664, the training unsup-loss is 0.011018547359541808.\n",
      "At step 360, the training (sup)loss is 0.1587595717774497, the training unsup-loss is 0.011023948351956076.\n",
      "At step 362, the training (sup)loss is 0.15788244707149696, the training unsup-loss is 0.011038686318621451.\n",
      "At step 364, the training (sup)loss is 0.15701496109857663, the training unsup-loss is 0.01106664070163618.\n",
      "At step 366, the training (sup)loss is 0.15615695584667186, the training unsup-loss is 0.011066579995320631.\n",
      "At step 368, the training (sup)loss is 0.1553082767388095, the training unsup-loss is 0.011089032462737321.\n",
      "At step 370, the training (sup)loss is 0.15446877254022134, the training unsup-loss is 0.011095373475974475.\n",
      "At step 372, the training (sup)loss is 0.15363829526849973, the training unsup-loss is 0.011110721241121972.\n",
      "At step 374, the training (sup)loss is 0.1528167001066361, the training unsup-loss is 0.011111271368429303.\n",
      "At step 376, the training (sup)loss is 0.15200384531883485, the training unsup-loss is 0.011165063986752896.\n",
      "At step 378, the training (sup)loss is 0.15119959216899972, the training unsup-loss is 0.011187163469869466.\n",
      "At step 380, the training (sup)loss is 0.15202675508825403, the training unsup-loss is 0.011231456510722637.\n",
      "At step 382, the training (sup)loss is 0.15123080349093332, the training unsup-loss is 0.011223633629781881.\n",
      "At step 384, the training (sup)loss is 0.15044314305608472, the training unsup-loss is 0.011240924866190957.\n",
      "At step 386, the training (sup)loss is 0.14966364490553505, the training unsup-loss is 0.011271274354250937.\n",
      "At step 388, the training (sup)loss is 0.14889218281839312, the training unsup-loss is 0.011319339726619498.\n",
      "At step 390, the training (sup)loss is 0.1481286331629142, the training unsup-loss is 0.011364204708773357.\n",
      "At step 392, the training (sup)loss is 0.14737287483045033, the training unsup-loss is 0.011358969189150601.\n",
      "At step 394, the training (sup)loss is 0.1466247891714125, the training unsup-loss is 0.011395884110464662.\n",
      "At step 396, the training (sup)loss is 0.14588425993317305, the training unsup-loss is 0.011398317409923883.\n",
      "At step 398, the training (sup)loss is 0.14515117319984053, the training unsup-loss is 0.011399918273278516.\n",
      "At step 400, the training (sup)loss is 0.14442541733384132, the training unsup-loss is 0.011418007814791053.\n",
      "At step 402, the training (sup)loss is 0.1437068829192451, the training unsup-loss is 0.011454502704428203.\n",
      "At step 404, the training (sup)loss is 0.1429954627067736, the training unsup-loss is 0.011457980439841453.\n",
      "At step 406, the training (sup)loss is 0.1422910515604348, the training unsup-loss is 0.011488430727309928.\n",
      "At step 408, the training (sup)loss is 0.1415935464057268, the training unsup-loss is 0.011499180536096295.\n",
      "At step 410, the training (sup)loss is 0.1409028461793574, the training unsup-loss is 0.011507027855188381.\n",
      "At step 412, the training (sup)loss is 0.14021885178042848, the training unsup-loss is 0.01152510620716923.\n",
      "At step 414, the training (sup)loss is 0.1395414660230351, the training unsup-loss is 0.011537406188638313.\n",
      "At step 416, the training (sup)loss is 0.13887059359023204, the training unsup-loss is 0.011560236690386843.\n",
      "At step 418, the training (sup)loss is 0.13820614098932185, the training unsup-loss is 0.011569814852318552.\n",
      "At step 420, the training (sup)loss is 0.1375480165084203, the training unsup-loss is 0.011608255315305932.\n",
      "At step 422, the training (sup)loss is 0.13689613017425717, the training unsup-loss is 0.01162119877582054.\n",
      "At step 424, the training (sup)loss is 0.13625039371117106, the training unsup-loss is 0.011622861497771909.\n",
      "At step 426, the training (sup)loss is 0.13561072050125947, the training unsup-loss is 0.011619473116673494.\n",
      "At step 428, the training (sup)loss is 0.13497702554564608, the training unsup-loss is 0.011633763307374772.\n",
      "At step 430, the training (sup)loss is 0.13434922542682914, the training unsup-loss is 0.011622485158921675.\n",
      "At step 432, the training (sup)loss is 0.1337272382720753, the training unsup-loss is 0.011622419927907348.\n",
      "At step 434, the training (sup)loss is 0.1331109837178261, the training unsup-loss is 0.01162663980683286.\n",
      "At step 436, the training (sup)loss is 0.13250038287508378, the training unsup-loss is 0.011622871757151747.\n",
      "At step 438, the training (sup)loss is 0.1318953582957455, the training unsup-loss is 0.011640773303636677.\n",
      "At step 440, the training (sup)loss is 0.13129583393985575, the training unsup-loss is 0.01163265170444819.\n",
      "At step 442, the training (sup)loss is 0.1307017351437478, the training unsup-loss is 0.011637400458587915.\n",
      "At step 444, the training (sup)loss is 0.13011298858904624, the training unsup-loss is 0.011631981604984281.\n",
      "At step 446, the training (sup)loss is 0.12952952227250342, the training unsup-loss is 0.011632733469862374.\n",
      "At step 448, the training (sup)loss is 0.12895126547664404, the training unsup-loss is 0.011626298571562594.\n",
      "At step 450, the training (sup)loss is 0.1283781487411923, the training unsup-loss is 0.011606188504439261.\n",
      "At step 452, the training (sup)loss is 0.12781010383525782, the training unsup-loss is 0.011613695557740212.\n",
      "At step 454, the training (sup)loss is 0.12724706373025668, the training unsup-loss is 0.011606819453825641.\n",
      "At step 456, the training (sup)loss is 0.12668896257354503, the training unsup-loss is 0.011619211007073.\n",
      "At step 458, the training (sup)loss is 0.12613573566274353, the training unsup-loss is 0.01161875358404486.\n",
      "At step 460, the training (sup)loss is 0.1255873194207316, the training unsup-loss is 0.011596695795331312.\n",
      "At step 462, the training (sup)loss is 0.12631103280302766, the training unsup-loss is 0.0115811427293176.\n",
      "At step 464, the training (sup)loss is 0.12576658869611806, the training unsup-loss is 0.011570336201814291.\n",
      "At step 466, the training (sup)loss is 0.12522681792918192, the training unsup-loss is 0.011558485126458589.\n",
      "At step 468, the training (sup)loss is 0.12469166058760422, the training unsup-loss is 0.01155405970883325.\n",
      "At step 470, the training (sup)loss is 0.12545620405927618, the training unsup-loss is 0.011538456959966966.\n",
      "At step 472, the training (sup)loss is 0.12492460997427925, the training unsup-loss is 0.01153545405309237.\n",
      "At step 474, the training (sup)loss is 0.12439750191531604, the training unsup-loss is 0.011540710257036211.\n",
      "At step 476, the training (sup)loss is 0.12387482333583992, the training unsup-loss is 0.01154944532993054.\n",
      "At step 478, the training (sup)loss is 0.12335651863568996, the training unsup-loss is 0.011543914167187079.\n",
      "At step 480, the training (sup)loss is 0.12284253314137458, the training unsup-loss is 0.011547081358730792.\n",
      "At step 482, the training (sup)loss is 0.12233281308684606, the training unsup-loss is 0.01156632735730145.\n",
      "At step 484, the training (sup)loss is 0.12182730559475166, the training unsup-loss is 0.01155799301341176.\n",
      "At step 486, the training (sup)loss is 0.12132595865814774, the training unsup-loss is 0.011575473631529038.\n",
      "At step 488, the training (sup)loss is 0.1220880210643909, the training unsup-loss is 0.011588262852098121.\n",
      "At step 490, the training (sup)loss is 0.12158970261106686, the training unsup-loss is 0.01161334697271184.\n",
      "At step 492, the training (sup)loss is 0.12109543552728204, the training unsup-loss is 0.01162174084163233.\n",
      "At step 494, the training (sup)loss is 0.12060517060611894, the training unsup-loss is 0.01162985588804732.\n",
      "At step 496, the training (sup)loss is 0.12011885943432007, the training unsup-loss is 0.011614182937679993.\n",
      "At step 498, the training (sup)loss is 0.11963645437635093, the training unsup-loss is 0.011622399295564276.\n",
      "At step 500, the training (sup)loss is 0.12032592689990998, the training unsup-loss is 0.011653900554403662.\n",
      "At step 502, the training (sup)loss is 0.11984654073696212, the training unsup-loss is 0.01166036518928836.\n",
      "At step 504, the training (sup)loss is 0.11937095922610116, the training unsup-loss is 0.011678787861167201.\n",
      "At step 506, the training (sup)loss is 0.11889913725287547, the training unsup-loss is 0.011680130235733015.\n",
      "At step 508, the training (sup)loss is 0.11967210788426437, the training unsup-loss is 0.011705401964030984.\n",
      "At step 510, the training (sup)loss is 0.1192028055004045, the training unsup-loss is 0.011747389609980233.\n",
      "At step 512, the training (sup)loss is 0.11873716954141855, the training unsup-loss is 0.01176399343603407.\n",
      "At step 514, the training (sup)loss is 0.11827515720857257, the training unsup-loss is 0.011774180125416136.\n",
      "At step 516, the training (sup)loss is 0.11893279434636582, the training unsup-loss is 0.011772404004307103.\n",
      "At step 518, the training (sup)loss is 0.11847359436819452, the training unsup-loss is 0.011797814315335976.\n",
      "At step 520, the training (sup)loss is 0.11910584316803859, the training unsup-loss is 0.011812339040737313.\n",
      "At step 522, the training (sup)loss is 0.11864949894134112, the training unsup-loss is 0.011828238297953231.\n",
      "At step 524, the training (sup)loss is 0.11819663825835891, the training unsup-loss is 0.011841583664649885.\n",
      "At step 526, the training (sup)loss is 0.11774722138285183, the training unsup-loss is 0.011870520789629493.\n",
      "At step 528, the training (sup)loss is 0.11730120918064406, the training unsup-loss is 0.011868725266222927.\n",
      "At step 530, the training (sup)loss is 0.11685856310826427, the training unsup-loss is 0.011905209663903939.\n",
      "At step 532, the training (sup)loss is 0.11641924520184223, the training unsup-loss is 0.01192445008266241.\n",
      "At step 534, the training (sup)loss is 0.1159832180662548, the training unsup-loss is 0.011977858457728271.\n",
      "At step 536, the training (sup)loss is 0.11664904476101719, the training unsup-loss is 0.01199251078520971.\n",
      "At step 538, the training (sup)loss is 0.1162154051894149, the training unsup-loss is 0.012002930225160486.\n",
      "At step 540, the training (sup)loss is 0.11578497776278743, the training unsup-loss is 0.012046961124158567.\n",
      "At step 542, the training (sup)loss is 0.11535772692233434, the training unsup-loss is 0.012051528194280018.\n",
      "At step 544, the training (sup)loss is 0.1149336176321787, the training unsup-loss is 0.012070352509480846.\n",
      "At step 546, the training (sup)loss is 0.11451261536978977, the training unsup-loss is 0.012100813095031422.\n",
      "At step 548, the training (sup)loss is 0.11409468611661536, the training unsup-loss is 0.012126941172691592.\n",
      "At step 550, the training (sup)loss is 0.11367979634891857, the training unsup-loss is 0.012154564429074526.\n",
      "At step 552, the training (sup)loss is 0.1132679130288138, the training unsup-loss is 0.01216318947263062.\n",
      "At step 554, the training (sup)loss is 0.11285900359549678, the training unsup-loss is 0.012190039293161368.\n",
      "At step 556, the training (sup)loss is 0.11245303595666405, the training unsup-loss is 0.012200088148079009.\n",
      "At step 558, the training (sup)loss is 0.11204997848011687, the training unsup-loss is 0.012208581798606448.\n",
      "At step 560, the training (sup)loss is 0.1127242013812065, the training unsup-loss is 0.012220402246540678.\n",
      "At step 562, the training (sup)loss is 0.11232304763963638, the training unsup-loss is 0.012231343435753283.\n",
      "At step 564, the training (sup)loss is 0.1119247389600632, the training unsup-loss is 0.012248309585100687.\n",
      "At step 566, the training (sup)loss is 0.11152924518281916, the training unsup-loss is 0.012281535555322264.\n",
      "At step 568, the training (sup)loss is 0.1111365365730205, the training unsup-loss is 0.01230290535063496.\n",
      "At step 570, the training (sup)loss is 0.11074658381311517, the training unsup-loss is 0.01232748333560793.\n",
      "At step 572, the training (sup)loss is 0.1103593579955868, the training unsup-loss is 0.01236257053117131.\n",
      "At step 574, the training (sup)loss is 0.10997483061581123, the training unsup-loss is 0.012378881416098582.\n",
      "At step 576, the training (sup)loss is 0.11061583583553632, the training unsup-loss is 0.012419772509019822.\n",
      "At step 578, the training (sup)loss is 0.11116807961958915, the training unsup-loss is 0.012439279257928933.\n",
      "At step 580, the training (sup)loss is 0.11078474141400436, the training unsup-loss is 0.012519917918112258.\n",
      "At step 582, the training (sup)loss is 0.11040403783526208, the training unsup-loss is 0.012554230160460747.\n",
      "At step 584, the training (sup)loss is 0.1100259418152783, the training unsup-loss is 0.012621540781939784.\n",
      "At step 586, the training (sup)loss is 0.10965042665549919, the training unsup-loss is 0.012648812773297366.\n",
      "At step 588, the training (sup)loss is 0.10927746602061654, the training unsup-loss is 0.012706479056085757.\n",
      "At step 590, the training (sup)loss is 0.10890703393241107, the training unsup-loss is 0.012744511018301976.\n",
      "At step 592, the training (sup)loss is 0.10853910476372049, the training unsup-loss is 0.012798061448061285.\n",
      "At step 594, the training (sup)loss is 0.1081736532325295, the training unsup-loss is 0.012852005461412748.\n",
      "At step 596, the training (sup)loss is 0.10781065439617873, the training unsup-loss is 0.012877973495051265.\n",
      "At step 598, the training (sup)loss is 0.10745008364568985, the training unsup-loss is 0.012920291725247102.\n",
      "At step 600, the training (sup)loss is 0.10709191670020421, the training unsup-loss is 0.012964313603006303.\n",
      "At step 602, the training (sup)loss is 0.10673612960153243, the training unsup-loss is 0.013009596441213375.\n",
      "At step 604, the training (sup)loss is 0.10638269870881213, the training unsup-loss is 0.01304399743460316.\n",
      "At step 606, the training (sup)loss is 0.1060316006932715, the training unsup-loss is 0.0130938730055079.\n",
      "At step 608, the training (sup)loss is 0.10568281253309626, the training unsup-loss is 0.01313720149827484.\n",
      "At step 610, the training (sup)loss is 0.10533631150839759, the training unsup-loss is 0.013143762515583.\n",
      "At step 612, the training (sup)loss is 0.10499207519627864, the training unsup-loss is 0.01320192229392287.\n",
      "At step 614, the training (sup)loss is 0.10465008146599761, the training unsup-loss is 0.013260078840928474.\n",
      "At step 616, the training (sup)loss is 0.10431030847422489, the training unsup-loss is 0.013344969542987355.\n",
      "At step 618, the training (sup)loss is 0.10397273466039245, the training unsup-loss is 0.013372156530932123.\n",
      "At step 620, the training (sup)loss is 0.10363733874213311, the training unsup-loss is 0.013390076794331112.\n",
      "At step 622, the training (sup)loss is 0.10330409971080792, the training unsup-loss is 0.013419843632188832.\n",
      "At step 624, the training (sup)loss is 0.10297299682711944, the training unsup-loss is 0.013460287528757293.\n",
      "At step 626, the training (sup)loss is 0.10264400961680915, the training unsup-loss is 0.013514462450798898.\n",
      "At step 628, the training (sup)loss is 0.10231711786643714, the training unsup-loss is 0.013532142037715597.\n",
      "At step 630, the training (sup)loss is 0.10199230161924211, the training unsup-loss is 0.01358134617496814.\n",
      "At step 632, the training (sup)loss is 0.10166954117107994, the training unsup-loss is 0.013597387344505685.\n",
      "At step 634, the training (sup)loss is 0.10134881706643932, the training unsup-loss is 0.013627061536623759.\n",
      "At step 636, the training (sup)loss is 0.10103011009453228, the training unsup-loss is 0.013659600014710482.\n",
      "At step 638, the training (sup)loss is 0.10155371989949744, the training unsup-loss is 0.013673016910957972.\n",
      "At step 640, the training (sup)loss is 0.10123636452481151, the training unsup-loss is 0.013721140402776656.\n",
      "At step 642, the training (sup)loss is 0.1009209864421797, the training unsup-loss is 0.0137679474500829.\n",
      "At step 644, the training (sup)loss is 0.10060756722962634, the training unsup-loss is 0.013784972885739266.\n",
      "At step 646, the training (sup)loss is 0.1002960886933117, the training unsup-loss is 0.01381224718654165.\n",
      "At step 648, the training (sup)loss is 0.09998653286401137, the training unsup-loss is 0.013831715316534687.\n",
      "At step 650, the training (sup)loss is 0.09967888199366055, the training unsup-loss is 0.013846102366940333.\n",
      "At step 652, the training (sup)loss is 0.09937311855196221, the training unsup-loss is 0.013888596209977393.\n",
      "At step 654, the training (sup)loss is 0.09906922522305713, the training unsup-loss is 0.013899292680078872.\n",
      "At step 656, the training (sup)loss is 0.09961292720059069, the training unsup-loss is 0.013915246361071562.\n",
      "At step 658, the training (sup)loss is 0.09931015234587764, the training unsup-loss is 0.013928908193738718.\n",
      "At step 660, the training (sup)loss is 0.09900921249028408, the training unsup-loss is 0.013969850801213672.\n",
      "At step 662, the training (sup)loss is 0.09871009100239803, the training unsup-loss is 0.013998375171542258.\n",
      "At step 664, the training (sup)loss is 0.09841277145118599, the training unsup-loss is 0.014019771155070918.\n",
      "At step 666, the training (sup)loss is 0.09811723760298423, the training unsup-loss is 0.014072472569406838.\n",
      "At step 668, the training (sup)loss is 0.09782347341854415, the training unsup-loss is 0.01408732086169104.\n",
      "At step 670, the training (sup)loss is 0.09753146305013059, the training unsup-loss is 0.014094305768219838.\n",
      "At step 672, the training (sup)loss is 0.09724119083867186, the training unsup-loss is 0.014125055327202148.\n",
      "At step 674, the training (sup)loss is 0.09695264131096068, the training unsup-loss is 0.014146066131208435.\n",
      "At step 676, the training (sup)loss is 0.09666579917690457, the training unsup-loss is 0.014196980362978472.\n",
      "At step 678, the training (sup)loss is 0.09638064932682522, the training unsup-loss is 0.014221545311575618.\n",
      "At step 680, the training (sup)loss is 0.09609717682880514, the training unsup-loss is 0.014247953727403107.\n",
      "At step 682, the training (sup)loss is 0.09581536692608136, the training unsup-loss is 0.014260614584432231.\n",
      "At step 684, the training (sup)loss is 0.09553520503448464, the training unsup-loss is 0.014283171208161447.\n",
      "At step 686, the training (sup)loss is 0.09525667673992345, the training unsup-loss is 0.014305600151148117.\n",
      "At step 688, the training (sup)loss is 0.09497976779591205, the training unsup-loss is 0.014321881399197546.\n",
      "At step 690, the training (sup)loss is 0.0947044641211413, the training unsup-loss is 0.014334226082470538.\n",
      "At step 692, the training (sup)loss is 0.09443075179709176, the training unsup-loss is 0.014362166416319894.\n",
      "At step 694, the training (sup)loss is 0.09415861706568804, the training unsup-loss is 0.014380990051173253.\n",
      "At step 696, the training (sup)loss is 0.09388804632699353, the training unsup-loss is 0.01439906209680498.\n",
      "At step 698, the training (sup)loss is 0.09361902613694484, the training unsup-loss is 0.014399319143971178.\n",
      "At step 700, the training (sup)loss is 0.093351543205125, the training unsup-loss is 0.014406168436897652.\n",
      "At step 702, the training (sup)loss is 0.09308558439257478, the training unsup-loss is 0.014413859268008304.\n",
      "At step 704, the training (sup)loss is 0.09282113670964133, the training unsup-loss is 0.014419047632889653.\n",
      "At step 706, the training (sup)loss is 0.09255818731386331, the training unsup-loss is 0.01443851840479178.\n",
      "At step 708, the training (sup)loss is 0.09229672350789193, the training unsup-loss is 0.01445901126727083.\n",
      "At step 710, the training (sup)loss is 0.09203673273744717, the training unsup-loss is 0.014453730723020477.\n",
      "At step 712, the training (sup)loss is 0.09177820258930827, the training unsup-loss is 0.014457516118444586.\n",
      "At step 714, the training (sup)loss is 0.09152112078933823, the training unsup-loss is 0.014473702482023492.\n",
      "At step 716, the training (sup)loss is 0.0912654752005412, the training unsup-loss is 0.01448515135087281.\n",
      "At step 718, the training (sup)loss is 0.0910112538211525, the training unsup-loss is 0.014507192665643596.\n",
      "At step 720, the training (sup)loss is 0.0907584447827604, the training unsup-loss is 0.014493461469343553.\n",
      "At step 722, the training (sup)loss is 0.09050703634845914, the training unsup-loss is 0.014494618591839587.\n",
      "At step 724, the training (sup)loss is 0.09025701691103245, the training unsup-loss is 0.014487533997332687.\n",
      "At step 726, the training (sup)loss is 0.09000837499116734, the training unsup-loss is 0.014498427355041107.\n",
      "At step 728, the training (sup)loss is 0.08976109923569711, the training unsup-loss is 0.014490302650201656.\n",
      "At step 730, the training (sup)loss is 0.08951517841587328, the training unsup-loss is 0.014493312330100021.\n",
      "At step 732, the training (sup)loss is 0.08927060142566598, the training unsup-loss is 0.014496905617848487.\n",
      "At step 734, the training (sup)loss is 0.08902735728009195, the training unsup-loss is 0.014492498125185637.\n",
      "At step 736, the training (sup)loss is 0.08878543511356997, the training unsup-loss is 0.014482028292031433.\n",
      "At step 738, the training (sup)loss is 0.08854482417830284, the training unsup-loss is 0.014500284265237166.\n",
      "At step 740, the training (sup)loss is 0.0883055138426858, the training unsup-loss is 0.014507608205658962.\n",
      "At step 742, the training (sup)loss is 0.08806749358974056, the training unsup-loss is 0.0145353972202674.\n",
      "At step 744, the training (sup)loss is 0.0878307530155746, the training unsup-loss is 0.01454024957755821.\n",
      "At step 746, the training (sup)loss is 0.08759528182786527, the training unsup-loss is 0.01453109133936185.\n",
      "At step 748, the training (sup)loss is 0.08736106984436831, the training unsup-loss is 0.01453727948725881.\n",
      "At step 750, the training (sup)loss is 0.08712810699144999, the training unsup-loss is 0.014550136328364412.\n",
      "At step 752, the training (sup)loss is 0.08689638330264295, the training unsup-loss is 0.014534047589280662.\n",
      "At step 754, the training (sup)loss is 0.08732423874048087, the training unsup-loss is 0.014531301875507326.\n",
      "At step 756, the training (sup)loss is 0.08709322223587641, the training unsup-loss is 0.014530848413607272.\n",
      "At step 758, the training (sup)loss is 0.08686342481572898, the training unsup-loss is 0.014520774000769083.\n",
      "At step 760, the training (sup)loss is 0.0866348368556876, the training unsup-loss is 0.014522554726633979.\n",
      "At step 762, the training (sup)loss is 0.08640744883244432, the training unsup-loss is 0.014535115452439297.\n",
      "At step 764, the training (sup)loss is 0.08618125132241174, the training unsup-loss is 0.014532614122656861.\n",
      "At step 766, the training (sup)loss is 0.08595623500042111, the training unsup-loss is 0.014547228741437664.\n",
      "At step 768, the training (sup)loss is 0.08573239063844085, the training unsup-loss is 0.014573493275141422.\n",
      "At step 770, the training (sup)loss is 0.08550970910431503, the training unsup-loss is 0.014581621440683867.\n",
      "At step 772, the training (sup)loss is 0.08528818136052146, the training unsup-loss is 0.014597236097988147.\n",
      "At step 774, the training (sup)loss is 0.08506779846294905, the training unsup-loss is 0.014611041362250575.\n",
      "At step 776, the training (sup)loss is 0.08484855155969404, the training unsup-loss is 0.014619380136058886.\n",
      "At step 778, the training (sup)loss is 0.085324081249899, the training unsup-loss is 0.014623075874479233.\n",
      "At step 780, the training (sup)loss is 0.08510530155438643, the training unsup-loss is 0.014622060875766552.\n",
      "At step 782, the training (sup)loss is 0.08488764093660028, the training unsup-loss is 0.014648143097262858.\n",
      "At step 784, the training (sup)loss is 0.08467109083217017, the training unsup-loss is 0.014682915064088089.\n",
      "At step 786, the training (sup)loss is 0.08445564276389493, the training unsup-loss is 0.01469319639135029.\n",
      "At step 788, the training (sup)loss is 0.08424128834063632, the training unsup-loss is 0.014738411236043811.\n",
      "At step 790, the training (sup)loss is 0.08402801925622964, the training unsup-loss is 0.014752887851924081.\n",
      "At step 792, the training (sup)loss is 0.08381582728841087, the training unsup-loss is 0.014772680733879709.\n",
      "At step 794, the training (sup)loss is 0.08360470429775997, the training unsup-loss is 0.01478919140544136.\n",
      "At step 796, the training (sup)loss is 0.08339464222666007, the training unsup-loss is 0.014805735283105368.\n",
      "At step 798, the training (sup)loss is 0.08318563309827245, the training unsup-loss is 0.014827871982260305.\n",
      "At step 800, the training (sup)loss is 0.08297766901552678, the training unsup-loss is 0.014866570958402008.\n",
      "At step 802, the training (sup)loss is 0.08277074216012646, the training unsup-loss is 0.014880331598937585.\n",
      "At step 804, the training (sup)loss is 0.08256484479156893, the training unsup-loss is 0.014914113056350185.\n",
      "At step 806, the training (sup)loss is 0.08235996924618041, the training unsup-loss is 0.014932605103998636.\n",
      "At step 808, the training (sup)loss is 0.08215610793616512, the training unsup-loss is 0.014957072122234592.\n",
      "At step 810, the training (sup)loss is 0.08195325334866842, the training unsup-loss is 0.014967071642110377.\n",
      "At step 812, the training (sup)loss is 0.08175139804485396, the training unsup-loss is 0.014999961374979007.\n",
      "At step 814, the training (sup)loss is 0.08155053465899437, the training unsup-loss is 0.01502641812149957.\n",
      "At step 816, the training (sup)loss is 0.08135065589757527, the training unsup-loss is 0.015035694385148292.\n",
      "At step 818, the training (sup)loss is 0.08115175453841249, the training unsup-loss is 0.015046489935067726.\n",
      "At step 820, the training (sup)loss is 0.08095382342978222, the training unsup-loss is 0.015043015960921965.\n",
      "At step 822, the training (sup)loss is 0.08075685548956377, the training unsup-loss is 0.015071763549398839.\n",
      "At step 824, the training (sup)loss is 0.08115988796197095, the training unsup-loss is 0.015077661005192373.\n",
      "At step 826, the training (sup)loss is 0.08096337491605819, the training unsup-loss is 0.01509155645362185.\n",
      "At step 828, the training (sup)loss is 0.08076781120853148, the training unsup-loss is 0.015130347295569769.\n",
      "At step 830, the training (sup)loss is 0.0805731899767037, the training unsup-loss is 0.015153760348547655.\n",
      "At step 832, the training (sup)loss is 0.08037950442387508, the training unsup-loss is 0.015171097493014084.\n",
      "At step 834, the training (sup)loss is 0.08018674781854204, the training unsup-loss is 0.015203976894257594.\n",
      "At step 836, the training (sup)loss is 0.0799949134936173, the training unsup-loss is 0.015240971255227567.\n",
      "At step 838, the training (sup)loss is 0.07980399484566117, the training unsup-loss is 0.015250699116730604.\n",
      "At step 840, the training (sup)loss is 0.07961398533412388, the training unsup-loss is 0.0152825863654947.\n",
      "At step 842, the training (sup)loss is 0.07942487848059865, the training unsup-loss is 0.015326484675008158.\n",
      "At step 844, the training (sup)loss is 0.07923666786808538, the training unsup-loss is 0.015337993811277487.\n",
      "At step 846, the training (sup)loss is 0.07967329983451969, the training unsup-loss is 0.015347932659124793.\n",
      "At step 848, the training (sup)loss is 0.07948539110849488, the training unsup-loss is 0.015367420906429443.\n",
      "At step 850, the training (sup)loss is 0.07929836665882783, the training unsup-loss is 0.015398711667341344.\n",
      "At step 852, the training (sup)loss is 0.07911222025822026, the training unsup-loss is 0.015407188823927597.\n",
      "At step 854, the training (sup)loss is 0.07892694573770921, the training unsup-loss is 0.015445662570781395.\n",
      "At step 856, the training (sup)loss is 0.07874253698598559, the training unsup-loss is 0.015491275782653384.\n",
      "At step 858, the training (sup)loss is 0.07855898794872222, the training unsup-loss is 0.015535337342457338.\n",
      "At step 860, the training (sup)loss is 0.07837629262791124, the training unsup-loss is 0.015578923240130724.\n",
      "At step 862, the training (sup)loss is 0.07819444508121075, the training unsup-loss is 0.015601762695769562.\n",
      "At step 864, the training (sup)loss is 0.07801343942130054, the training unsup-loss is 0.015608986459146426.\n",
      "At step 866, the training (sup)loss is 0.07783326981524673, the training unsup-loss is 0.015663010417752605.\n",
      "At step 868, the training (sup)loss is 0.07765393048387519, the training unsup-loss is 0.015698423391328223.\n",
      "At step 870, the training (sup)loss is 0.07747541570115364, the training unsup-loss is 0.01572168929416729.\n",
      "At step 872, the training (sup)loss is 0.07729771979358219, the training unsup-loss is 0.015744518335917196.\n",
      "At step 874, the training (sup)loss is 0.0771208371395923, the training unsup-loss is 0.015775368848359488.\n",
      "At step 876, the training (sup)loss is 0.07694476216895395, the training unsup-loss is 0.015804475681541582.\n",
      "At step 878, the training (sup)loss is 0.07676948936219095, the training unsup-loss is 0.015836249875723636.\n",
      "At step 880, the training (sup)loss is 0.07659501325000416, the training unsup-loss is 0.01586385062176057.\n",
      "At step 882, the training (sup)loss is 0.07642132841270256, the training unsup-loss is 0.015889099936366353.\n",
      "At step 884, the training (sup)loss is 0.07624842947964215, the training unsup-loss is 0.015913085611663522.\n",
      "At step 886, the training (sup)loss is 0.0760763111286723, the training unsup-loss is 0.01593551991200474.\n",
      "At step 888, the training (sup)loss is 0.0759049680855897, the training unsup-loss is 0.01597110671830271.\n",
      "At step 890, the training (sup)loss is 0.07573439512359963, the training unsup-loss is 0.01597253180663572.\n",
      "At step 892, the training (sup)loss is 0.07556458706278438, the training unsup-loss is 0.01598750048154138.\n",
      "At step 894, the training (sup)loss is 0.07653844886578169, the training unsup-loss is 0.01601213552016073.\n",
      "At step 896, the training (sup)loss is 0.07636760411384914, the training unsup-loss is 0.016030037923233716.\n",
      "At step 898, the training (sup)loss is 0.0761975203630388, the training unsup-loss is 0.016045893928272555.\n",
      "At step 900, the training (sup)loss is 0.07602819254000982, the training unsup-loss is 0.016068750911702713.\n",
      "At step 902, the training (sup)loss is 0.07585961561641778, the training unsup-loss is 0.016085476858206837.\n",
      "At step 904, the training (sup)loss is 0.07569178460841686, the training unsup-loss is 0.016115844595880634.\n",
      "At step 906, the training (sup)loss is 0.07552469457616869, the training unsup-loss is 0.01612582491098124.\n",
      "At step 908, the training (sup)loss is 0.07535834062335775, the training unsup-loss is 0.01613256802150594.\n",
      "At step 910, the training (sup)loss is 0.075192717896713, the training unsup-loss is 0.016146293411461208.\n",
      "At step 912, the training (sup)loss is 0.07565889059843724, the training unsup-loss is 0.016161431891337168.\n",
      "At step 914, the training (sup)loss is 0.07549333503914088, the training unsup-loss is 0.01619678484663139.\n",
      "At step 916, the training (sup)loss is 0.07532850242988512, the training unsup-loss is 0.016229900098344775.\n",
      "At step 918, the training (sup)loss is 0.07516438804550628, the training unsup-loss is 0.016251615601670898.\n",
      "At step 920, the training (sup)loss is 0.07500098720192909, the training unsup-loss is 0.016295214742422105.\n",
      "At step 922, the training (sup)loss is 0.074838295255721, the training unsup-loss is 0.01631854086459782.\n",
      "At step 924, the training (sup)loss is 0.07467630760365235, the training unsup-loss is 0.016384069777589726.\n",
      "At step 926, the training (sup)loss is 0.07451501968226217, the training unsup-loss is 0.016431876886676248.\n",
      "At step 928, the training (sup)loss is 0.07435442696742971, the training unsup-loss is 0.016496617531660814.\n",
      "At step 930, the training (sup)loss is 0.07419452497395136, the training unsup-loss is 0.01653171285426104.\n",
      "At step 932, the training (sup)loss is 0.07403530925512314, the training unsup-loss is 0.016620696554272216.\n",
      "At step 934, the training (sup)loss is 0.07387677540232844, the training unsup-loss is 0.016666592389935474.\n",
      "At step 936, the training (sup)loss is 0.07371891904463115, the training unsup-loss is 0.016734246970512547.\n",
      "At step 938, the training (sup)loss is 0.07356173584837394, the training unsup-loss is 0.016771585392608826.\n",
      "At step 940, the training (sup)loss is 0.07340522151678167, the training unsup-loss is 0.016790989776478804.\n",
      "At step 942, the training (sup)loss is 0.07324937178956982, the training unsup-loss is 0.016801739346845007.\n",
      "At step 944, the training (sup)loss is 0.07309418244255801, the training unsup-loss is 0.01685588378695992.\n",
      "At step 946, the training (sup)loss is 0.07293964928728834, the training unsup-loss is 0.016873715004350115.\n",
      "At step 948, the training (sup)loss is 0.07278576817064848, the training unsup-loss is 0.016899263209227144.\n",
      "At step 950, the training (sup)loss is 0.07263253497449976, the training unsup-loss is 0.016924015105162796.\n",
      "At step 952, the training (sup)loss is 0.07296066609125178, the training unsup-loss is 0.016939576065653012.\n",
      "At step 954, the training (sup)loss is 0.07280770871999129, the training unsup-loss is 0.01697986639754952.\n",
      "At step 956, the training (sup)loss is 0.0726553913377319, the training unsup-loss is 0.017016259140907847.\n",
      "At step 958, the training (sup)loss is 0.07250370993619175, the training unsup-loss is 0.017046719099276374.\n",
      "At step 960, the training (sup)loss is 0.07235266054049135, the training unsup-loss is 0.017078979757692046.\n",
      "At step 962, the training (sup)loss is 0.07220223920880633, the training unsup-loss is 0.01714104851765903.\n",
      "At step 964, the training (sup)loss is 0.07205244203202457, the training unsup-loss is 0.01718713626984306.\n",
      "At step 966, the training (sup)loss is 0.07190326513340754, the training unsup-loss is 0.01720393308309171.\n",
      "At step 968, the training (sup)loss is 0.07235850834033707, the training unsup-loss is 0.01722450442188843.\n",
      "At step 970, the training (sup)loss is 0.07220931553963533, the training unsup-loss is 0.017238411541605735.\n",
      "At step 972, the training (sup)loss is 0.07206073670107642, the training unsup-loss is 0.01723247968377124.\n",
      "At step 974, the training (sup)loss is 0.07191276804255264, the training unsup-loss is 0.017251541555975802.\n",
      "At step 976, the training (sup)loss is 0.07176540581295725, the training unsup-loss is 0.017287070457403717.\n",
      "At step 978, the training (sup)loss is 0.07161864629186736, the training unsup-loss is 0.01737011235744401.\n",
      "At step 980, the training (sup)loss is 0.07147248578923089, the training unsup-loss is 0.01740019410107361.\n",
      "At step 982, the training (sup)loss is 0.0713269206450573, the training unsup-loss is 0.017455692618303837.\n",
      "At step 984, the training (sup)loss is 0.07118194722911207, the training unsup-loss is 0.017520666100667805.\n",
      "At step 986, the training (sup)loss is 0.07103756194061488, the training unsup-loss is 0.01760870612436116.\n",
      "At step 988, the training (sup)loss is 0.07089376120794157, the training unsup-loss is 0.017661095771237242.\n",
      "At step 990, the training (sup)loss is 0.07075054148832957, the training unsup-loss is 0.017727251878629127.\n",
      "At step 992, the training (sup)loss is 0.07060789926758697, the training unsup-loss is 0.017775533580964793.\n",
      "At step 994, the training (sup)loss is 0.0704658310598051, the training unsup-loss is 0.017828469023190935.\n",
      "At step 996, the training (sup)loss is 0.07032433340707457, the training unsup-loss is 0.017857588308650148.\n",
      "At step 998, the training (sup)loss is 0.07018340287920469, the training unsup-loss is 0.01786597644656389.\n",
      "At step 1000, the training (sup)loss is 0.07106772896647454, the training unsup-loss is 0.017905699108727278.\n",
      "At step 1002, the training (sup)loss is 0.07137637592837244, the training unsup-loss is 0.01792714961278373.\n",
      "At step 1004, the training (sup)loss is 0.07123419191257888, the training unsup-loss is 0.01803647312409879.\n",
      "At step 1006, the training (sup)loss is 0.07109257324078448, the training unsup-loss is 0.01810180569237939.\n",
      "At step 1008, the training (sup)loss is 0.07095151654784641, the training unsup-loss is 0.018152191286115716.\n",
      "At step 1010, the training (sup)loss is 0.07081101849527642, the training unsup-loss is 0.01821085643299883.\n",
      "At step 1012, the training (sup)loss is 0.07167717882178047, the training unsup-loss is 0.018236549676988612.\n",
      "At step 1014, the training (sup)loss is 0.07153580371562311, the training unsup-loss is 0.018254372062569364.\n",
      "At step 1016, the training (sup)loss is 0.07139498520437187, the training unsup-loss is 0.01828528329359705.\n",
      "At step 1018, the training (sup)loss is 0.07125472000750671, the training unsup-loss is 0.018353492128598724.\n",
      "At step 1020, the training (sup)loss is 0.07111500487023709, the training unsup-loss is 0.018419171490834333.\n",
      "At step 1022, the training (sup)loss is 0.07097583656325032, the training unsup-loss is 0.018465791868115183.\n",
      "At step 1024, the training (sup)loss is 0.07083721188246273, the training unsup-loss is 0.01856062317801843.\n",
      "At step 1026, the training (sup)loss is 0.07069912764877372, the training unsup-loss is 0.018730275430053938.\n",
      "At step 1028, the training (sup)loss is 0.0705615807078228, the training unsup-loss is 0.0188093270503115.\n",
      "At step 1030, the training (sup)loss is 0.07042456792974935, the training unsup-loss is 0.018883604551801113.\n",
      "At step 1032, the training (sup)loss is 0.07028808620895526, the training unsup-loss is 0.018956719978004983.\n",
      "At step 1034, the training (sup)loss is 0.07015213246387024, the training unsup-loss is 0.01905302915315016.\n",
      "At step 1036, the training (sup)loss is 0.07001670363671991, the training unsup-loss is 0.019114182950051423.\n",
      "At step 1038, the training (sup)loss is 0.06988179669329656, the training unsup-loss is 0.019253099452962594.\n",
      "At step 1040, the training (sup)loss is 0.06974740862273253, the training unsup-loss is 0.019302462002871414.\n",
      "At step 1042, the training (sup)loss is 0.06961353643727623, the training unsup-loss is 0.019388499600909322.\n",
      "At step 1044, the training (sup)loss is 0.06948017717207072, the training unsup-loss is 0.019495535271699.\n",
      "At step 1046, the training (sup)loss is 0.06934732788493483, the training unsup-loss is 0.019526521905893407.\n",
      "At step 1048, the training (sup)loss is 0.06921498565614678, the training unsup-loss is 0.019613098208294856.\n",
      "At step 1050, the training (sup)loss is 0.06908314758823031, the training unsup-loss is 0.01970232900498169.\n",
      "At step 1052, the training (sup)loss is 0.06895181080574318, the training unsup-loss is 0.019742166100268185.\n",
      "At step 1054, the training (sup)loss is 0.06882097245506814, the training unsup-loss is 0.019780730737631248.\n",
      "At step 1056, the training (sup)loss is 0.06869062970420628, the training unsup-loss is 0.019858148330857864.\n",
      "At step 1058, the training (sup)loss is 0.06856077974257262, the training unsup-loss is 0.019888860973686206.\n",
      "At step 1060, the training (sup)loss is 0.06843141978079419, the training unsup-loss is 0.01993532116032856.\n",
      "At step 1062, the training (sup)loss is 0.0683025470505102, the training unsup-loss is 0.019994085948395135.\n",
      "At step 1064, the training (sup)loss is 0.06817415880417466, the training unsup-loss is 0.020095924149211356.\n",
      "At step 1066, the training (sup)loss is 0.068046252314861, the training unsup-loss is 0.020160419998365483.\n",
      "At step 1068, the training (sup)loss is 0.06791882487606914, the training unsup-loss is 0.020229603626943204.\n",
      "At step 1070, the training (sup)loss is 0.06779187380153442, the training unsup-loss is 0.02031075574858027.\n",
      "At step 1072, the training (sup)loss is 0.06766539642503902, the training unsup-loss is 0.020353617468925278.\n",
      "At step 1074, the training (sup)loss is 0.06753939010022517, the training unsup-loss is 0.020395595021025593.\n",
      "At step 1076, the training (sup)loss is 0.06741385220041063, the training unsup-loss is 0.02045451529708355.\n",
      "At step 1078, the training (sup)loss is 0.06728878011840615, the training unsup-loss is 0.02050718245509466.\n",
      "At step 1080, the training (sup)loss is 0.06716417126633503, the training unsup-loss is 0.02056405522118978.\n",
      "At step 1082, the training (sup)loss is 0.06704002307545456, the training unsup-loss is 0.020615539053031512.\n",
      "At step 1084, the training (sup)loss is 0.06691633299597954, the training unsup-loss is 0.02064705835019558.\n",
      "At step 1086, the training (sup)loss is 0.06679309849690776, the training unsup-loss is 0.02065926443521297.\n",
      "At step 1088, the training (sup)loss is 0.06667031706584726, the training unsup-loss is 0.0207103353981545.\n",
      "At step 1090, the training (sup)loss is 0.06654798620884572, the training unsup-loss is 0.02075523131125427.\n",
      "At step 1092, the training (sup)loss is 0.06642610345022146, the training unsup-loss is 0.020798711965010663.\n",
      "At step 1094, the training (sup)loss is 0.06630466633239655, the training unsup-loss is 0.020843732706228548.\n",
      "At step 1096, the training (sup)loss is 0.0661836724157316, the training unsup-loss is 0.020888262910149773.\n",
      "At step 1098, the training (sup)loss is 0.06606311927836232, the training unsup-loss is 0.020942750455597875.\n",
      "At step 1100, the training (sup)loss is 0.06594300451603803, the training unsup-loss is 0.020971929045732727.\n",
      "At step 1102, the training (sup)loss is 0.06582332574196173, the training unsup-loss is 0.020998074119562407.\n",
      "At step 1104, the training (sup)loss is 0.06570408058663209, the training unsup-loss is 0.02103037559477936.\n",
      "At step 1106, the training (sup)loss is 0.06558526669768701, the training unsup-loss is 0.021055061130146168.\n",
      "At step 1108, the training (sup)loss is 0.06546688173974895, the training unsup-loss is 0.021087637294709197.\n",
      "At step 1110, the training (sup)loss is 0.06534892339427192, the training unsup-loss is 0.021116711394777436.\n",
      "At step 1112, the training (sup)loss is 0.06523138935939013, the training unsup-loss is 0.021126031362210407.\n",
      "At step 1114, the training (sup)loss is 0.06511427734976825, the training unsup-loss is 0.02114780130419927.\n",
      "At step 1116, the training (sup)loss is 0.06499758509645326, the training unsup-loss is 0.021179789262506642.\n",
      "At step 1118, the training (sup)loss is 0.06488131034672794, the training unsup-loss is 0.021212481707834335.\n",
      "At step 1120, the training (sup)loss is 0.06476545086396591, the training unsup-loss is 0.021250060668847125.\n",
      "At step 1122, the training (sup)loss is 0.06465000442748826, the training unsup-loss is 0.02127711596996681.\n",
      "At step 1124, the training (sup)loss is 0.06453496883242156, the training unsup-loss is 0.021320747774180516.\n",
      "At step 1126, the training (sup)loss is 0.06442034188955757, the training unsup-loss is 0.021356315849114006.\n",
      "At step 1128, the training (sup)loss is 0.06430612142521439, the training unsup-loss is 0.021368213571733564.\n",
      "At step 1130, the training (sup)loss is 0.06419230528109897, the training unsup-loss is 0.021389026184096535.\n",
      "At step 1132, the training (sup)loss is 0.06407889131417123, the training unsup-loss is 0.02141719911487727.\n",
      "At step 1134, the training (sup)loss is 0.06433617324951044, the training unsup-loss is 0.02143554508807614.\n",
      "At step 1136, the training (sup)loss is 0.0642229053388599, the training unsup-loss is 0.021452910141383206.\n",
      "At step 1138, the training (sup)loss is 0.06411003555794802, the training unsup-loss is 0.021503115180054898.\n",
      "At step 1140, the training (sup)loss is 0.06399756181135512, the training unsup-loss is 0.02153225606879252.\n",
      "At step 1142, the training (sup)loss is 0.06388548201834049, the training unsup-loss is 0.02153755580164792.\n",
      "At step 1144, the training (sup)loss is 0.06377379411271403, the training unsup-loss is 0.021575021685322524.\n",
      "At step 1146, the training (sup)loss is 0.06366249604270928, the training unsup-loss is 0.02159159013799946.\n",
      "At step 1148, the training (sup)loss is 0.06355158577085787, the training unsup-loss is 0.021638939331548503.\n",
      "At step 1150, the training (sup)loss is 0.06344106127386508, the training unsup-loss is 0.021702820515341083.\n",
      "At step 1152, the training (sup)loss is 0.06333092054248685, the training unsup-loss is 0.021737169117841404.\n",
      "At step 1154, the training (sup)loss is 0.063221161581408, the training unsup-loss is 0.021784168375136276.\n",
      "At step 1156, the training (sup)loss is 0.06311178240912184, the training unsup-loss is 0.02183966948264069.\n",
      "At step 1158, the training (sup)loss is 0.06300278105781074, the training unsup-loss is 0.021886499638030618.\n",
      "At step 1160, the training (sup)loss is 0.0628941555732283, the training unsup-loss is 0.021904279326152954.\n",
      "At step 1162, the training (sup)loss is 0.06278590401458248, the training unsup-loss is 0.021923083360763762.\n",
      "At step 1164, the training (sup)loss is 0.06267802445441996, the training unsup-loss is 0.021958246194242255.\n",
      "At step 1166, the training (sup)loss is 0.06257051497851186, the training unsup-loss is 0.021989181996395053.\n",
      "At step 1168, the training (sup)loss is 0.06284674553021993, the training unsup-loss is 0.022049506686863885.\n",
      "At step 1170, the training (sup)loss is 0.06273931519598024, the training unsup-loss is 0.022098672120139384.\n",
      "At step 1172, the training (sup)loss is 0.0626322515181714, the training unsup-loss is 0.02214825266905434.\n",
      "At step 1174, the training (sup)loss is 0.06252555262291046, the training unsup-loss is 0.02218324405869415.\n",
      "At step 1176, the training (sup)loss is 0.06241921664906197, the training unsup-loss is 0.022246968017758002.\n",
      "At step 1178, the training (sup)loss is 0.06231324174812977, the training unsup-loss is 0.022291121119712376.\n",
      "At step 1180, the training (sup)loss is 0.0622076260841499, the training unsup-loss is 0.022309217266580562.\n",
      "At step 1182, the training (sup)loss is 0.0621023678335845, the training unsup-loss is 0.02231868893276843.\n",
      "At step 1184, the training (sup)loss is 0.061997465185216954, the training unsup-loss is 0.022358008261574934.\n",
      "At step 1186, the training (sup)loss is 0.061892916340047954, the training unsup-loss is 0.022367895207828004.\n",
      "At step 1188, the training (sup)loss is 0.06178871951119266, the training unsup-loss is 0.022378577729248086.\n",
      "At step 1190, the training (sup)loss is 0.06168487292377889, the training unsup-loss is 0.02243340940551222.\n",
      "At step 1192, the training (sup)loss is 0.06158137481484637, the training unsup-loss is 0.022480395042037454.\n",
      "At step 1194, the training (sup)loss is 0.061478223433246965, the training unsup-loss is 0.022542593970277053.\n",
      "At step 1196, the training (sup)loss is 0.06137541703954588, the training unsup-loss is 0.022591483948228998.\n",
      "At step 1198, the training (sup)loss is 0.06127295390592394, the training unsup-loss is 0.022643294049355393.\n",
      "At step 1200, the training (sup)loss is 0.06117083231608073, the training unsup-loss is 0.022687180251038323.\n",
      "At step 1202, the training (sup)loss is 0.06106905056513883, the training unsup-loss is 0.022773485671815627.\n",
      "At step 1204, the training (sup)loss is 0.0609676069595489, the training unsup-loss is 0.02279392185936561.\n",
      "At step 1206, the training (sup)loss is 0.06086649981699575, the training unsup-loss is 0.022829335288702207.\n",
      "At step 1208, the training (sup)loss is 0.06076572746630536, the training unsup-loss is 0.022880626303476856.\n",
      "At step 1210, the training (sup)loss is 0.06066528824735279, the training unsup-loss is 0.022933961916714908.\n",
      "At step 1212, the training (sup)loss is 0.06056518051097102, the training unsup-loss is 0.02296595726540257.\n",
      "At step 1214, the training (sup)loss is 0.060465402618860686, the training unsup-loss is 0.022988433735613916.\n",
      "At step 1216, the training (sup)loss is 0.06036595294350072, the training unsup-loss is 0.02302225045384962.\n",
      "At step 1218, the training (sup)loss is 0.06026682986805983, the training unsup-loss is 0.023054459211011288.\n",
      "At step 1220, the training (sup)loss is 0.06016803178630891, the training unsup-loss is 0.023107286138826462.\n",
      "At step 1222, the training (sup)loss is 0.06006955710253427, the training unsup-loss is 0.02314166678877855.\n",
      "At step 1224, the training (sup)loss is 0.0599714042314517, the training unsup-loss is 0.023179752946210403.\n",
      "At step 1226, the training (sup)loss is 0.059873571598121435, the training unsup-loss is 0.02323486910810843.\n",
      "At step 1228, the training (sup)loss is 0.05977605763786391, the training unsup-loss is 0.023249410764019974.\n",
      "At step 1230, the training (sup)loss is 0.05967886079617632, the training unsup-loss is 0.023262819565257165.\n",
      "At step 1232, the training (sup)loss is 0.05958197952865006, the training unsup-loss is 0.023275244188065605.\n",
      "At step 1234, the training (sup)loss is 0.05948541230088888, the training unsup-loss is 0.023319917475137515.\n",
      "At step 1236, the training (sup)loss is 0.059389157588427895, the training unsup-loss is 0.02332984079594775.\n",
      "At step 1238, the training (sup)loss is 0.05929321387665337, the training unsup-loss is 0.02336948285306622.\n",
      "At step 1240, the training (sup)loss is 0.05919757966072329, the training unsup-loss is 0.023404346013651982.\n",
      "At step 1242, the training (sup)loss is 0.059102253445488626, the training unsup-loss is 0.02341925472414028.\n",
      "At step 1244, the training (sup)loss is 0.05900723374541549, the training unsup-loss is 0.023454534766560727.\n",
      "At step 1246, the training (sup)loss is 0.058912519084507925, the training unsup-loss is 0.02349135306787888.\n",
      "At step 1248, the training (sup)loss is 0.058818107996231474, the training unsup-loss is 0.023553728937165428.\n",
      "At step 1250, the training (sup)loss is 0.0587239990234375, the training unsup-loss is 0.02359482194110751.\n",
      "At step 1252, the training (sup)loss is 0.05863019071828824, the training unsup-loss is 0.023639837529111547.\n",
      "At step 1254, the training (sup)loss is 0.058536681642182516, the training unsup-loss is 0.023661210296001208.\n",
      "At step 1256, the training (sup)loss is 0.058443470365682225, the training unsup-loss is 0.02366545209053097.\n",
      "At step 1258, the training (sup)loss is 0.05835055546843949, the training unsup-loss is 0.023689693392999173.\n",
      "At step 1260, the training (sup)loss is 0.058257935539124506, the training unsup-loss is 0.023702913629896346.\n",
      "At step 1262, the training (sup)loss is 0.0581656091753541, the training unsup-loss is 0.023717818916782665.\n",
      "At step 1264, the training (sup)loss is 0.058073574983620944, the training unsup-loss is 0.023750184075111123.\n",
      "At step 1266, the training (sup)loss is 0.05798183157922344, the training unsup-loss is 0.023755652743934597.\n",
      "At step 1268, the training (sup)loss is 0.05789037758619627, the training unsup-loss is 0.02375999185820742.\n",
      "At step 1270, the training (sup)loss is 0.05779921163724163, the training unsup-loss is 0.023769729115860904.\n",
      "At step 1272, the training (sup)loss is 0.05770833237366107, the training unsup-loss is 0.02379126723272925.\n",
      "At step 1274, the training (sup)loss is 0.05761773844528797, the training unsup-loss is 0.0237992267194533.\n",
      "At step 1276, the training (sup)loss is 0.057527428510420746, the training unsup-loss is 0.023835475532466482.\n",
      "At step 1278, the training (sup)loss is 0.05743740123575655, the training unsup-loss is 0.023856649891632217.\n",
      "At step 1280, the training (sup)loss is 0.057347655296325684, the training unsup-loss is 0.023879720978584373.\n",
      "At step 1282, the training (sup)loss is 0.05725818937542658, the training unsup-loss is 0.023881434244299875.\n",
      "At step 1284, the training (sup)loss is 0.05716900216456143, the training unsup-loss is 0.023921327732903796.\n",
      "At step 1286, the training (sup)loss is 0.05708009236337237, the training unsup-loss is 0.023935139562332102.\n",
      "At step 1288, the training (sup)loss is 0.05699145867957832, the training unsup-loss is 0.02394290119105869.\n",
      "At step 1290, the training (sup)loss is 0.056903099828912306, the training unsup-loss is 0.02395632764642206.\n",
      "At step 1292, the training (sup)loss is 0.0568150145350595, the training unsup-loss is 0.023963557910202965.\n",
      "At step 1294, the training (sup)loss is 0.05672720152959573, the training unsup-loss is 0.02400084401205919.\n",
      "At step 1296, the training (sup)loss is 0.056639659551926604, the training unsup-loss is 0.024015826005689845.\n",
      "At step 1298, the training (sup)loss is 0.05655238734922718, the training unsup-loss is 0.024024759438635065.\n",
      "At step 1300, the training (sup)loss is 0.05646538367638221, the training unsup-loss is 0.024046580378109444.\n",
      "At step 1302, the training (sup)loss is 0.05637864729592694, the training unsup-loss is 0.024087753306399062.\n",
      "At step 1304, the training (sup)loss is 0.0562921769779884, the training unsup-loss is 0.02411718464076542.\n",
      "At step 1306, the training (sup)loss is 0.05620597150022732, the training unsup-loss is 0.024152770351082268.\n",
      "At step 1308, the training (sup)loss is 0.056120029647780484, the training unsup-loss is 0.024168792395035107.\n",
      "At step 1310, the training (sup)loss is 0.05603435021320372, the training unsup-loss is 0.02421224763730669.\n",
      "At step 1312, the training (sup)loss is 0.0559489319964153, the training unsup-loss is 0.024234448770407542.\n",
      "At step 1314, the training (sup)loss is 0.055863773804639937, the training unsup-loss is 0.024253019380498016.\n",
      "At step 1316, the training (sup)loss is 0.05577887445235325, the training unsup-loss is 0.024287228198654066.\n",
      "At step 1318, the training (sup)loss is 0.055694232761226765, the training unsup-loss is 0.024311872486771285.\n",
      "At step 1320, the training (sup)loss is 0.05560984756007339, the training unsup-loss is 0.024321252393366938.\n",
      "At step 1322, the training (sup)loss is 0.0555257176847934, the training unsup-loss is 0.024340071072224273.\n",
      "At step 1324, the training (sup)loss is 0.0554418419783209, the training unsup-loss is 0.024343401159151398.\n",
      "At step 1326, the training (sup)loss is 0.0553582192905708, the training unsup-loss is 0.02437167289016451.\n",
      "At step 1328, the training (sup)loss is 0.0552748484783862, the training unsup-loss is 0.02439960984566462.\n",
      "At step 1330, the training (sup)loss is 0.05519172840548637, the training unsup-loss is 0.024418049468833924.\n",
      "At step 1332, the training (sup)loss is 0.055108857942415074, the training unsup-loss is 0.024432738390597362.\n",
      "At step 1334, the training (sup)loss is 0.05502623596648941, the training unsup-loss is 0.024446025691127804.\n",
      "At step 1336, the training (sup)loss is 0.054943861361749155, the training unsup-loss is 0.02445399474901911.\n",
      "At step 1338, the training (sup)loss is 0.05486173301890648, the training unsup-loss is 0.024451699775402812.\n",
      "At step 1340, the training (sup)loss is 0.05477984983529618, the training unsup-loss is 0.024464189548240003.\n",
      "At step 1342, the training (sup)loss is 0.054698210714826286, the training unsup-loss is 0.024489014307467263.\n",
      "At step 1344, the training (sup)loss is 0.05489521970351537, the training unsup-loss is 0.0244918766651868.\n",
      "At step 1346, the training (sup)loss is 0.054813651769334816, the training unsup-loss is 0.024492574374801734.\n",
      "At step 1348, the training (sup)loss is 0.054732325876501974, the training unsup-loss is 0.024507143844930506.\n",
      "At step 1350, the training (sup)loss is 0.05465124094927752, the training unsup-loss is 0.024548683531444383.\n",
      "At step 1352, the training (sup)loss is 0.05457039591828747, the training unsup-loss is 0.024582518251563185.\n",
      "At step 1354, the training (sup)loss is 0.05448978972047611, the training unsup-loss is 0.02459996579451544.\n",
      "At step 1356, the training (sup)loss is 0.054409421299059485, the training unsup-loss is 0.024621881929893806.\n",
      "At step 1358, the training (sup)loss is 0.05432928960347913, the training unsup-loss is 0.024655618652048492.\n",
      "At step 1360, the training (sup)loss is 0.05424939358935637, the training unsup-loss is 0.024656300356967704.\n",
      "At step 1362, the training (sup)loss is 0.054169732218446884, the training unsup-loss is 0.02470149151863265.\n",
      "At step 1364, the training (sup)loss is 0.05409030445859579, the training unsup-loss is 0.024709874411083518.\n",
      "At step 1366, the training (sup)loss is 0.054011109283693015, the training unsup-loss is 0.024746225827058578.\n",
      "At step 1368, the training (sup)loss is 0.05393214567362914, the training unsup-loss is 0.024757642461645978.\n",
      "At step 1370, the training (sup)loss is 0.053853412614251576, the training unsup-loss is 0.02478615928400498.\n",
      "At step 1372, the training (sup)loss is 0.05377490909732118, the training unsup-loss is 0.024834217703914807.\n",
      "At step 1374, the training (sup)loss is 0.05369663412046918, the training unsup-loss is 0.02485674041698875.\n",
      "At step 1376, the training (sup)loss is 0.05361858668715455, the training unsup-loss is 0.0248927446765836.\n",
      "At step 1378, the training (sup)loss is 0.05354076580662167, the training unsup-loss is 0.024920718509876892.\n",
      "At step 1380, the training (sup)loss is 0.053463170493858445, the training unsup-loss is 0.024964595597534292.\n",
      "At step 1382, the training (sup)loss is 0.053385799769554744, the training unsup-loss is 0.024999257867856627.\n",
      "At step 1384, the training (sup)loss is 0.05330865266006117, the training unsup-loss is 0.02502235632074857.\n",
      "At step 1386, the training (sup)loss is 0.05323172819734824, the training unsup-loss is 0.025084167280104038.\n",
      "At step 1388, the training (sup)loss is 0.05315502541896589, the training unsup-loss is 0.02510800461622991.\n",
      "At step 1390, the training (sup)loss is 0.05307854336800335, the training unsup-loss is 0.025122406322294646.\n",
      "At step 1392, the training (sup)loss is 0.053002281093049324, the training unsup-loss is 0.025138823049886943.\n",
      "At step 1394, the training (sup)loss is 0.05292623764815255, the training unsup-loss is 0.0251850575373015.\n",
      "At step 1396, the training (sup)loss is 0.052850412092782705, the training unsup-loss is 0.025237982659694073.\n",
      "At step 1398, the training (sup)loss is 0.0527748034917916, the training unsup-loss is 0.02527746359866981.\n",
      "At step 1400, the training (sup)loss is 0.05269941091537476, the training unsup-loss is 0.025321649900371473.\n",
      "At step 1402, the training (sup)loss is 0.05262423343903328, the training unsup-loss is 0.025338800548949145.\n",
      "At step 1404, the training (sup)loss is 0.05254927014353608, the training unsup-loss is 0.02536581842655958.\n",
      "At step 1406, the training (sup)loss is 0.05271909005441842, the training unsup-loss is 0.02538861353864435.\n",
      "At step 1408, the training (sup)loss is 0.05264420498331839, the training unsup-loss is 0.02543764387916202.\n",
      "At step 1410, the training (sup)loss is 0.052569532352136385, the training unsup-loss is 0.025472900166394228.\n",
      "At step 1412, the training (sup)loss is 0.052495071258153184, the training unsup-loss is 0.025485811960189564.\n",
      "At step 1414, the training (sup)loss is 0.05242082080375693, the training unsup-loss is 0.025505602285344958.\n",
      "At step 1416, the training (sup)loss is 0.05234678009640699, the training unsup-loss is 0.025543876349436657.\n",
      "At step 1418, the training (sup)loss is 0.05227294824859824, the training unsup-loss is 0.02555210788882691.\n",
      "At step 1420, the training (sup)loss is 0.05219932437782556, the training unsup-loss is 0.025579673763443257.\n",
      "At step 1422, the training (sup)loss is 0.052125907606548734, the training unsup-loss is 0.02560972168834673.\n",
      "At step 1424, the training (sup)loss is 0.05205269706215751, the training unsup-loss is 0.02564106559514309.\n",
      "At step 1426, the training (sup)loss is 0.051979691876937094, the training unsup-loss is 0.025664200791422943.\n",
      "At step 1428, the training (sup)loss is 0.05190689118803382, the training unsup-loss is 0.025686706225218185.\n",
      "At step 1430, the training (sup)loss is 0.05183429413742119, the training unsup-loss is 0.025691878461519945.\n",
      "At step 1432, the training (sup)loss is 0.05176189987186613, the training unsup-loss is 0.025711551110270594.\n",
      "At step 1434, the training (sup)loss is 0.051689707542895605, the training unsup-loss is 0.02573495873313714.\n",
      "At step 1436, the training (sup)loss is 0.05161771630676344, the training unsup-loss is 0.025769215642051736.\n",
      "At step 1438, the training (sup)loss is 0.05154592532441745, the training unsup-loss is 0.02578584802102541.\n",
      "At step 1440, the training (sup)loss is 0.05147433376146687, the training unsup-loss is 0.02583311908141089.\n",
      "At step 1442, the training (sup)loss is 0.05140294078815, the training unsup-loss is 0.025880442807598145.\n",
      "At step 1444, the training (sup)loss is 0.051331745579302146, the training unsup-loss is 0.025899356328482403.\n",
      "At step 1446, the training (sup)loss is 0.05126074731432386, the training unsup-loss is 0.025909360171647362.\n",
      "At step 1448, the training (sup)loss is 0.05118994517714938, the training unsup-loss is 0.025926670125809122.\n",
      "At step 1450, the training (sup)loss is 0.05111933835621538, the training unsup-loss is 0.025921156356946147.\n",
      "At step 1452, the training (sup)loss is 0.05104892604442996, the training unsup-loss is 0.025960828295008332.\n",
      "At step 1454, the training (sup)loss is 0.05097870743914188, the training unsup-loss is 0.025980594063861787.\n",
      "At step 1456, the training (sup)loss is 0.05090868174211009, the training unsup-loss is 0.026023593357032433.\n",
      "At step 1458, the training (sup)loss is 0.050838848159473454, the training unsup-loss is 0.026039701305835702.\n",
      "At step 1460, the training (sup)loss is 0.05076920590172075, the training unsup-loss is 0.026052721452656878.\n",
      "At step 1462, the training (sup)loss is 0.05069975418366094, the training unsup-loss is 0.026082365423894492.\n",
      "At step 1464, the training (sup)loss is 0.05063049222439365, the training unsup-loss is 0.026109445484512855.\n",
      "At step 1466, the training (sup)loss is 0.05056141924727987, the training unsup-loss is 0.026126319933225277.\n",
      "At step 1468, the training (sup)loss is 0.05049253447991301, the training unsup-loss is 0.026127048337334712.\n",
      "At step 1470, the training (sup)loss is 0.05042383715409, the training unsup-loss is 0.026162432025813934.\n",
      "At step 1472, the training (sup)loss is 0.05035532650578281, the training unsup-loss is 0.026192080804637797.\n",
      "At step 1474, the training (sup)loss is 0.050287001775110106, the training unsup-loss is 0.02622335463360437.\n",
      "At step 1476, the training (sup)loss is 0.050218862206309146, the training unsup-loss is 0.026237453015389926.\n",
      "At step 1478, the training (sup)loss is 0.050150907047707916, the training unsup-loss is 0.026253054429511053.\n",
      "At step 1480, the training (sup)loss is 0.0500831355516975, the training unsup-loss is 0.026251914041947474.\n",
      "At step 1482, the training (sup)loss is 0.05001554697470466, the training unsup-loss is 0.026275499999729286.\n",
      "At step 1484, the training (sup)loss is 0.049948140577164624, the training unsup-loss is 0.026315220833110883.\n",
      "At step 1486, the training (sup)loss is 0.050106151528345626, the training unsup-loss is 0.026343510798924424.\n",
      "At step 1488, the training (sup)loss is 0.05003880455048494, the training unsup-loss is 0.026403892262693573.\n",
      "At step 1490, the training (sup)loss is 0.049971638369880264, the training unsup-loss is 0.02640825173061266.\n",
      "At step 1492, the training (sup)loss is 0.04990465225946488, the training unsup-loss is 0.02643545954513186.\n",
      "At step 1494, the training (sup)loss is 0.049837845496065326, the training unsup-loss is 0.026465854933547726.\n",
      "At step 1496, the training (sup)loss is 0.0497712173603754, the training unsup-loss is 0.02648586113277305.\n",
      "At step 1498, the training (sup)loss is 0.049704767136930304, the training unsup-loss is 0.026522888945607104.\n",
      "At step 1500, the training (sup)loss is 0.04963849411408106, the training unsup-loss is 0.02654782675517102.\n",
      "At step 1502, the training (sup)loss is 0.049572397583969106, the training unsup-loss is 0.026562514080580437.\n",
      "At step 1504, the training (sup)loss is 0.04950647684250106, the training unsup-loss is 0.026578880142435393.\n",
      "At step 1506, the training (sup)loss is 0.04944073118932377, the training unsup-loss is 0.026601008664579148.\n",
      "At step 1508, the training (sup)loss is 0.04937515992779947, the training unsup-loss is 0.026616426478127703.\n",
      "At step 1510, the training (sup)loss is 0.04930976236498119, the training unsup-loss is 0.026651446824116225.\n",
      "At step 1512, the training (sup)loss is 0.04924453781158836, the training unsup-loss is 0.026707750956389914.\n",
      "At step 1514, the training (sup)loss is 0.04917948558198256, the training unsup-loss is 0.026739156889253084.\n",
      "At step 1516, the training (sup)loss is 0.04911460499414353, the training unsup-loss is 0.026802668482492773.\n",
      "At step 1518, the training (sup)loss is 0.049049895369645324, the training unsup-loss is 0.02684805682297104.\n",
      "At step 1520, the training (sup)loss is 0.04898535603363263, the training unsup-loss is 0.026865117419143453.\n",
      "At step 1522, the training (sup)loss is 0.04892098631479737, the training unsup-loss is 0.026894038612250624.\n",
      "At step 1524, the training (sup)loss is 0.04885678554535538, the training unsup-loss is 0.02693005056325786.\n",
      "At step 1526, the training (sup)loss is 0.048792753061023326, the training unsup-loss is 0.02697214823848474.\n",
      "At step 1528, the training (sup)loss is 0.04872888820099581, the training unsup-loss is 0.02702806784505639.\n",
      "At step 1530, the training (sup)loss is 0.048665190307922615, the training unsup-loss is 0.027065554544256597.\n",
      "At step 1532, the training (sup)loss is 0.04860165872788616, the training unsup-loss is 0.027105794906465447.\n",
      "At step 1534, the training (sup)loss is 0.048538292810379136, the training unsup-loss is 0.027124130431164392.\n",
      "At step 1536, the training (sup)loss is 0.04847509190828229, the training unsup-loss is 0.027170687014707557.\n",
      "At step 1538, the training (sup)loss is 0.04841205537784239, the training unsup-loss is 0.027209855626386885.\n",
      "At step 1540, the training (sup)loss is 0.04834918257865039, the training unsup-loss is 0.027237094155597417.\n",
      "At step 1542, the training (sup)loss is 0.048286472873619715, the training unsup-loss is 0.027269763790491373.\n",
      "At step 1544, the training (sup)loss is 0.04822392562896476, the training unsup-loss is 0.027340864941810736.\n",
      "At step 1546, the training (sup)loss is 0.04816154021417956, the training unsup-loss is 0.02735556260457342.\n",
      "At step 1548, the training (sup)loss is 0.048099316002016536, the training unsup-loss is 0.027381062322929115.\n",
      "At step 1550, the training (sup)loss is 0.048037252368465545, the training unsup-loss is 0.027412372847358066.\n",
      "At step 1552, the training (sup)loss is 0.04797534869273299, the training unsup-loss is 0.027422508298654648.\n",
      "At step 1554, the training (sup)loss is 0.047913604357221105, the training unsup-loss is 0.02744362519051942.\n",
      "At step 1556, the training (sup)loss is 0.047852018747507455, the training unsup-loss is 0.027449194966171386.\n",
      "At step 1558, the training (sup)loss is 0.047790591252324516, the training unsup-loss is 0.027449041744275676.\n",
      "At step 1560, the training (sup)loss is 0.04795552345040517, the training unsup-loss is 0.02744975096653574.\n",
      "At step 1562, the training (sup)loss is 0.04789412073151861, the training unsup-loss is 0.02745919535890527.\n",
      "At step 1564, the training (sup)loss is 0.047832875052833804, the training unsup-loss is 0.027490609933448304.\n",
      "At step 1566, the training (sup)loss is 0.047771785812664155, the training unsup-loss is 0.027492454286788426.\n",
      "At step 1568, the training (sup)loss is 0.047710852412392896, the training unsup-loss is 0.027540563322408885.\n",
      "At step 1570, the training (sup)loss is 0.047650074256453546, the training unsup-loss is 0.027562229977742693.\n",
      "At step 1572, the training (sup)loss is 0.047589450752310476, the training unsup-loss is 0.02760088610766176.\n",
      "At step 1574, the training (sup)loss is 0.047528981310439686, the training unsup-loss is 0.027615357347168832.\n",
      "At step 1576, the training (sup)loss is 0.047468665344309685, the training unsup-loss is 0.02765688934389426.\n",
      "At step 1578, the training (sup)loss is 0.04740850227036252, the training unsup-loss is 0.027664019556831763.\n",
      "At step 1580, the training (sup)loss is 0.047589312531525575, the training unsup-loss is 0.02769991011075864.\n",
      "At step 1582, the training (sup)loss is 0.04752914905171328, the training unsup-loss is 0.02776894319682428.\n",
      "At step 1584, the training (sup)loss is 0.04746913749988031, the training unsup-loss is 0.02778846961434347.\n",
      "At step 1586, the training (sup)loss is 0.0474092773012676, the training unsup-loss is 0.027791045050043966.\n",
      "At step 1588, the training (sup)loss is 0.047349567884011595, the training unsup-loss is 0.02786904901690706.\n",
      "At step 1590, the training (sup)loss is 0.04729000867912604, the training unsup-loss is 0.02790106282915053.\n",
      "At step 1592, the training (sup)loss is 0.047230599120483927, the training unsup-loss is 0.027978355624834684.\n",
      "At step 1594, the training (sup)loss is 0.047171338644799506, the training unsup-loss is 0.02799181356486237.\n",
      "At step 1596, the training (sup)loss is 0.04711222669161053, the training unsup-loss is 0.028033710339860546.\n",
      "At step 1598, the training (sup)loss is 0.04705326270326058, the training unsup-loss is 0.02808260903268288.\n",
      "At step 1600, the training (sup)loss is 0.0469944461248815, the training unsup-loss is 0.02808333520719316.\n",
      "At step 1602, the training (sup)loss is 0.04693577640437604, the training unsup-loss is 0.028089748393772134.\n",
      "At step 1604, the training (sup)loss is 0.0468772529924005, the training unsup-loss is 0.028117550744273308.\n",
      "At step 1606, the training (sup)loss is 0.0468188753423477, the training unsup-loss is 0.02813324091519933.\n",
      "At step 1608, the training (sup)loss is 0.04676064291032986, the training unsup-loss is 0.02815056274197903.\n",
      "At step 1610, the training (sup)loss is 0.046702555155161744, the training unsup-loss is 0.02817381782434094.\n",
      "At step 1612, the training (sup)loss is 0.04664461153834393, the training unsup-loss is 0.028198006618929745.\n",
      "At step 1614, the training (sup)loss is 0.046586811524046104, the training unsup-loss is 0.028215218398286102.\n",
      "At step 1616, the training (sup)loss is 0.0465291545790906, the training unsup-loss is 0.028226373196777767.\n",
      "At step 1618, the training (sup)loss is 0.04647164017293598, the training unsup-loss is 0.028267873935194223.\n",
      "At step 1620, the training (sup)loss is 0.04641426777766074, the training unsup-loss is 0.028288979765694635.\n",
      "At step 1622, the training (sup)loss is 0.04635703686794723, the training unsup-loss is 0.028312247815517808.\n",
      "At step 1624, the training (sup)loss is 0.046299946921065524, the training unsup-loss is 0.02835228839795119.\n",
      "At step 1626, the training (sup)loss is 0.04624299741685757, the training unsup-loss is 0.028377476607095383.\n",
      "At step 1628, the training (sup)loss is 0.04618618783772138, the training unsup-loss is 0.028396293547814505.\n",
      "At step 1630, the training (sup)loss is 0.04612951766859534, the training unsup-loss is 0.02843136549898726.\n",
      "At step 1632, the training (sup)loss is 0.046072986396942656, the training unsup-loss is 0.02845967797660634.\n",
      "At step 1634, the training (sup)loss is 0.04601659351273587, the training unsup-loss is 0.0284954131925148.\n",
      "At step 1636, the training (sup)loss is 0.04596033850844157, the training unsup-loss is 0.028527504518004204.\n",
      "At step 1638, the training (sup)loss is 0.045904220879005135, the training unsup-loss is 0.028559064382848685.\n",
      "At step 1640, the training (sup)loss is 0.045848240121835616, the training unsup-loss is 0.02856836386831341.\n",
      "At step 1642, the training (sup)loss is 0.045792395736790746, the training unsup-loss is 0.028580069489182677.\n",
      "At step 1644, the training (sup)loss is 0.04573668722616205, the training unsup-loss is 0.02862419454673183.\n",
      "At step 1646, the training (sup)loss is 0.04568111409466003, the training unsup-loss is 0.028670732768757194.\n",
      "At step 1648, the training (sup)loss is 0.04562567584939952, the training unsup-loss is 0.02870704651102695.\n",
      "At step 1650, the training (sup)loss is 0.0455703719998851, the training unsup-loss is 0.028715444264199697.\n",
      "At step 1652, the training (sup)loss is 0.04551520205799662, the training unsup-loss is 0.028760774593831875.\n",
      "At step 1654, the training (sup)loss is 0.045460165537974856, the training unsup-loss is 0.028779098929042287.\n",
      "At step 1656, the training (sup)loss is 0.045405261956407254, the training unsup-loss is 0.028845698001866503.\n",
      "At step 1658, the training (sup)loss is 0.045350490832213756, the training unsup-loss is 0.028884354770403756.\n",
      "At step 1660, the training (sup)loss is 0.045295851686632777, the training unsup-loss is 0.028904871275109878.\n",
      "At step 1662, the training (sup)loss is 0.045241344043207225, the training unsup-loss is 0.028919329476194227.\n",
      "At step 1664, the training (sup)loss is 0.04536048038146244, the training unsup-loss is 0.02895165162660235.\n",
      "At step 1666, the training (sup)loss is 0.045521991343057455, the training unsup-loss is 0.028981678963203927.\n",
      "For epoch 1, the mean sup loss is: 0.045494683609798275, and accuracy is: 0.9895645976066589.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|     | 1/2 [16:08<16:08, 968.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8119999766349792.\n",
      "\n",
      "At step 2, the training (sup)loss is 0.0, the training unsup-loss is 0.037479495629668236.\n",
      "At step 4, the training (sup)loss is 0.0, the training unsup-loss is 0.048472246155142784.\n",
      "At step 6, the training (sup)loss is 0.0, the training unsup-loss is 0.04943556276460489.\n",
      "At step 8, the training (sup)loss is 0.0, the training unsup-loss is 0.04904877534136176.\n",
      "At step 10, the training (sup)loss is 0.0, the training unsup-loss is 0.05183429643511772.\n",
      "At step 12, the training (sup)loss is 0.0, the training unsup-loss is 0.05256942007690668.\n",
      "At step 14, the training (sup)loss is 0.0, the training unsup-loss is 0.052746785538537164.\n",
      "At step 16, the training (sup)loss is 0.0, the training unsup-loss is 0.05023617227561772.\n",
      "At step 18, the training (sup)loss is 0.0, the training unsup-loss is 0.05149162126084169.\n",
      "At step 20, the training (sup)loss is 0.0, the training unsup-loss is 0.050935244746506216.\n",
      "At step 22, the training (sup)loss is 0.0, the training unsup-loss is 0.051273589953780174.\n",
      "At step 24, the training (sup)loss is 0.0, the training unsup-loss is 0.051452208465586104.\n",
      "At step 26, the training (sup)loss is 0.0, the training unsup-loss is 0.050490754369932875.\n",
      "At step 28, the training (sup)loss is 0.0, the training unsup-loss is 0.05062699244756784.\n",
      "At step 30, the training (sup)loss is 0.0, the training unsup-loss is 0.050501283071935174.\n",
      "At step 32, the training (sup)loss is 0.0, the training unsup-loss is 0.050351013138424605.\n",
      "At step 34, the training (sup)loss is 0.0, the training unsup-loss is 0.0499117792419651.\n",
      "At step 36, the training (sup)loss is 0.0, the training unsup-loss is 0.05077071048112379.\n",
      "At step 38, the training (sup)loss is 0.0, the training unsup-loss is 0.050992078834066264.\n",
      "At step 40, the training (sup)loss is 0.0, the training unsup-loss is 0.05162065769545734.\n",
      "At step 42, the training (sup)loss is 0.0, the training unsup-loss is 0.05169851878391845.\n",
      "At step 44, the training (sup)loss is 0.0, the training unsup-loss is 0.05187731900845061.\n",
      "At step 46, the training (sup)loss is 0.0, the training unsup-loss is 0.05203762460176064.\n",
      "At step 48, the training (sup)loss is 0.0, the training unsup-loss is 0.052086184228149555.\n",
      "At step 50, the training (sup)loss is 0.0, the training unsup-loss is 0.052191428281366824.\n",
      "At step 52, the training (sup)loss is 0.0, the training unsup-loss is 0.0523845492145763.\n",
      "At step 54, the training (sup)loss is 0.0, the training unsup-loss is 0.05285249219310505.\n",
      "At step 56, the training (sup)loss is 0.0, the training unsup-loss is 0.053332982884187786.\n",
      "At step 58, the training (sup)loss is 0.0, the training unsup-loss is 0.05261200977938956.\n",
      "At step 60, the training (sup)loss is 0.0, the training unsup-loss is 0.05264154737815261.\n",
      "At step 62, the training (sup)loss is 0.0, the training unsup-loss is 0.05268170813759489.\n",
      "At step 64, the training (sup)loss is 0.0, the training unsup-loss is 0.05286020241328515.\n",
      "At step 66, the training (sup)loss is 0.0, the training unsup-loss is 0.052935490074257054.\n",
      "At step 68, the training (sup)loss is 0.0, the training unsup-loss is 0.052815747365136355.\n",
      "At step 70, the training (sup)loss is 0.0, the training unsup-loss is 0.052485726800348076.\n",
      "At step 72, the training (sup)loss is 0.0, the training unsup-loss is 0.05183535910004543.\n",
      "At step 74, the training (sup)loss is 0.0, the training unsup-loss is 0.05151503047326932.\n",
      "At step 76, the training (sup)loss is 0.0, the training unsup-loss is 0.05156469004424779.\n",
      "At step 78, the training (sup)loss is 0.0, the training unsup-loss is 0.05131852199347355.\n",
      "At step 80, the training (sup)loss is 0.0, the training unsup-loss is 0.05153326110448688.\n",
      "At step 82, the training (sup)loss is 0.0, the training unsup-loss is 0.05117875017316603.\n",
      "At step 84, the training (sup)loss is 0.0, the training unsup-loss is 0.05087627898458214.\n",
      "At step 86, the training (sup)loss is 0.0, the training unsup-loss is 0.05045862543548262.\n",
      "At step 88, the training (sup)loss is 0.0, the training unsup-loss is 0.05047822280109606.\n",
      "At step 90, the training (sup)loss is 0.0, the training unsup-loss is 0.05020720853159825.\n",
      "At step 92, the training (sup)loss is 0.0, the training unsup-loss is 0.04987896053606401.\n",
      "At step 94, the training (sup)loss is 0.0, the training unsup-loss is 0.04949491734596643.\n",
      "At step 96, the training (sup)loss is 0.0, the training unsup-loss is 0.04926265987645214.\n",
      "At step 98, the training (sup)loss is 0.0, the training unsup-loss is 0.04958033594017734.\n",
      "At step 100, the training (sup)loss is 0.0, the training unsup-loss is 0.04976583166047931.\n",
      "At step 102, the training (sup)loss is 0.0, the training unsup-loss is 0.04966876770862762.\n",
      "At step 104, the training (sup)loss is 0.0, the training unsup-loss is 0.04929609324496526.\n",
      "At step 106, the training (sup)loss is 0.0, the training unsup-loss is 0.049331451608043794.\n",
      "At step 108, the training (sup)loss is 0.0, the training unsup-loss is 0.04909340856182906.\n",
      "At step 110, the training (sup)loss is 0.0, the training unsup-loss is 0.048739260926165366.\n",
      "At step 112, the training (sup)loss is 0.0, the training unsup-loss is 0.04868716232678188.\n",
      "At step 114, the training (sup)loss is 0.0, the training unsup-loss is 0.048856290138996485.\n",
      "At step 116, the training (sup)loss is 0.0, the training unsup-loss is 0.048718959333953156.\n",
      "At step 118, the training (sup)loss is 0.0, the training unsup-loss is 0.04842567639583248.\n",
      "At step 120, the training (sup)loss is 0.0, the training unsup-loss is 0.048253313172608614.\n",
      "At step 122, the training (sup)loss is 0.0, the training unsup-loss is 0.04819920259054567.\n",
      "At step 124, the training (sup)loss is 0.0, the training unsup-loss is 0.0480885925672708.\n",
      "At step 126, the training (sup)loss is 0.0, the training unsup-loss is 0.048053580883239945.\n",
      "At step 128, the training (sup)loss is 0.0, the training unsup-loss is 0.048096033540787175.\n",
      "At step 130, the training (sup)loss is 0.0, the training unsup-loss is 0.047798394684035045.\n",
      "At step 132, the training (sup)loss is 0.002115032889626243, the training unsup-loss is 0.047699724225270926.\n",
      "At step 134, the training (sup)loss is 0.0020834652345571945, the training unsup-loss is 0.047899939634128294.\n",
      "At step 136, the training (sup)loss is 0.0020528260399313536, the training unsup-loss is 0.04780928424888236.\n",
      "At step 138, the training (sup)loss is 0.0020230749379033628, the training unsup-loss is 0.04783616769734932.\n",
      "At step 140, the training (sup)loss is 0.001994173867361886, the training unsup-loss is 0.047802531679293936.\n",
      "At step 142, the training (sup)loss is 0.00196608691148355, the training unsup-loss is 0.048004004015574156.\n",
      "At step 144, the training (sup)loss is 0.001938780148824056, the training unsup-loss is 0.04785508388239476.\n",
      "At step 146, the training (sup)loss is 0.0037231157495550915, the training unsup-loss is 0.047555488228083476.\n",
      "At step 148, the training (sup)loss is 0.0036728033745611036, the training unsup-loss is 0.047747867345507886.\n",
      "At step 150, the training (sup)loss is 0.003623832662900289, the training unsup-loss is 0.04779987785965204.\n",
      "At step 152, the training (sup)loss is 0.003576150654177917, the training unsup-loss is 0.04793917849708937.\n",
      "At step 154, the training (sup)loss is 0.003529707139188593, the training unsup-loss is 0.048381940751977555.\n",
      "At step 156, the training (sup)loss is 0.0034844544835579703, the training unsup-loss is 0.048349432133806824.\n",
      "At step 158, the training (sup)loss is 0.0034403474647787553, the training unsup-loss is 0.048275223791693585.\n",
      "At step 160, the training (sup)loss is 0.0033973431214690208, the training unsup-loss is 0.04813263834221289.\n",
      "At step 162, the training (sup)loss is 0.0033554006137965636, the training unsup-loss is 0.04824358308802784.\n",
      "At step 164, the training (sup)loss is 0.003314481094116118, the training unsup-loss is 0.04838443364630022.\n",
      "At step 166, the training (sup)loss is 0.0032745475869580924, the training unsup-loss is 0.04816174071775862.\n",
      "At step 168, the training (sup)loss is 0.003235564877589544, the training unsup-loss is 0.048083113972097635.\n",
      "At step 170, the training (sup)loss is 0.003197499408441431, the training unsup-loss is 0.04805360403569306.\n",
      "At step 172, the training (sup)loss is 0.00316031918276188, the training unsup-loss is 0.048006447963416576.\n",
      "At step 174, the training (sup)loss is 0.003123993674914042, the training unsup-loss is 0.04808451577849772.\n",
      "At step 176, the training (sup)loss is 0.003088493746790019, the training unsup-loss is 0.048371007327329026.\n",
      "At step 178, the training (sup)loss is 0.003053791569859794, the training unsup-loss is 0.04841570451520802.\n",
      "At step 180, the training (sup)loss is 0.0030198605524169074, the training unsup-loss is 0.04842788169367446.\n",
      "At step 182, the training (sup)loss is 0.0029866752716211174, the training unsup-loss is 0.04837373195850587.\n",
      "At step 184, the training (sup)loss is 0.0029542114099730616, the training unsup-loss is 0.048691398119958845.\n",
      "At step 186, the training (sup)loss is 0.0029224456958873296, the training unsup-loss is 0.048712384115944626.\n",
      "At step 188, the training (sup)loss is 0.0028913558480587413, the training unsup-loss is 0.048863108032085795.\n",
      "At step 190, the training (sup)loss is 0.0028609205233423333, the training unsup-loss is 0.04900140650570393.\n",
      "At step 192, the training (sup)loss is 0.0028311192678908506, the training unsup-loss is 0.04905149550177157.\n",
      "At step 194, the training (sup)loss is 0.002801932471314656, the training unsup-loss is 0.049039926341513995.\n",
      "At step 196, the training (sup)loss is 0.004093205442233961, the training unsup-loss is 0.049040182816738984.\n",
      "At step 198, the training (sup)loss is 0.004051859932716446, the training unsup-loss is 0.048953125347392726.\n",
      "At step 200, the training (sup)loss is 0.004011341333389282, the training unsup-loss is 0.048927202504128214.\n",
      "At step 202, the training (sup)loss is 0.0039716250825636455, the training unsup-loss is 0.048964913215230006.\n",
      "At step 204, the training (sup)loss is 0.0039326875817541984, the training unsup-loss is 0.04882316649252293.\n",
      "At step 206, the training (sup)loss is 0.003894506148921633, the training unsup-loss is 0.04898988605630629.\n",
      "At step 208, the training (sup)loss is 0.0038570589744127714, the training unsup-loss is 0.04885781723826837.\n",
      "At step 210, the training (sup)loss is 0.003820325079418364, the training unsup-loss is 0.04897969094592901.\n",
      "At step 212, the training (sup)loss is 0.0037842842767823416, the training unsup-loss is 0.04906955488764171.\n",
      "At step 214, the training (sup)loss is 0.003748917134008675, the training unsup-loss is 0.049181600240555325.\n",
      "At step 216, the training (sup)loss is 0.0037142049383234095, the training unsup-loss is 0.04935906400593618.\n",
      "At step 218, the training (sup)loss is 0.003680129663659892, the training unsup-loss is 0.04924991330422393.\n",
      "At step 220, the training (sup)loss is 0.003646673939444802, the training unsup-loss is 0.049264855310320856.\n",
      "At step 222, the training (sup)loss is 0.0036138210210714256, the training unsup-loss is 0.04915880813758384.\n",
      "At step 224, the training (sup)loss is 0.003581554761954716, the training unsup-loss is 0.04912401844714103.\n",
      "At step 226, the training (sup)loss is 0.0035498595870701614, the training unsup-loss is 0.049423337947016796.\n",
      "At step 228, the training (sup)loss is 0.0035187204678853354, the training unsup-loss is 0.04952830214282138.\n",
      "At step 230, the training (sup)loss is 0.0034881228985993756, the training unsup-loss is 0.04947465691391541.\n",
      "At step 232, the training (sup)loss is 0.00345805287361145, the training unsup-loss is 0.04957102402113378.\n",
      "At step 234, the training (sup)loss is 0.0034284968661446855, the training unsup-loss is 0.04966837547432918.\n",
      "At step 236, the training (sup)loss is 0.003399441807957019, the training unsup-loss is 0.04975163807966194.\n",
      "At step 238, the training (sup)loss is 0.003370875070075027, the training unsup-loss is 0.0501162776754809.\n",
      "At step 240, the training (sup)loss is 0.0033427844444910686, the training unsup-loss is 0.050283329739856227.\n",
      "At step 242, the training (sup)loss is 0.003315158126768002, the training unsup-loss is 0.05038019011859313.\n",
      "At step 244, the training (sup)loss is 0.003287984699499412, the training unsup-loss is 0.05048482020034409.\n",
      "At step 246, the training (sup)loss is 0.0032612531165766524, the training unsup-loss is 0.050430540926754475.\n",
      "At step 248, the training (sup)loss is 0.003234952688217163, the training unsup-loss is 0.05047011702141214.\n",
      "At step 250, the training (sup)loss is 0.0032090730667114257, the training unsup-loss is 0.050519363887608054.\n",
      "At step 252, the training (sup)loss is 0.0031836042328486367, the training unsup-loss is 0.05058083100067008.\n",
      "At step 254, the training (sup)loss is 0.003158536482983687, the training unsup-loss is 0.050628848087893225.\n",
      "At step 256, the training (sup)loss is 0.0031338604167103767, the training unsup-loss is 0.05087350373651134.\n",
      "At step 258, the training (sup)loss is 0.0031095669251079705, the training unsup-loss is 0.05086147957734128.\n",
      "At step 260, the training (sup)loss is 0.003085647179530217, the training unsup-loss is 0.05091543784365058.\n",
      "At step 262, the training (sup)loss is 0.003062092620907849, the training unsup-loss is 0.05081076713150922.\n",
      "At step 264, the training (sup)loss is 0.003038894949537335, the training unsup-loss is 0.05083736838688227.\n",
      "At step 266, the training (sup)loss is 0.004233348750530329, the training unsup-loss is 0.050665897870422305.\n",
      "At step 268, the training (sup)loss is 0.004201756595675625, the training unsup-loss is 0.05074263067999438.\n",
      "At step 270, the training (sup)loss is 0.004170632472744695, the training unsup-loss is 0.050695914788930506.\n",
      "At step 272, the training (sup)loss is 0.0041399660575039245, the training unsup-loss is 0.050690795116893506.\n",
      "At step 274, the training (sup)loss is 0.004109747327157181, the training unsup-loss is 0.05067112644876007.\n",
      "At step 276, the training (sup)loss is 0.004079966549424158, the training unsup-loss is 0.05066789802757726.\n",
      "At step 278, the training (sup)loss is 0.004050614272090171, the training unsup-loss is 0.050767428636979715.\n",
      "At step 280, the training (sup)loss is 0.004021681313003812, the training unsup-loss is 0.05094993856868574.\n",
      "At step 282, the training (sup)loss is 0.003993158750500239, the training unsup-loss is 0.051134994451669934.\n",
      "At step 284, the training (sup)loss is 0.003965037914229111, the training unsup-loss is 0.051232208261712334.\n",
      "At step 286, the training (sup)loss is 0.003937310376367369, the training unsup-loss is 0.051348756894573466.\n",
      "At step 288, the training (sup)loss is 0.003909967943198151, the training unsup-loss is 0.05155422242306587.\n",
      "At step 290, the training (sup)loss is 0.003883002647038164, the training unsup-loss is 0.051542827628296.\n",
      "At step 292, the training (sup)loss is 0.0038564067384968064, the training unsup-loss is 0.05143354063828106.\n",
      "At step 294, the training (sup)loss is 0.00383017267905125, the training unsup-loss is 0.05139331111595744.\n",
      "At step 296, the training (sup)loss is 0.0038042931339225252, the training unsup-loss is 0.05132223050286238.\n",
      "At step 298, the training (sup)loss is 0.0037787609652384815, the training unsup-loss is 0.051351750569555585.\n",
      "At step 300, the training (sup)loss is 0.003753569225470225, the training unsup-loss is 0.0514142981916666.\n",
      "At step 302, the training (sup)loss is 0.0037287111511293625, the training unsup-loss is 0.0512774751910212.\n",
      "At step 304, the training (sup)loss is 0.0037041801567140376, the training unsup-loss is 0.05129132618956072.\n",
      "At step 306, the training (sup)loss is 0.0036799698288923774, the training unsup-loss is 0.05123186491892424.\n",
      "At step 308, the training (sup)loss is 0.003656073920912557, the training unsup-loss is 0.051179159281460884.\n",
      "At step 310, the training (sup)loss is 0.00363248634722925, the training unsup-loss is 0.05105782893276022.\n",
      "At step 312, the training (sup)loss is 0.003609201178336755, the training unsup-loss is 0.051189402267575644.\n",
      "At step 314, the training (sup)loss is 0.003586212635799578, the training unsup-loss is 0.0510657701795553.\n",
      "At step 316, the training (sup)loss is 0.0035635150874717327, the training unsup-loss is 0.05096447565062325.\n",
      "At step 318, the training (sup)loss is 0.004272255929386091, the training unsup-loss is 0.05092603511194575.\n",
      "At step 320, the training (sup)loss is 0.004245554329827428, the training unsup-loss is 0.050958354858448726.\n",
      "At step 322, the training (sup)loss is 0.004219184427157692, the training unsup-loss is 0.050888512728978756.\n",
      "At step 324, the training (sup)loss is 0.004193140078841904, the training unsup-loss is 0.05093047652324593.\n",
      "At step 326, the training (sup)loss is 0.004167415293082137, the training unsup-loss is 0.05098333717695226.\n",
      "At step 328, the training (sup)loss is 0.004884630305374541, the training unsup-loss is 0.05095904842359809.\n",
      "At step 330, the training (sup)loss is 0.004855026485341968, the training unsup-loss is 0.05090517450801351.\n",
      "At step 332, the training (sup)loss is 0.004825779337839908, the training unsup-loss is 0.050917845460619914.\n",
      "At step 334, the training (sup)loss is 0.004796882455577393, the training unsup-loss is 0.05095470526595851.\n",
      "At step 336, the training (sup)loss is 0.004768329583818004, the training unsup-loss is 0.050957073363298105.\n",
      "At step 338, the training (sup)loss is 0.004740114615866419, the training unsup-loss is 0.05105868701068078.\n",
      "At step 340, the training (sup)loss is 0.004712231588714263, the training unsup-loss is 0.05110153675846317.\n",
      "At step 342, the training (sup)loss is 0.004684674678838741, the training unsup-loss is 0.05099908115984927.\n",
      "At step 344, the training (sup)loss is 0.004657438198147818, the training unsup-loss is 0.050998417741813976.\n",
      "At step 346, the training (sup)loss is 0.004630516590066039, the training unsup-loss is 0.05095532936490857.\n",
      "At step 348, the training (sup)loss is 0.0046039044257553145, the training unsup-loss is 0.05083168752962488.\n",
      "At step 350, the training (sup)loss is 0.004577596400465284, the training unsup-loss is 0.05087895595601627.\n",
      "At step 352, the training (sup)loss is 0.004551587330008095, the training unsup-loss is 0.0508934437318451.\n",
      "At step 354, the training (sup)loss is 0.004525872147352682, the training unsup-loss is 0.0508423175789037.\n",
      "At step 356, the training (sup)loss is 0.004500445899333847, the training unsup-loss is 0.05065348712214677.\n",
      "At step 358, the training (sup)loss is 0.004475303743471646, the training unsup-loss is 0.05063172286459604.\n",
      "At step 360, the training (sup)loss is 0.005100767314434052, the training unsup-loss is 0.05060976063056539.\n",
      "At step 362, the training (sup)loss is 0.005072586279547675, the training unsup-loss is 0.050725205973799224.\n",
      "At step 364, the training (sup)loss is 0.005044714926363348, the training unsup-loss is 0.05080777790735138.\n",
      "At step 366, the training (sup)loss is 0.005017148178131854, the training unsup-loss is 0.05082987584021004.\n",
      "At step 368, the training (sup)loss is 0.004989881068468094, the training unsup-loss is 0.05114930264054514.\n",
      "At step 370, the training (sup)loss is 0.004962908738368266, the training unsup-loss is 0.051303349528461696.\n",
      "At step 372, the training (sup)loss is 0.004936226433323276, the training unsup-loss is 0.051642654556280344.\n",
      "At step 374, the training (sup)loss is 0.004909829500524755, the training unsup-loss is 0.05181341934301716.\n",
      "At step 376, the training (sup)loss is 0.004883713386160262, the training unsup-loss is 0.05206835001677354.\n",
      "At step 378, the training (sup)loss is 0.004857873632794335, the training unsup-loss is 0.052279055544801015.\n",
      "At step 380, the training (sup)loss is 0.004832305876832259, the training unsup-loss is 0.052443288671048846.\n",
      "At step 382, the training (sup)loss is 0.0048070058460635045, the training unsup-loss is 0.05268473130606934.\n",
      "At step 384, the training (sup)loss is 0.004781969357281923, the training unsup-loss is 0.05271749149202757.\n",
      "At step 386, the training (sup)loss is 0.004757192313979945, the training unsup-loss is 0.05296549499469177.\n",
      "At step 388, the training (sup)loss is 0.004732670704114069, the training unsup-loss is 0.053141202693613075.\n",
      "At step 390, the training (sup)loss is 0.004708400597939124, the training unsup-loss is 0.05329997076485784.\n",
      "At step 392, the training (sup)loss is 0.004684378145908823, the training unsup-loss is 0.053547216857289325.\n",
      "At step 394, the training (sup)loss is 0.00466059957664025, the training unsup-loss is 0.05376489620495039.\n",
      "At step 396, the training (sup)loss is 0.004637061194940047, the training unsup-loss is 0.05371571623136976.\n",
      "At step 398, the training (sup)loss is 0.004613759379890097, the training unsup-loss is 0.053613174428236696.\n",
      "At step 400, the training (sup)loss is 0.004590690582990647, the training unsup-loss is 0.053710326899308714.\n",
      "At step 402, the training (sup)loss is 0.004567851326358852, the training unsup-loss is 0.05374775578809986.\n",
      "At step 404, the training (sup)loss is 0.004545238200980838, the training unsup-loss is 0.0539180254125691.\n",
      "At step 406, the training (sup)loss is 0.004522847865015415, the training unsup-loss is 0.05400351161742812.\n",
      "At step 408, the training (sup)loss is 0.004500677042147692, the training unsup-loss is 0.053993223986004056.\n",
      "At step 410, the training (sup)loss is 0.004478722519990874, the training unsup-loss is 0.054023089716438114.\n",
      "At step 412, the training (sup)loss is 0.004456981148534608, the training unsup-loss is 0.05403594974003443.\n",
      "At step 414, the training (sup)loss is 0.004435449838638306, the training unsup-loss is 0.05414965559148054.\n",
      "At step 416, the training (sup)loss is 0.004414125560567929, the training unsup-loss is 0.054219415959848374.\n",
      "At step 418, the training (sup)loss is 0.004393005342574781, the training unsup-loss is 0.05429767609827898.\n",
      "At step 420, the training (sup)loss is 0.004372086269514902, the training unsup-loss is 0.054318968741045824.\n",
      "At step 422, the training (sup)loss is 0.004351365481507722, the training unsup-loss is 0.05434850530292821.\n",
      "At step 424, the training (sup)loss is 0.004330840172632685, the training unsup-loss is 0.05446557621779096.\n",
      "At step 426, the training (sup)loss is 0.004310507589662579, the training unsup-loss is 0.05458833153267771.\n",
      "At step 428, the training (sup)loss is 0.00429036503083238, the training unsup-loss is 0.054603988193758876.\n",
      "At step 430, the training (sup)loss is 0.004270409844642462, the training unsup-loss is 0.054602388234072645.\n",
      "At step 432, the training (sup)loss is 0.004250639428695043, the training unsup-loss is 0.054701484767806334.\n",
      "At step 434, the training (sup)loss is 0.004231051228562808, the training unsup-loss is 0.054746075093420006.\n",
      "At step 436, the training (sup)loss is 0.0042116427366886665, the training unsup-loss is 0.05469004764785529.\n",
      "At step 438, the training (sup)loss is 0.004192411491315658, the training unsup-loss is 0.054687219482663696.\n",
      "At step 440, the training (sup)loss is 0.004173355075446042, the training unsup-loss is 0.054727706340649586.\n",
      "At step 442, the training (sup)loss is 0.004154471115828639, the training unsup-loss is 0.054645775865797136.\n",
      "At step 444, the training (sup)loss is 0.0045955268380878204, the training unsup-loss is 0.05461823767646812.\n",
      "At step 446, the training (sup)loss is 0.004574919094419265, the training unsup-loss is 0.05450009034462106.\n",
      "At step 448, the training (sup)loss is 0.0045544953484620366, the training unsup-loss is 0.054450193239193014.\n",
      "At step 450, the training (sup)loss is 0.004534253146913317, the training unsup-loss is 0.054478145914359226.\n",
      "At step 452, the training (sup)loss is 0.004514190079891576, the training unsup-loss is 0.054450100471174954.\n",
      "At step 454, the training (sup)loss is 0.004494303779980159, the training unsup-loss is 0.05451749786447389.\n",
      "At step 456, the training (sup)loss is 0.004474591921296036, the training unsup-loss is 0.05458171776160924.\n",
      "At step 458, the training (sup)loss is 0.004455052218582953, the training unsup-loss is 0.054517861217764654.\n",
      "At step 460, the training (sup)loss is 0.004435682426328244, the training unsup-loss is 0.05452363772682198.\n",
      "At step 462, the training (sup)loss is 0.004416480337902581, the training unsup-loss is 0.054710251119923156.\n",
      "At step 464, the training (sup)loss is 0.004397443784721966, the training unsup-loss is 0.05474428951154174.\n",
      "At step 466, the training (sup)loss is 0.004378570635431314, the training unsup-loss is 0.05481419310797118.\n",
      "At step 468, the training (sup)loss is 0.004359858795108958, the training unsup-loss is 0.05488370451081194.\n",
      "At step 470, the training (sup)loss is 0.004341306204491473, the training unsup-loss is 0.0549985669612726.\n",
      "At step 472, the training (sup)loss is 0.004322910839218205, the training unsup-loss is 0.05499869229674529.\n",
      "At step 474, the training (sup)loss is 0.004304670709094921, the training unsup-loss is 0.054951818634359016.\n",
      "At step 476, the training (sup)loss is 0.004286583857376035, the training unsup-loss is 0.054932724236424604.\n",
      "At step 478, the training (sup)loss is 0.00468999842594857, the training unsup-loss is 0.05490984313119842.\n",
      "At step 480, the training (sup)loss is 0.004670456765840451, the training unsup-loss is 0.05492246685510812.\n",
      "At step 482, the training (sup)loss is 0.0046510772771855115, the training unsup-loss is 0.05494066448186628.\n",
      "At step 484, the training (sup)loss is 0.0046318579495938355, the training unsup-loss is 0.05501313627435833.\n",
      "At step 486, the training (sup)loss is 0.004612796805768347, the training unsup-loss is 0.05508558738616452.\n",
      "At step 488, the training (sup)loss is 0.004593891900826673, the training unsup-loss is 0.055123522608602024.\n",
      "At step 490, the training (sup)loss is 0.004575141321639625, the training unsup-loss is 0.05513661027676901.\n",
      "At step 492, the training (sup)loss is 0.004556543186185806, the training unsup-loss is 0.05521065637098277.\n",
      "At step 494, the training (sup)loss is 0.004538095642921895, the training unsup-loss is 0.055422837025889383.\n",
      "At step 496, the training (sup)loss is 0.004519796870168178, the training unsup-loss is 0.05551495939125157.\n",
      "At step 498, the training (sup)loss is 0.0045016450755088686, the training unsup-loss is 0.05556574435979427.\n",
      "At step 500, the training (sup)loss is 0.004483638495206833, the training unsup-loss is 0.05570365740917623.\n",
      "At step 502, the training (sup)loss is 0.004465775393632304, the training unsup-loss is 0.05568126919846314.\n",
      "At step 504, the training (sup)loss is 0.0048447657849580526, the training unsup-loss is 0.055713555470685516.\n",
      "At step 506, the training (sup)loss is 0.004825616513080748, the training unsup-loss is 0.05596314151789772.\n",
      "At step 508, the training (sup)loss is 0.004806618022871768, the training unsup-loss is 0.05606811894580546.\n",
      "At step 510, the training (sup)loss is 0.004787768540429134, the training unsup-loss is 0.05612757833236281.\n",
      "At step 512, the training (sup)loss is 0.004769066319568083, the training unsup-loss is 0.05621908578723378.\n",
      "At step 514, the training (sup)loss is 0.004750509641281825, the training unsup-loss is 0.05630457501111965.\n",
      "At step 516, the training (sup)loss is 0.004732096813214842, the training unsup-loss is 0.056313704408690796.\n",
      "At step 518, the training (sup)loss is 0.004713826169148375, the training unsup-loss is 0.056366605854963586.\n",
      "At step 520, the training (sup)loss is 0.004695696068497805, the training unsup-loss is 0.056309263363408926.\n",
      "At step 522, the training (sup)loss is 0.0046777048958215675, the training unsup-loss is 0.05643389219689358.\n",
      "At step 524, the training (sup)loss is 0.004659851060341333, the training unsup-loss is 0.05653091072005341.\n",
      "At step 526, the training (sup)loss is 0.004642132995473115, the training unsup-loss is 0.05647957122250825.\n",
      "At step 528, the training (sup)loss is 0.00462454915836905, the training unsup-loss is 0.05660890134326606.\n",
      "At step 530, the training (sup)loss is 0.0046070980294695444, the training unsup-loss is 0.05670816886924067.\n",
      "At step 532, the training (sup)loss is 0.004589778112065524, the training unsup-loss is 0.05664896204388399.\n",
      "At step 534, the training (sup)loss is 0.004572587931870521, the training unsup-loss is 0.056784039326639994.\n",
      "At step 536, the training (sup)loss is 0.004555526036602348, the training unsup-loss is 0.05684801041489161.\n",
      "At step 538, the training (sup)loss is 0.005403607668265091, the training unsup-loss is 0.056859718128471015.\n",
      "At step 540, the training (sup)loss is 0.005383594306530776, the training unsup-loss is 0.05687255085859862.\n",
      "At step 542, the training (sup)loss is 0.005363728644883061, the training unsup-loss is 0.05701767092886831.\n",
      "At step 544, the training (sup)loss is 0.005344009054276873, the training unsup-loss is 0.05714032740626648.\n",
      "At step 546, the training (sup)loss is 0.0053244339295359325, the training unsup-loss is 0.05733830236655834.\n",
      "At step 548, the training (sup)loss is 0.005305001688917188, the training unsup-loss is 0.057474208225829215.\n",
      "At step 550, the training (sup)loss is 0.005285710773684762, the training unsup-loss is 0.05748912478543141.\n",
      "At step 552, the training (sup)loss is 0.0052665596476931505, the training unsup-loss is 0.057614543088846774.\n",
      "At step 554, the training (sup)loss is 0.005247546796979457, the training unsup-loss is 0.05772872323452727.\n",
      "At step 556, the training (sup)loss is 0.0052286707293644225, the training unsup-loss is 0.0578296539832657.\n",
      "At step 558, the training (sup)loss is 0.005209929974062041, the training unsup-loss is 0.058010277119682145.\n",
      "At step 560, the training (sup)loss is 0.005191323081297534, the training unsup-loss is 0.05812457536979179.\n",
      "At step 562, the training (sup)loss is 0.0051728486219334855, the training unsup-loss is 0.058353787041153245.\n",
      "At step 564, the training (sup)loss is 0.005154505187103934, the training unsup-loss is 0.05855668951585519.\n",
      "At step 566, the training (sup)loss is 0.005136291387856217, the training unsup-loss is 0.058654794421757805.\n",
      "At step 568, the training (sup)loss is 0.005118205854800386, the training unsup-loss is 0.05877487554746142.\n",
      "At step 570, the training (sup)loss is 0.005100247237765998, the training unsup-loss is 0.05881318979847588.\n",
      "At step 572, the training (sup)loss is 0.005082414205466117, the training unsup-loss is 0.05904581812470891.\n",
      "At step 574, the training (sup)loss is 0.005064705445168326, the training unsup-loss is 0.05915124982251471.\n",
      "At step 576, the training (sup)loss is 0.005047119662372602, the training unsup-loss is 0.059341708500546195.\n",
      "At step 578, the training (sup)loss is 0.005029655580495881, the training unsup-loss is 0.05935409195740485.\n",
      "At step 580, the training (sup)loss is 0.005012311940563136, the training unsup-loss is 0.059365907842935675.\n",
      "At step 582, the training (sup)loss is 0.004995087500904843, the training unsup-loss is 0.059455447082961445.\n",
      "At step 584, the training (sup)loss is 0.004977981036860649, the training unsup-loss is 0.0594858017172792.\n",
      "At step 586, the training (sup)loss is 0.0049609913404891105, the training unsup-loss is 0.05951009393087985.\n",
      "At step 588, the training (sup)loss is 0.004944117220283366, the training unsup-loss is 0.059570630047736425.\n",
      "At step 590, the training (sup)loss is 0.004927357500892574, the training unsup-loss is 0.059634576433227726.\n",
      "At step 592, the training (sup)loss is 0.004910711022849019, the training unsup-loss is 0.059839167341124266.\n",
      "At step 594, the training (sup)loss is 0.004894176642300705, the training unsup-loss is 0.059875888321924996.\n",
      "At step 596, the training (sup)loss is 0.004877753230749361, the training unsup-loss is 0.05989208424170095.\n",
      "At step 598, the training (sup)loss is 0.004861439674793677, the training unsup-loss is 0.059988219407776426.\n",
      "At step 600, the training (sup)loss is 0.004845234875877698, the training unsup-loss is 0.06007943435727308.\n",
      "At step 602, the training (sup)loss is 0.0048291377500442175, the training unsup-loss is 0.060118345350361244.\n",
      "At step 604, the training (sup)loss is 0.004813147227693077, the training unsup-loss is 0.06012125127480075.\n",
      "At step 606, the training (sup)loss is 0.004797262253344255, the training unsup-loss is 0.06010900063919696.\n",
      "At step 608, the training (sup)loss is 0.004781481785405623, the training unsup-loss is 0.0601776092739065.\n",
      "At step 610, the training (sup)loss is 0.004765804795945277, the training unsup-loss is 0.0601386521851308.\n",
      "At step 612, the training (sup)loss is 0.004750230270468332, the training unsup-loss is 0.06021038636266942.\n",
      "At step 614, the training (sup)loss is 0.004734757207698077, the training unsup-loss is 0.06020739839445642.\n",
      "At step 616, the training (sup)loss is 0.004719384619361395, the training unsup-loss is 0.060197844984941185.\n",
      "At step 618, the training (sup)loss is 0.004704111529978348, the training unsup-loss is 0.060192795661745235.\n",
      "At step 620, the training (sup)loss is 0.004688936976655837, the training unsup-loss is 0.06017502700579503.\n",
      "At step 622, the training (sup)loss is 0.004673860008885239, the training unsup-loss is 0.06013153124918077.\n",
      "At step 624, the training (sup)loss is 0.00465887968834394, the training unsup-loss is 0.06026793466373466.\n",
      "At step 626, the training (sup)loss is 0.004643995088700669, the training unsup-loss is 0.060366001127424615.\n",
      "At step 628, the training (sup)loss is 0.004629205295424553, the training unsup-loss is 0.06043439025539834.\n",
      "At step 630, the training (sup)loss is 0.004614509405597808, the training unsup-loss is 0.06053195661820826.\n",
      "At step 632, the training (sup)loss is 0.004599906527731992, the training unsup-loss is 0.060553981325754166.\n",
      "At step 634, the training (sup)loss is 0.004585395781587727, the training unsup-loss is 0.06061114283713397.\n",
      "At step 636, the training (sup)loss is 0.004570976297997829, the training unsup-loss is 0.06074264226042016.\n",
      "At step 638, the training (sup)loss is 0.00455664721869376, the training unsup-loss is 0.060763791079419815.\n",
      "At step 640, the training (sup)loss is 0.004542407696135342, the training unsup-loss is 0.060861995902087074.\n",
      "At step 642, the training (sup)loss is 0.0045282568933436435, the training unsup-loss is 0.060903645673261904.\n",
      "At step 644, the training (sup)loss is 0.004514193983736986, the training unsup-loss is 0.06093065877248365.\n",
      "At step 646, the training (sup)loss is 0.004500218150969998, the training unsup-loss is 0.060984110769645626.\n",
      "At step 648, the training (sup)loss is 0.004486328588775647, the training unsup-loss is 0.06094063112920403.\n",
      "At step 650, the training (sup)loss is 0.004472524500810183, the training unsup-loss is 0.06095490469239079.\n",
      "At step 652, the training (sup)loss is 0.004458805100500949, the training unsup-loss is 0.0609672210262117.\n",
      "At step 654, the training (sup)loss is 0.004445169610896971, the training unsup-loss is 0.06100598679832681.\n",
      "At step 656, the training (sup)loss is 0.004431617264522285, the training unsup-loss is 0.060991465396577176.\n",
      "At step 658, the training (sup)loss is 0.004418147303231944, the training unsup-loss is 0.06099225035344893.\n",
      "At step 660, the training (sup)loss is 0.004404758978070635, the training unsup-loss is 0.061051332350876744.\n",
      "At step 662, the training (sup)loss is 0.004391451549133865, the training unsup-loss is 0.06105715458863767.\n",
      "At step 664, the training (sup)loss is 0.004631346562899739, the training unsup-loss is 0.06107010405527495.\n",
      "At step 666, the training (sup)loss is 0.004617438615263403, the training unsup-loss is 0.06104729352966398.\n",
      "At step 668, the training (sup)loss is 0.004603613948750638, the training unsup-loss is 0.061070689578780424.\n",
      "At step 670, the training (sup)loss is 0.004589871817560339, the training unsup-loss is 0.06114807811551797.\n",
      "At step 672, the training (sup)loss is 0.00457621148476998, the training unsup-loss is 0.06111821936792694.\n",
      "At step 674, the training (sup)loss is 0.004562632222203897, the training unsup-loss is 0.06112239426222992.\n",
      "At step 676, the training (sup)loss is 0.004549133310303885, the training unsup-loss is 0.06127798882831086.\n",
      "At step 678, the training (sup)loss is 0.004535714038002104, the training unsup-loss is 0.06133418526242221.\n",
      "At step 680, the training (sup)loss is 0.004522373702596215, the training unsup-loss is 0.061393393098157556.\n",
      "At step 682, the training (sup)loss is 0.0045091116096267255, the training unsup-loss is 0.061535007917100995.\n",
      "At step 684, the training (sup)loss is 0.004495927072756472, the training unsup-loss is 0.06152960208070339.\n",
      "At step 686, the training (sup)loss is 0.004482819413652225, the training unsup-loss is 0.06158030578193225.\n",
      "At step 688, the training (sup)loss is 0.004469787961868353, the training unsup-loss is 0.061607458659848416.\n",
      "At step 690, the training (sup)loss is 0.004456832054732502, the training unsup-loss is 0.06165112551652651.\n",
      "At step 692, the training (sup)loss is 0.004443951037233275, the training unsup-loss is 0.06176697014766406.\n",
      "At step 694, the training (sup)loss is 0.004431144261909836, the training unsup-loss is 0.06176128322580663.\n",
      "At step 696, the training (sup)loss is 0.004418411088743429, the training unsup-loss is 0.06177079674236519.\n",
      "At step 698, the training (sup)loss is 0.004405750885050755, the training unsup-loss is 0.06182349154480164.\n",
      "At step 700, the training (sup)loss is 0.004655580541917256, the training unsup-loss is 0.061829941255439606.\n",
      "At step 702, the training (sup)loss is 0.004885476687525073, the training unsup-loss is 0.061777982392488974.\n",
      "At step 704, the training (sup)loss is 0.0048715974923900585, the training unsup-loss is 0.06182361777146897.\n",
      "At step 706, the training (sup)loss is 0.004857796932921531, the training unsup-loss is 0.06188310983408739.\n",
      "At step 708, the training (sup)loss is 0.0048440743427155385, the training unsup-loss is 0.06184393800347833.\n",
      "At step 710, the training (sup)loss is 0.004830429062876903, the training unsup-loss is 0.0620097383476374.\n",
      "At step 712, the training (sup)loss is 0.004816860441913765, the training unsup-loss is 0.06200989989663234.\n",
      "At step 714, the training (sup)loss is 0.004803367835633895, the training unsup-loss is 0.06209740602914818.\n",
      "At step 716, the training (sup)loss is 0.004789950607042739, the training unsup-loss is 0.06216490586475579.\n",
      "At step 718, the training (sup)loss is 0.004776608126243177, the training unsup-loss is 0.062148695737694215.\n",
      "At step 720, the training (sup)loss is 0.004763339770336946, the training unsup-loss is 0.0622049459346777.\n",
      "At step 722, the training (sup)loss is 0.004750144923327702, the training unsup-loss is 0.062267904073113466.\n",
      "At step 724, the training (sup)loss is 0.004737022976025692, the training unsup-loss is 0.06228248099796474.\n",
      "At step 726, the training (sup)loss is 0.004723973325953996, the training unsup-loss is 0.062339659537812155.\n",
      "At step 728, the training (sup)loss is 0.00471099537725632, the training unsup-loss is 0.06241132684210622.\n",
      "At step 730, the training (sup)loss is 0.00492372700612839, the training unsup-loss is 0.06241195732982804.\n",
      "At step 732, the training (sup)loss is 0.0049102742001007164, the training unsup-loss is 0.06244657219570802.\n",
      "At step 734, the training (sup)loss is 0.004896894706367472, the training unsup-loss is 0.06243272505228583.\n",
      "At step 736, the training (sup)loss is 0.004883587927274082, the training unsup-loss is 0.06252285914935941.\n",
      "At step 738, the training (sup)loss is 0.004870353271644613, the training unsup-loss is 0.0625127691732654.\n",
      "At step 740, the training (sup)loss is 0.004857190154694222, the training unsup-loss is 0.06257620863490612.\n",
      "At step 742, the training (sup)loss is 0.0048440979979430246, the training unsup-loss is 0.06263577034245082.\n",
      "At step 744, the training (sup)loss is 0.00483107622913135, the training unsup-loss is 0.06263538268119377.\n",
      "At step 746, the training (sup)loss is 0.004818124282136359, the training unsup-loss is 0.06269516433706512.\n",
      "At step 748, the training (sup)loss is 0.004805241596890006, the training unsup-loss is 0.06273711246019976.\n",
      "At step 750, the training (sup)loss is 0.004792427619298299, the training unsup-loss is 0.06279380004977186.\n",
      "At step 752, the training (sup)loss is 0.004779681801161868, the training unsup-loss is 0.06283556878383449.\n",
      "At step 754, the training (sup)loss is 0.004767003600097778, the training unsup-loss is 0.06292331636085395.\n",
      "At step 756, the training (sup)loss is 0.004754392479462599, the training unsup-loss is 0.06300039010777793.\n",
      "At step 758, the training (sup)loss is 0.004741847908276681, the training unsup-loss is 0.06311113422608745.\n",
      "At step 760, the training (sup)loss is 0.004729369361149638, the training unsup-loss is 0.0632116368420324.\n",
      "At step 762, the training (sup)loss is 0.004716956318206987, the training unsup-loss is 0.06335725392681837.\n",
      "At step 764, the training (sup)loss is 0.004704608265017964, the training unsup-loss is 0.06343248264049789.\n",
      "At step 766, the training (sup)loss is 0.004692324692524444, the training unsup-loss is 0.0635298443046294.\n",
      "At step 768, the training (sup)loss is 0.004680105096970995, the training unsup-loss is 0.06359198146067986.\n",
      "At step 770, the training (sup)loss is 0.004667948979836006, the training unsup-loss is 0.06363830540159887.\n",
      "At step 772, the training (sup)loss is 0.004655855847763892, the training unsup-loss is 0.06377618916096273.\n",
      "At step 774, the training (sup)loss is 0.004643825212498352, the training unsup-loss is 0.06393027570199712.\n",
      "At step 776, the training (sup)loss is 0.004631856590816655, the training unsup-loss is 0.06394279005903673.\n",
      "At step 778, the training (sup)loss is 0.004619949504464941, the training unsup-loss is 0.06396588034674065.\n",
      "At step 780, the training (sup)loss is 0.0046081034800945185, the training unsup-loss is 0.0639290440767908.\n",
      "At step 782, the training (sup)loss is 0.0047961735092770415, the training unsup-loss is 0.06395295376072416.\n",
      "At step 784, the training (sup)loss is 0.004783938372773784, the training unsup-loss is 0.06400886854057067.\n",
      "At step 786, the training (sup)loss is 0.004771765501596242, the training unsup-loss is 0.06405588872043497.\n",
      "At step 788, the training (sup)loss is 0.004759654421642952, the training unsup-loss is 0.06405530568699205.\n",
      "At step 790, the training (sup)loss is 0.0047476046636134766, the training unsup-loss is 0.06405597655809944.\n",
      "At step 792, the training (sup)loss is 0.004735615762947786, the training unsup-loss is 0.06415606868062922.\n",
      "At step 794, the training (sup)loss is 0.004723687259766557, the training unsup-loss is 0.0642798407418508.\n",
      "At step 796, the training (sup)loss is 0.0047118186988123695, the training unsup-loss is 0.06430630306666484.\n",
      "At step 798, the training (sup)loss is 0.004700009629391788, the training unsup-loss is 0.0643956854713049.\n",
      "At step 800, the training (sup)loss is 0.004688259605318308, the training unsup-loss is 0.06445300711668096.\n",
      "At step 802, the training (sup)loss is 0.004676568184856168, the training unsup-loss is 0.06444082910687363.\n",
      "At step 804, the training (sup)loss is 0.004664934930664983, the training unsup-loss is 0.06443464223049877.\n",
      "At step 806, the training (sup)loss is 0.004653359409745219, the training unsup-loss is 0.06448543337855497.\n",
      "At step 808, the training (sup)loss is 0.004641841193384464, the training unsup-loss is 0.06450535814245555.\n",
      "At step 810, the training (sup)loss is 0.0046303798571045015, the training unsup-loss is 0.06451365555303148.\n",
      "At step 812, the training (sup)loss is 0.00461897498060917, the training unsup-loss is 0.06455815292486526.\n",
      "At step 814, the training (sup)loss is 0.0046076261477329804, the training unsup-loss is 0.06455729523655347.\n",
      "At step 816, the training (sup)loss is 0.004596332946390498, the training unsup-loss is 0.06457799745554689.\n",
      "At step 818, the training (sup)loss is 0.004585094968526463, the training unsup-loss is 0.06466234758983579.\n",
      "At step 820, the training (sup)loss is 0.004573911810066642, the training unsup-loss is 0.06468488925999803.\n",
      "At step 822, the training (sup)loss is 0.0045627830708694, the training unsup-loss is 0.06471047887052897.\n",
      "At step 824, the training (sup)loss is 0.004551708354677969, the training unsup-loss is 0.06468944402425143.\n",
      "At step 826, the training (sup)loss is 0.004540687269073421, the training unsup-loss is 0.0646634132750152.\n",
      "At step 828, the training (sup)loss is 0.0045297194254283165, the training unsup-loss is 0.06466331758878334.\n",
      "At step 830, the training (sup)loss is 0.00451880443886102, the training unsup-loss is 0.06464836166902299.\n",
      "At step 832, the training (sup)loss is 0.00450794192819068, the training unsup-loss is 0.06464889534422233.\n",
      "At step 834, the training (sup)loss is 0.004497131515892861, the training unsup-loss is 0.06463088114730281.\n",
      "At step 836, the training (sup)loss is 0.004486372828055797, the training unsup-loss is 0.06467631583895231.\n",
      "At step 838, the training (sup)loss is 0.004475665494337287, the training unsup-loss is 0.06470256724099235.\n",
      "At step 840, the training (sup)loss is 0.004465009147922198, the training unsup-loss is 0.06467069407281954.\n",
      "At step 842, the training (sup)loss is 0.004454403425480578, the training unsup-loss is 0.06465196268270212.\n",
      "At step 844, the training (sup)loss is 0.004443847967126358, the training unsup-loss is 0.06466689412658637.\n",
      "At step 846, the training (sup)loss is 0.004433342416376651, the training unsup-loss is 0.0646916941836494.\n",
      "At step 848, the training (sup)loss is 0.004422886420111611, the training unsup-loss is 0.06471884400992356.\n",
      "At step 850, the training (sup)loss is 0.004598429501056671, the training unsup-loss is 0.06479211180635235.\n",
      "At step 852, the training (sup)loss is 0.004765991611240056, the training unsup-loss is 0.06486468764892102.\n",
      "At step 854, the training (sup)loss is 0.004754830038380009, the training unsup-loss is 0.06486380315414113.\n",
      "At step 856, the training (sup)loss is 0.004743720622402485, the training unsup-loss is 0.0649283477681837.\n",
      "At step 858, the training (sup)loss is 0.0047326629985740415, the training unsup-loss is 0.06498169814429673.\n",
      "At step 860, the training (sup)loss is 0.004721656805554102, the training unsup-loss is 0.0650098279345954.\n",
      "At step 862, the training (sup)loss is 0.0047107016853556, the training unsup-loss is 0.06497546589745343.\n",
      "At step 864, the training (sup)loss is 0.004699797283306166, the training unsup-loss is 0.0650669309942276.\n",
      "At step 866, the training (sup)loss is 0.004688943248009847, the training unsup-loss is 0.06510879415222906.\n",
      "At step 868, the training (sup)loss is 0.004678139231309364, the training unsup-loss is 0.06511111983338431.\n",
      "At step 870, the training (sup)loss is 0.004667384888248882, the training unsup-loss is 0.06509985785995578.\n",
      "At step 872, the training (sup)loss is 0.004656679877037302, the training unsup-loss is 0.0651207933589013.\n",
      "At step 874, the training (sup)loss is 0.0046460238590120455, the training unsup-loss is 0.06517464613107202.\n",
      "At step 876, the training (sup)loss is 0.004635416498603342, the training unsup-loss is 0.06522315078984826.\n",
      "At step 878, the training (sup)loss is 0.004624857463299006, the training unsup-loss is 0.0652859636883682.\n",
      "At step 880, the training (sup)loss is 0.00461434642360969, the training unsup-loss is 0.06521935184507377.\n",
      "At step 882, the training (sup)loss is 0.004768095200970059, the training unsup-loss is 0.0652347764184027.\n",
      "At step 884, the training (sup)loss is 0.004757307655266507, the training unsup-loss is 0.0652388932480481.\n",
      "At step 886, the training (sup)loss is 0.004746568811800894, the training unsup-loss is 0.06527712953888704.\n",
      "At step 888, the training (sup)loss is 0.004735878341504045, the training unsup-loss is 0.06533342469085075.\n",
      "At step 890, the training (sup)loss is 0.00472523591826471, the training unsup-loss is 0.06533932559547967.\n",
      "At step 892, the training (sup)loss is 0.004714641218896404, the training unsup-loss is 0.06538112938608724.\n",
      "At step 894, the training (sup)loss is 0.004704093923104689, the training unsup-loss is 0.06549477798164191.\n",
      "At step 896, the training (sup)loss is 0.004693593713454902, the training unsup-loss is 0.06552468543876395.\n",
      "At step 898, the training (sup)loss is 0.004683140275340303, the training unsup-loss is 0.06557303926034236.\n",
      "At step 900, the training (sup)loss is 0.004672733296950658, the training unsup-loss is 0.06565022656590574.\n",
      "At step 902, the training (sup)loss is 0.004662372469241233, the training unsup-loss is 0.06568834597054878.\n",
      "At step 904, the training (sup)loss is 0.004652057485902204, the training unsup-loss is 0.06574547144201115.\n",
      "At step 906, the training (sup)loss is 0.004641788043328468, the training unsup-loss is 0.0658520041722352.\n",
      "At step 908, the training (sup)loss is 0.004631563840589859, the training unsup-loss is 0.06605942354594985.\n",
      "At step 910, the training (sup)loss is 0.00462138457940175, the training unsup-loss is 0.06616997788506714.\n",
      "At step 912, the training (sup)loss is 0.004611249964096044, the training unsup-loss is 0.06627363167609203.\n",
      "At step 914, the training (sup)loss is 0.004601159701592552, the training unsup-loss is 0.06642208594107425.\n",
      "At step 916, the training (sup)loss is 0.004591113501370734, the training unsup-loss is 0.06650354478235751.\n",
      "At step 918, the training (sup)loss is 0.004581111075441822, the training unsup-loss is 0.0666557392014154.\n",
      "At step 920, the training (sup)loss is 0.0045711521383212965, the training unsup-loss is 0.06664718182696758.\n",
      "At step 922, the training (sup)loss is 0.004561236407001727, the training unsup-loss is 0.06671028853032471.\n",
      "At step 924, the training (sup)loss is 0.004551363600925965, the training unsup-loss is 0.06678545040086524.\n",
      "At step 926, the training (sup)loss is 0.004541533441960683, the training unsup-loss is 0.06673318271064571.\n",
      "At step 928, the training (sup)loss is 0.00453174565437025, the training unsup-loss is 0.06670580865867468.\n",
      "At step 930, the training (sup)loss is 0.0045219999647909594, the training unsup-loss is 0.06671977337269533.\n",
      "At step 932, the training (sup)loss is 0.004512296102205571, the training unsup-loss is 0.06673843276990746.\n",
      "At step 934, the training (sup)loss is 0.004502633797918193, the training unsup-loss is 0.06675823955503501.\n",
      "At step 936, the training (sup)loss is 0.004625130150244277, the training unsup-loss is 0.06674270815928426.\n",
      "At step 938, the training (sup)loss is 0.004615268465488959, the training unsup-loss is 0.06670158984822663.\n",
      "At step 940, the training (sup)loss is 0.00460544874534962, the training unsup-loss is 0.06677958804499755.\n",
      "At step 942, the training (sup)loss is 0.004595670722535715, the training unsup-loss is 0.06678245037165322.\n",
      "At step 944, the training (sup)loss is 0.004585934132021867, the training unsup-loss is 0.06677897218160994.\n",
      "At step 946, the training (sup)loss is 0.004576238711023936, the training unsup-loss is 0.06677731637789791.\n",
      "At step 948, the training (sup)loss is 0.004566584198975362, the training unsup-loss is 0.06681336714092344.\n",
      "At step 950, the training (sup)loss is 0.004556970337503834, the training unsup-loss is 0.06691578520854052.\n",
      "At step 952, the training (sup)loss is 0.004547396870408238, the training unsup-loss is 0.06696734034765449.\n",
      "At step 954, the training (sup)loss is 0.004795598854121422, the training unsup-loss is 0.06700120744172414.\n",
      "At step 956, the training (sup)loss is 0.004915060327392243, the training unsup-loss is 0.06714288038240597.\n",
      "At step 958, the training (sup)loss is 0.004904799241113762, the training unsup-loss is 0.06714350775774716.\n",
      "At step 960, the training (sup)loss is 0.004894580909361442, the training unsup-loss is 0.06723407928851279.\n",
      "At step 962, the training (sup)loss is 0.004884405065475036, the training unsup-loss is 0.06733371653998747.\n",
      "At step 964, the training (sup)loss is 0.004874271445007245, the training unsup-loss is 0.0673672719076044.\n",
      "At step 966, the training (sup)loss is 0.004864179785700811, the training unsup-loss is 0.0674900468525633.\n",
      "At step 968, the training (sup)loss is 0.004854129827465893, the training unsup-loss is 0.06761545518293885.\n",
      "At step 970, the training (sup)loss is 0.004844121312357716, the training unsup-loss is 0.06772390549964051.\n",
      "At step 972, the training (sup)loss is 0.004834153984554511, the training unsup-loss is 0.06784810929620493.\n",
      "At step 974, the training (sup)loss is 0.004824227590335713, the training unsup-loss is 0.06796757844458401.\n",
      "At step 976, the training (sup)loss is 0.004814341878060435, the training unsup-loss is 0.06800493124402968.\n",
      "At step 978, the training (sup)loss is 0.004804496598146201, the training unsup-loss is 0.06803794923297497.\n",
      "At step 980, the training (sup)loss is 0.004794691503047943, the training unsup-loss is 0.06813241005495038.\n",
      "At step 982, the training (sup)loss is 0.004784926347237255, the training unsup-loss is 0.06805838331629478.\n",
      "At step 984, the training (sup)loss is 0.004775200887181894, the training unsup-loss is 0.068063596468911.\n",
      "At step 986, the training (sup)loss is 0.004765514881325542, the training unsup-loss is 0.06815313611417616.\n",
      "At step 988, the training (sup)loss is 0.004755868090067798, the training unsup-loss is 0.06814446821073561.\n",
      "At step 990, the training (sup)loss is 0.004746260275744429, the training unsup-loss is 0.06808706031020995.\n",
      "At step 992, the training (sup)loss is 0.004736691202607847, the training unsup-loss is 0.06808815103666406.\n",
      "At step 994, the training (sup)loss is 0.0047271606368078315, the training unsup-loss is 0.06807656521614738.\n",
      "At step 996, the training (sup)loss is 0.004860271052663106, the training unsup-loss is 0.06810822464192817.\n",
      "At step 998, the training (sup)loss is 0.004850531030513481, the training unsup-loss is 0.0681445989263186.\n",
      "At step 1000, the training (sup)loss is 0.004840829968452453, the training unsup-loss is 0.06823909746762365.\n",
      "At step 1002, the training (sup)loss is 0.004831167633186082, the training unsup-loss is 0.06827734147445735.\n",
      "At step 1004, the training (sup)loss is 0.0048215437932793365, the training unsup-loss is 0.06836672707468242.\n",
      "At step 1006, the training (sup)loss is 0.004811958219137628, the training unsup-loss is 0.0684941980023137.\n",
      "At step 1008, the training (sup)loss is 0.004802410682988545, the training unsup-loss is 0.06857639582853557.\n",
      "At step 1010, the training (sup)loss is 0.004792900958863816, the training unsup-loss is 0.06870417477526139.\n",
      "At step 1012, the training (sup)loss is 0.004783428822581476, the training unsup-loss is 0.068788815178382.\n",
      "At step 1014, the training (sup)loss is 0.004773994051728258, the training unsup-loss is 0.06885056406226472.\n",
      "At step 1016, the training (sup)loss is 0.004764596425642179, the training unsup-loss is 0.06890676097163388.\n",
      "At step 1018, the training (sup)loss is 0.0047552357253953376, the training unsup-loss is 0.06899526357295793.\n",
      "At step 1020, the training (sup)loss is 0.004745911733776915, the training unsup-loss is 0.06907922202520364.\n",
      "At step 1022, the training (sup)loss is 0.004736624235276373, the training unsup-loss is 0.06919481567291219.\n",
      "At step 1024, the training (sup)loss is 0.004727373016066849, the training unsup-loss is 0.06939659100953577.\n",
      "At step 1026, the training (sup)loss is 0.004718157863988747, the training unsup-loss is 0.06954111349741705.\n",
      "At step 1028, the training (sup)loss is 0.004708978568533515, the training unsup-loss is 0.06959643080044205.\n",
      "At step 1030, the training (sup)loss is 0.004699834920827625, the training unsup-loss is 0.06960548788435685.\n",
      "At step 1032, the training (sup)loss is 0.004690726713616719, the training unsup-loss is 0.06978831630355369.\n",
      "At step 1034, the training (sup)loss is 0.0046816537412499555, the training unsup-loss is 0.06991280756108967.\n",
      "At step 1036, the training (sup)loss is 0.0046726157996645305, the training unsup-loss is 0.0699991221318532.\n",
      "At step 1038, the training (sup)loss is 0.004663612686370379, the training unsup-loss is 0.07002785108778949.\n",
      "At step 1040, the training (sup)loss is 0.004654644200435052, the training unsup-loss is 0.0700664310886238.\n",
      "At step 1042, the training (sup)loss is 0.004645710142468766, the training unsup-loss is 0.07013486812942288.\n",
      "At step 1044, the training (sup)loss is 0.00463681031460963, the training unsup-loss is 0.0702303336867242.\n",
      "At step 1046, the training (sup)loss is 0.004627944520509038, the training unsup-loss is 0.07028011678776795.\n",
      "At step 1048, the training (sup)loss is 0.004619112565317227, the training unsup-loss is 0.07036715691450642.\n",
      "At step 1050, the training (sup)loss is 0.004610314255669003, the training unsup-loss is 0.0704288581607952.\n",
      "At step 1052, the training (sup)loss is 0.004601549399669632, the training unsup-loss is 0.07053829865802802.\n",
      "At step 1054, the training (sup)loss is 0.004592817806880886, the training unsup-loss is 0.07067384473551806.\n",
      "At step 1056, the training (sup)loss is 0.004584119288307248, the training unsup-loss is 0.0708122880111632.\n",
      "At step 1058, the training (sup)loss is 0.004575453656382281, the training unsup-loss is 0.07083316085268594.\n",
      "At step 1060, the training (sup)loss is 0.004566820724955145, the training unsup-loss is 0.0708396287942481.\n",
      "At step 1062, the training (sup)loss is 0.0045582203092772635, the training unsup-loss is 0.07090682513124386.\n",
      "At step 1064, the training (sup)loss is 0.004549652225989148, the training unsup-loss is 0.07102214626152124.\n",
      "At step 1066, the training (sup)loss is 0.004541116293107368, the training unsup-loss is 0.0711436658166349.\n",
      "At step 1068, the training (sup)loss is 0.0045326123300116605, the training unsup-loss is 0.07129548234631367.\n",
      "At step 1070, the training (sup)loss is 0.0045241401574321995, the training unsup-loss is 0.07139814099053217.\n",
      "At step 1072, the training (sup)loss is 0.00451569959743699, the training unsup-loss is 0.07150655793662031.\n",
      "At step 1074, the training (sup)loss is 0.004507290473419417, the training unsup-loss is 0.0716131312254074.\n",
      "At step 1076, the training (sup)loss is 0.0044989126100859235, the training unsup-loss is 0.07172379821978621.\n",
      "At step 1078, the training (sup)loss is 0.004490565833443834, the training unsup-loss is 0.07188212801659123.\n",
      "At step 1080, the training (sup)loss is 0.004482249970789309, the training unsup-loss is 0.07213061778133528.\n",
      "At step 1082, the training (sup)loss is 0.004473964850695429, the training unsup-loss is 0.07218014138928044.\n",
      "At step 1084, the training (sup)loss is 0.0044657103030004185, the training unsup-loss is 0.07227312081933324.\n",
      "At step 1086, the training (sup)loss is 0.004457486158795998, the training unsup-loss is 0.07227217813809686.\n",
      "At step 1088, the training (sup)loss is 0.004449292250415858, the training unsup-loss is 0.07228297903459273.\n",
      "At step 1090, the training (sup)loss is 0.00444112841142427, the training unsup-loss is 0.07231620225388932.\n",
      "At step 1092, the training (sup)loss is 0.004432994476604811, the training unsup-loss is 0.07235265609078219.\n",
      "At step 1094, the training (sup)loss is 0.004424890281949226, the training unsup-loss is 0.07240341198451858.\n",
      "At step 1096, the training (sup)loss is 0.0044168156646464, the training unsup-loss is 0.07241473080383029.\n",
      "At step 1098, the training (sup)loss is 0.004408770463071451, the training unsup-loss is 0.07246468646981107.\n",
      "At step 1100, the training (sup)loss is 0.004400754516774958, the training unsup-loss is 0.07251800536347384.\n",
      "At step 1102, the training (sup)loss is 0.004392767666472281, the training unsup-loss is 0.07267907670126068.\n",
      "At step 1104, the training (sup)loss is 0.00438480975403302, the training unsup-loss is 0.07266782734296758.\n",
      "At step 1106, the training (sup)loss is 0.004376880622470573, the training unsup-loss is 0.07273047046809984.\n",
      "At step 1108, the training (sup)loss is 0.004368980115931818, the training unsup-loss is 0.07283389854029709.\n",
      "At step 1110, the training (sup)loss is 0.004361108079686895, the training unsup-loss is 0.07289636162537579.\n",
      "At step 1112, the training (sup)loss is 0.004436752155905576, the training unsup-loss is 0.0729322738320694.\n",
      "At step 1114, the training (sup)loss is 0.004428786712178636, the training unsup-loss is 0.07294821676244721.\n",
      "At step 1116, the training (sup)loss is 0.004420849818429212, the training unsup-loss is 0.07300000866010492.\n",
      "At step 1118, the training (sup)loss is 0.004412941321437389, the training unsup-loss is 0.07312242274150946.\n",
      "At step 1120, the training (sup)loss is 0.004405061069077679, the training unsup-loss is 0.07314430690686484.\n",
      "At step 1122, the training (sup)loss is 0.0043972089103092695, the training unsup-loss is 0.0731867633306765.\n",
      "At step 1124, the training (sup)loss is 0.00438938469516637, the training unsup-loss is 0.07321630942211006.\n",
      "At step 1126, the training (sup)loss is 0.004381588274748669, the training unsup-loss is 0.07320956654803659.\n",
      "At step 1128, the training (sup)loss is 0.00437381950121188, the training unsup-loss is 0.07322740555387519.\n",
      "At step 1130, the training (sup)loss is 0.004366078227758407, the training unsup-loss is 0.07326348818822112.\n",
      "At step 1132, the training (sup)loss is 0.004358364308628093, the training unsup-loss is 0.073362154266616.\n",
      "At step 1134, the training (sup)loss is 0.0043506775990890655, the training unsup-loss is 0.07334930396095128.\n",
      "At step 1136, the training (sup)loss is 0.004343017955428697, the training unsup-loss is 0.07330820712336326.\n",
      "At step 1138, the training (sup)loss is 0.00433538523494464, the training unsup-loss is 0.07343068844859729.\n",
      "At step 1140, the training (sup)loss is 0.004327779295935965, the training unsup-loss is 0.07341242341667806.\n",
      "At step 1142, the training (sup)loss is 0.004320199997694396, the training unsup-loss is 0.07347770918555388.\n",
      "At step 1144, the training (sup)loss is 0.00431264720049563, the training unsup-loss is 0.07347483344464009.\n",
      "At step 1146, the training (sup)loss is 0.004396904493969772, the training unsup-loss is 0.07350019572536749.\n",
      "At step 1148, the training (sup)loss is 0.00438924438161094, the training unsup-loss is 0.07355013229066461.\n",
      "At step 1150, the training (sup)loss is 0.004381610913121182, the training unsup-loss is 0.07356129975665522.\n",
      "At step 1152, the training (sup)loss is 0.004374003949730347, the training unsup-loss is 0.07362028107794079.\n",
      "At step 1154, the training (sup)loss is 0.004366423353630294, the training unsup-loss is 0.07367727644596268.\n",
      "At step 1156, the training (sup)loss is 0.004358868987966574, the training unsup-loss is 0.07382234738846423.\n",
      "At step 1158, the training (sup)loss is 0.004351340716830189, the training unsup-loss is 0.07391176290974176.\n",
      "At step 1160, the training (sup)loss is 0.004343838405249448, the training unsup-loss is 0.07400689461541458.\n",
      "At step 1162, the training (sup)loss is 0.0043363619191818924, the training unsup-loss is 0.07403176025809678.\n",
      "At step 1164, the training (sup)loss is 0.004328911125506323, the training unsup-loss is 0.07403555414735426.\n",
      "At step 1166, the training (sup)loss is 0.004321485892014888, the training unsup-loss is 0.0740559998744744.\n",
      "At step 1168, the training (sup)loss is 0.004387985698062263, the training unsup-loss is 0.0740253183925333.\n",
      "At step 1170, the training (sup)loss is 0.004380484867809165, the training unsup-loss is 0.0740331052738823.\n",
      "At step 1172, the training (sup)loss is 0.00437300963765932, the training unsup-loss is 0.07407337006534301.\n",
      "At step 1174, the training (sup)loss is 0.004365559876777447, the training unsup-loss is 0.07407612529594382.\n",
      "At step 1176, the training (sup)loss is 0.004358135455218302, the training unsup-loss is 0.07416882819803368.\n",
      "At step 1178, the training (sup)loss is 0.00435073624391912, the training unsup-loss is 0.07419301640852678.\n",
      "At step 1180, the training (sup)loss is 0.004408807743151309, the training unsup-loss is 0.074195889603757.\n",
      "At step 1182, the training (sup)loss is 0.0044013478315723725, the training unsup-loss is 0.07425302490765345.\n",
      "At step 1184, the training (sup)loss is 0.00439391312239742, the training unsup-loss is 0.07426834520113.\n",
      "At step 1186, the training (sup)loss is 0.004386503488126935, the training unsup-loss is 0.07429622869677904.\n",
      "At step 1188, the training (sup)loss is 0.0044476698193485886, the training unsup-loss is 0.0743583813584049.\n",
      "At step 1190, the training (sup)loss is 0.004440194744021952, the training unsup-loss is 0.07443358246002252.\n",
      "At step 1192, the training (sup)loss is 0.004432744752840708, the training unsup-loss is 0.074443282549357.\n",
      "At step 1194, the training (sup)loss is 0.004425319719753872, the training unsup-loss is 0.07451968444386493.\n",
      "At step 1196, the training (sup)loss is 0.004417919519553615, the training unsup-loss is 0.07450551310955955.\n",
      "At step 1198, the training (sup)loss is 0.004410544027868217, the training unsup-loss is 0.07450008531876891.\n",
      "At step 1200, the training (sup)loss is 0.004473944169779619, the training unsup-loss is 0.07448876976889247.\n",
      "At step 1202, the training (sup)loss is 0.004466500003107773, the training unsup-loss is 0.07449888920770979.\n",
      "At step 1204, the training (sup)loss is 0.004549022801096653, the training unsup-loss is 0.07455641283047233.\n",
      "At step 1206, the training (sup)loss is 0.004541478816351883, the training unsup-loss is 0.0745135447895386.\n",
      "At step 1208, the training (sup)loss is 0.004533959811689048, the training unsup-loss is 0.07457339908813576.\n",
      "At step 1210, the training (sup)loss is 0.0045264656632399754, the training unsup-loss is 0.07467082444162777.\n",
      "At step 1212, the training (sup)loss is 0.004518996247954101, the training unsup-loss is 0.07474274754066254.\n",
      "At step 1214, the training (sup)loss is 0.004511551443591739, the training unsup-loss is 0.07475950175442132.\n",
      "At step 1216, the training (sup)loss is 0.00450413112871741, the training unsup-loss is 0.07483446836934156.\n",
      "At step 1218, the training (sup)loss is 0.004496735182693243, the training unsup-loss is 0.07487472844018016.\n",
      "At step 1220, the training (sup)loss is 0.004489363485672435, the training unsup-loss is 0.07485852635648774.\n",
      "At step 1222, the training (sup)loss is 0.00454924508026891, the training unsup-loss is 0.07497404094056856.\n",
      "At step 1224, the training (sup)loss is 0.004541811673275007, the training unsup-loss is 0.0749957975862986.\n",
      "At step 1226, the training (sup)loss is 0.00453440251883247, the training unsup-loss is 0.07506363029663146.\n",
      "At step 1228, the training (sup)loss is 0.004527017498443492, the training unsup-loss is 0.07511797678620473.\n",
      "At step 1230, the training (sup)loss is 0.004519656494380982, the training unsup-loss is 0.07515681891406818.\n",
      "At step 1232, the training (sup)loss is 0.0045123193896823115, the training unsup-loss is 0.07522987129466076.\n",
      "At step 1234, the training (sup)loss is 0.0045050060681431185, the training unsup-loss is 0.07523083531884914.\n",
      "At step 1236, the training (sup)loss is 0.004497716414311171, the training unsup-loss is 0.07526059326577124.\n",
      "At step 1238, the training (sup)loss is 0.004490450313480297, the training unsup-loss is 0.07536729462730649.\n",
      "At step 1240, the training (sup)loss is 0.004483207651684361, the training unsup-loss is 0.07539432911848229.\n",
      "At step 1242, the training (sup)loss is 0.004475988315691311, the training unsup-loss is 0.07541646488271833.\n",
      "At step 1244, the training (sup)loss is 0.004468792192997273, the training unsup-loss is 0.07542507444700028.\n",
      "At step 1246, the training (sup)loss is 0.004461619171820712, the training unsup-loss is 0.07552104328727621.\n",
      "At step 1248, the training (sup)loss is 0.004454469141096641, the training unsup-loss is 0.07558149032112664.\n",
      "At step 1250, the training (sup)loss is 0.004447341990470886, the training unsup-loss is 0.07564838183149696.\n",
      "At step 1252, the training (sup)loss is 0.004440237610294415, the training unsup-loss is 0.0756972778860789.\n",
      "At step 1254, the training (sup)loss is 0.00443315589161771, the training unsup-loss is 0.07573562891607245.\n",
      "At step 1256, the training (sup)loss is 0.004426096726185197, the training unsup-loss is 0.07583769065829196.\n",
      "At step 1258, the training (sup)loss is 0.004419060006429736, the training unsup-loss is 0.07591493207064053.\n",
      "At step 1260, the training (sup)loss is 0.004412045625467149, the training unsup-loss is 0.07599922762370653.\n",
      "At step 1262, the training (sup)loss is 0.004405053477090815, the training unsup-loss is 0.07607318236818758.\n",
      "At step 1264, the training (sup)loss is 0.004398083455766304, the training unsup-loss is 0.07612952747002032.\n",
      "At step 1266, the training (sup)loss is 0.004391135456626073, the training unsup-loss is 0.07618473737028326.\n",
      "At step 1268, the training (sup)loss is 0.004384209375464201, the training unsup-loss is 0.07630336552796656.\n",
      "At step 1270, the training (sup)loss is 0.004377305108731187, the training unsup-loss is 0.07635246555973578.\n",
      "At step 1272, the training (sup)loss is 0.0044411447883214595, the training unsup-loss is 0.07651037110950001.\n",
      "At step 1274, the training (sup)loss is 0.00443417281848108, the training unsup-loss is 0.07652962803358163.\n",
      "At step 1276, the training (sup)loss is 0.00442722270434553, the training unsup-loss is 0.07656490016468803.\n",
      "At step 1278, the training (sup)loss is 0.004469638314325485, the training unsup-loss is 0.07662093692061277.\n",
      "At step 1280, the training (sup)loss is 0.004462654504459351, the training unsup-loss is 0.07658982474458753.\n",
      "At step 1282, the training (sup)loss is 0.004455692484951614, the training unsup-loss is 0.07660504281099496.\n",
      "At step 1284, the training (sup)loss is 0.00444875215397817, the training unsup-loss is 0.07661706288896408.\n",
      "At step 1286, the training (sup)loss is 0.004488271732074477, the training unsup-loss is 0.07664054249446625.\n",
      "At step 1288, the training (sup)loss is 0.004481302366030882, the training unsup-loss is 0.07668463043183066.\n",
      "At step 1290, the training (sup)loss is 0.004474354610424633, the training unsup-loss is 0.07673013868936619.\n",
      "At step 1292, the training (sup)loss is 0.00446742836489766, the training unsup-loss is 0.07678915956634279.\n",
      "At step 1294, the training (sup)loss is 0.004460523529712347, the training unsup-loss is 0.07682248772880555.\n",
      "At step 1296, the training (sup)loss is 0.004453640005746742, the training unsup-loss is 0.07679165733825806.\n",
      "At step 1298, the training (sup)loss is 0.004446777694489813, the training unsup-loss is 0.07684356449688505.\n",
      "At step 1300, the training (sup)loss is 0.004439936498036751, the training unsup-loss is 0.07695840716003799.\n",
      "At step 1302, the training (sup)loss is 0.004433116319084314, the training unsup-loss is 0.07706158146614098.\n",
      "At step 1304, the training (sup)loss is 0.004426317060926209, the training unsup-loss is 0.07716824336225979.\n",
      "At step 1306, the training (sup)loss is 0.004419538627448528, the training unsup-loss is 0.07721031984531108.\n",
      "At step 1308, the training (sup)loss is 0.0044127809231252115, the training unsup-loss is 0.07721871384476463.\n",
      "At step 1310, the training (sup)loss is 0.0044060438530135705, the training unsup-loss is 0.07733316367691141.\n",
      "At step 1312, the training (sup)loss is 0.00439932732274983, the training unsup-loss is 0.07737840819328728.\n",
      "At step 1314, the training (sup)loss is 0.004392631238544731, the training unsup-loss is 0.07747492274053996.\n",
      "At step 1316, the training (sup)loss is 0.004385955507179162, the training unsup-loss is 0.07745744844991997.\n",
      "At step 1318, the training (sup)loss is 0.004379300035999831, the training unsup-loss is 0.0775929731571247.\n",
      "At step 1320, the training (sup)loss is 0.004372664732914982, the training unsup-loss is 0.0776756288589571.\n",
      "At step 1322, the training (sup)loss is 0.004366049506390149, the training unsup-loss is 0.07771000240079821.\n",
      "At step 1324, the training (sup)loss is 0.00435945426544394, the training unsup-loss is 0.07776542418929003.\n",
      "At step 1326, the training (sup)loss is 0.004352878919643874, the training unsup-loss is 0.07777592869823852.\n",
      "At step 1328, the training (sup)loss is 0.004346323379102242, the training unsup-loss is 0.07781321859107836.\n",
      "At step 1330, the training (sup)loss is 0.004339787554472013, the training unsup-loss is 0.07781088424958568.\n",
      "At step 1332, the training (sup)loss is 0.004333271356942775, the training unsup-loss is 0.07789397379453364.\n",
      "At step 1334, the training (sup)loss is 0.004326774698236714, the training unsup-loss is 0.0778828338898629.\n",
      "At step 1336, the training (sup)loss is 0.004320297490604624, the training unsup-loss is 0.07787187624943002.\n",
      "At step 1338, the training (sup)loss is 0.004352526474738513, the training unsup-loss is 0.07785763884637009.\n",
      "At step 1340, the training (sup)loss is 0.004346030166567262, the training unsup-loss is 0.07792183390623932.\n",
      "At step 1342, the training (sup)loss is 0.004339553221460604, the training unsup-loss is 0.07798940994486701.\n",
      "At step 1344, the training (sup)loss is 0.0043330955529762875, the training unsup-loss is 0.07804731681175153.\n",
      "At step 1346, the training (sup)loss is 0.004326657075185833, the training unsup-loss is 0.0780990308594861.\n",
      "At step 1348, the training (sup)loss is 0.0043202377026707196, the training unsup-loss is 0.07810841048293878.\n",
      "At step 1350, the training (sup)loss is 0.004313837350518615, the training unsup-loss is 0.07814817475706891.\n",
      "At step 1352, the training (sup)loss is 0.004307455934319623, the training unsup-loss is 0.07818356637647321.\n",
      "At step 1354, the training (sup)loss is 0.004301093370162578, the training unsup-loss is 0.07820014858293722.\n",
      "At step 1356, the training (sup)loss is 0.004294749574631365, the training unsup-loss is 0.07823853428437097.\n",
      "At step 1358, the training (sup)loss is 0.004288424464801275, the training unsup-loss is 0.07826295972010326.\n",
      "At step 1360, the training (sup)loss is 0.00428211795823539, the training unsup-loss is 0.07827928670175265.\n",
      "At step 1362, the training (sup)loss is 0.004275829972981007, the training unsup-loss is 0.07827980113632857.\n",
      "At step 1364, the training (sup)loss is 0.004269560427566078, the training unsup-loss is 0.07836820616364981.\n",
      "At step 1366, the training (sup)loss is 0.004263309240995703, the training unsup-loss is 0.07843406166742238.\n",
      "At step 1368, the training (sup)loss is 0.004291084142979126, the training unsup-loss is 0.07848488625441269.\n",
      "At step 1370, the training (sup)loss is 0.004284819786566018, the training unsup-loss is 0.07846342017874122.\n",
      "At step 1372, the training (sup)loss is 0.004278573693582685, the training unsup-loss is 0.07856147026482958.\n",
      "At step 1374, the training (sup)loss is 0.00427234578427616, the training unsup-loss is 0.07857155277064339.\n",
      "At step 1376, the training (sup)loss is 0.004266135979357154, the training unsup-loss is 0.07864994672600852.\n",
      "At step 1378, the training (sup)loss is 0.00429787554917384, the training unsup-loss is 0.07870723096199647.\n",
      "At step 1380, the training (sup)loss is 0.00429164674403011, the training unsup-loss is 0.07869517768301286.\n",
      "At step 1382, the training (sup)loss is 0.0042854359672659555, the training unsup-loss is 0.07876285419674044.\n",
      "At step 1384, the training (sup)loss is 0.004279243140723664, the training unsup-loss is 0.07879279699981363.\n",
      "At step 1386, the training (sup)loss is 0.004273068186696646, the training unsup-loss is 0.07885688440847341.\n",
      "At step 1388, the training (sup)loss is 0.00429920222803478, the training unsup-loss is 0.07887753636726536.\n",
      "At step 1390, the training (sup)loss is 0.004293016325548398, the training unsup-loss is 0.07892873078356866.\n",
      "At step 1392, the training (sup)loss is 0.004324122468523425, the training unsup-loss is 0.07900831015210534.\n",
      "At step 1394, the training (sup)loss is 0.004351442997775427, the training unsup-loss is 0.07909236652402467.\n",
      "At step 1396, the training (sup)loss is 0.004345208838752826, the training unsup-loss is 0.07913116121966189.\n",
      "At step 1398, the training (sup)loss is 0.004338992517095097, the training unsup-loss is 0.07921047593121855.\n",
      "At step 1400, the training (sup)loss is 0.004332793956356389, the training unsup-loss is 0.07926357890013605.\n",
      "At step 1402, the training (sup)loss is 0.004326613080527065, the training unsup-loss is 0.07933645956045529.\n",
      "At step 1404, the training (sup)loss is 0.004320449814030587, the training unsup-loss is 0.07938445763505911.\n",
      "At step 1406, the training (sup)loss is 0.004314304081720444, the training unsup-loss is 0.07938676114110634.\n",
      "At step 1408, the training (sup)loss is 0.004308175808877091, the training unsup-loss is 0.07940421329840319.\n",
      "At step 1410, the training (sup)loss is 0.004302064921204925, the training unsup-loss is 0.07942047033207954.\n",
      "At step 1412, the training (sup)loss is 0.0042959713448292814, the training unsup-loss is 0.07943194494292499.\n",
      "At step 1414, the training (sup)loss is 0.004289895006293455, the training unsup-loss is 0.07947184206506888.\n",
      "At step 1416, the training (sup)loss is 0.004283835832555752, the training unsup-loss is 0.07953928859764973.\n",
      "At step 1418, the training (sup)loss is 0.004277793750986562, the training unsup-loss is 0.07952195936840221.\n",
      "At step 1420, the training (sup)loss is 0.004300126291706528, the training unsup-loss is 0.07957423977016038.\n",
      "At step 1422, the training (sup)loss is 0.004294078294109192, the training unsup-loss is 0.07956675727656518.\n",
      "At step 1424, the training (sup)loss is 0.004288047285269151, the training unsup-loss is 0.07963008544436562.\n",
      "At step 1426, the training (sup)loss is 0.0042820331937049585, the training unsup-loss is 0.07962000884604216.\n",
      "At step 1428, the training (sup)loss is 0.004276035948335624, the training unsup-loss is 0.07963826370044645.\n",
      "At step 1430, the training (sup)loss is 0.004270055478477811, the training unsup-loss is 0.07971607140766887.\n",
      "At step 1432, the training (sup)loss is 0.004264091713843066, the training unsup-loss is 0.0797966441143481.\n",
      "At step 1434, the training (sup)loss is 0.004258144584535056, the training unsup-loss is 0.07980543582257263.\n",
      "At step 1436, the training (sup)loss is 0.004277294906359529, the training unsup-loss is 0.07985438744169154.\n",
      "At step 1438, the training (sup)loss is 0.004271345956559308, the training unsup-loss is 0.0798747355114508.\n",
      "At step 1440, the training (sup)loss is 0.004290140378806326, the training unsup-loss is 0.07985810694840943.\n",
      "At step 1442, the training (sup)loss is 0.004312344523206167, the training unsup-loss is 0.07984663150757655.\n",
      "At step 1444, the training (sup)loss is 0.0043063717468582365, the training unsup-loss is 0.07989910672614842.\n",
      "At step 1446, the training (sup)loss is 0.004324466239964978, the training unsup-loss is 0.0798947010569295.\n",
      "At step 1448, the training (sup)loss is 0.0043184932202965185, the training unsup-loss is 0.07995149203465521.\n",
      "At step 1450, the training (sup)loss is 0.004312536677923696, the training unsup-loss is 0.07997916162977445.\n",
      "At step 1452, the training (sup)loss is 0.004306596544758512, the training unsup-loss is 0.07999293110461664.\n",
      "At step 1454, the training (sup)loss is 0.004323766760251381, the training unsup-loss is 0.08003547499622468.\n",
      "At step 1456, the training (sup)loss is 0.004317827520196091, the training unsup-loss is 0.08007907267636875.\n",
      "At step 1458, the training (sup)loss is 0.004335818252691025, the training unsup-loss is 0.0800952134118639.\n",
      "At step 1460, the training (sup)loss is 0.004329878775632545, the training unsup-loss is 0.0801471916773701.\n",
      "At step 1462, the training (sup)loss is 0.0043465530604287145, the training unsup-loss is 0.0802699306313453.\n",
      "At step 1464, the training (sup)loss is 0.004340615146411735, the training unsup-loss is 0.08032475055584715.\n",
      "At step 1466, the training (sup)loss is 0.004334693434070109, the training unsup-loss is 0.08047055002202827.\n",
      "At step 1468, the training (sup)loss is 0.004328787857184456, the training unsup-loss is 0.08057175777606382.\n",
      "At step 1470, the training (sup)loss is 0.004322898349895769, the training unsup-loss is 0.08055310595691914.\n",
      "At step 1472, the training (sup)loss is 0.004317024846702976, the training unsup-loss is 0.08065361797533747.\n",
      "At step 1474, the training (sup)loss is 0.004311167282460502, the training unsup-loss is 0.08069723575809412.\n",
      "At step 1476, the training (sup)loss is 0.004305325592375868, the training unsup-loss is 0.08071472519203556.\n",
      "At step 1478, the training (sup)loss is 0.004299499712007294, the training unsup-loss is 0.08078377783810153.\n",
      "At step 1480, the training (sup)loss is 0.004293689577261338, the training unsup-loss is 0.08079135562607867.\n",
      "At step 1482, the training (sup)loss is 0.00428789512439054, the training unsup-loss is 0.08080024356349257.\n",
      "At step 1484, the training (sup)loss is 0.004282116289991092, the training unsup-loss is 0.08081323083808942.\n",
      "At step 1486, the training (sup)loss is 0.004276353011000525, the training unsup-loss is 0.08082245422331653.\n",
      "At step 1488, the training (sup)loss is 0.004270605224695417, the training unsup-loss is 0.08087334510186807.\n",
      "At step 1490, the training (sup)loss is 0.004264872868689115, the training unsup-loss is 0.0808790739994471.\n",
      "At step 1492, the training (sup)loss is 0.0042591558809294775, the training unsup-loss is 0.08094145571775535.\n",
      "At step 1494, the training (sup)loss is 0.0042534541996966406, the training unsup-loss is 0.08096225316283434.\n",
      "At step 1496, the training (sup)loss is 0.0042764278138703845, the training unsup-loss is 0.08101861930843243.\n",
      "At step 1498, the training (sup)loss is 0.004270718297429969, the training unsup-loss is 0.0810650154663491.\n",
      "At step 1500, the training (sup)loss is 0.004301107931882143, the training unsup-loss is 0.0811366859767586.\n",
      "At step 1502, the training (sup)loss is 0.004295380757538758, the training unsup-loss is 0.081185481077638.\n",
      "At step 1504, the training (sup)loss is 0.004289668815042031, the training unsup-loss is 0.0811959408177806.\n",
      "At step 1506, the training (sup)loss is 0.004308997965102414, the training unsup-loss is 0.0811970169101885.\n",
      "At step 1508, the training (sup)loss is 0.004303283113689812, the training unsup-loss is 0.08117308963117355.\n",
      "At step 1510, the training (sup)loss is 0.0042975834009564475, the training unsup-loss is 0.08120567833585356.\n",
      "At step 1512, the training (sup)loss is 0.004291898766828198, the training unsup-loss is 0.08125993285128581.\n",
      "At step 1514, the training (sup)loss is 0.004286229151548373, the training unsup-loss is 0.08129567510658774.\n",
      "At step 1516, the training (sup)loss is 0.00429825486964239, the training unsup-loss is 0.08135301322862998.\n",
      "At step 1518, the training (sup)loss is 0.004323907562179371, the training unsup-loss is 0.08142477885435909.\n",
      "At step 1520, the training (sup)loss is 0.004338364838622511, the training unsup-loss is 0.08149306053038392.\n",
      "At step 1522, the training (sup)loss is 0.004347686107550673, the training unsup-loss is 0.08158828623032346.\n",
      "At step 1524, the training (sup)loss is 0.004341980482737614, the training unsup-loss is 0.08159376876343169.\n",
      "At step 1526, the training (sup)loss is 0.0043538412455161775, the training unsup-loss is 0.08161462964441232.\n",
      "At step 1528, the training (sup)loss is 0.004368206725031605, the training unsup-loss is 0.0816720000398711.\n",
      "At step 1530, the training (sup)loss is 0.004380240869629228, the training unsup-loss is 0.08176010403891697.\n",
      "At step 1532, the training (sup)loss is 0.0043745225395122175, the training unsup-loss is 0.08186500692586889.\n",
      "At step 1534, the training (sup)loss is 0.004368819120295122, the training unsup-loss is 0.08194584371230651.\n",
      "At step 1536, the training (sup)loss is 0.004380274936314284, the training unsup-loss is 0.08204580962228647.\n",
      "At step 1538, the training (sup)loss is 0.004374578870077204, the training unsup-loss is 0.08207857061325244.\n",
      "At step 1540, the training (sup)loss is 0.004368897598817364, the training unsup-loss is 0.08219367869428129.\n",
      "At step 1542, the training (sup)loss is 0.004363231064966758, the training unsup-loss is 0.08221389075388454.\n",
      "At step 1544, the training (sup)loss is 0.004357579211255661, the training unsup-loss is 0.08233286352005396.\n",
      "At step 1546, the training (sup)loss is 0.004365185974467001, the training unsup-loss is 0.08238403933763716.\n",
      "At step 1548, the training (sup)loss is 0.004374312494402554, the training unsup-loss is 0.08242342573378802.\n",
      "At step 1550, the training (sup)loss is 0.004382875206009034, the training unsup-loss is 0.08246648079385199.\n",
      "At step 1552, the training (sup)loss is 0.004390294052727674, the training unsup-loss is 0.08255311629204463.\n",
      "At step 1554, the training (sup)loss is 0.004398871108918346, the training unsup-loss is 0.08261246231241941.\n",
      "At step 1556, the training (sup)loss is 0.0044061658822555465, the training unsup-loss is 0.08265258341125072.\n",
      "At step 1558, the training (sup)loss is 0.0044135466595784075, the training unsup-loss is 0.08267082922359084.\n",
      "At step 1560, the training (sup)loss is 0.004407888266425102, the training unsup-loss is 0.08272596454809014.\n",
      "At step 1562, the training (sup)loss is 0.0044133069682937886, the training unsup-loss is 0.0827359928267742.\n",
      "At step 1564, the training (sup)loss is 0.004418696867554542, the training unsup-loss is 0.0827919687198051.\n",
      "At step 1566, the training (sup)loss is 0.004426651320266769, the training unsup-loss is 0.08279089700242702.\n",
      "At step 1568, the training (sup)loss is 0.004431536771613649, the training unsup-loss is 0.0828457391002377.\n",
      "At step 1570, the training (sup)loss is 0.004425891501840892, the training unsup-loss is 0.08289540220396059.\n",
      "At step 1572, the training (sup)loss is 0.004420260596622265, the training unsup-loss is 0.08295923360248994.\n",
      "At step 1574, the training (sup)loss is 0.00441464400120089, the training unsup-loss is 0.08313940036296372.\n",
      "At step 1576, the training (sup)loss is 0.004409041661097843, the training unsup-loss is 0.08316082738725195.\n",
      "At step 1578, the training (sup)loss is 0.004403453522110393, the training unsup-loss is 0.08322357282181789.\n",
      "At step 1580, the training (sup)loss is 0.0043978795303102535, the training unsup-loss is 0.08334304279812813.\n",
      "At step 1582, the training (sup)loss is 0.004401626418593108, the training unsup-loss is 0.08340056587832446.\n",
      "At step 1584, the training (sup)loss is 0.004396068809478722, the training unsup-loss is 0.08345710524919941.\n",
      "At step 1586, the training (sup)loss is 0.004390525217032973, the training unsup-loss is 0.08353790599197469.\n",
      "At step 1588, the training (sup)loss is 0.00439289179266963, the training unsup-loss is 0.08358124928532511.\n",
      "At step 1590, the training (sup)loss is 0.004396524067686976, the training unsup-loss is 0.08365110471176933.\n",
      "At step 1592, the training (sup)loss is 0.004391000796245158, the training unsup-loss is 0.08374073295257331.\n",
      "At step 1594, the training (sup)loss is 0.004385491384957523, the training unsup-loss is 0.08385379972997375.\n",
      "At step 1596, the training (sup)loss is 0.0043879251327262935, the training unsup-loss is 0.08396012196609083.\n",
      "At step 1598, the training (sup)loss is 0.004389202365598108, the training unsup-loss is 0.08398848962917625.\n",
      "At step 1600, the training (sup)loss is 0.004383715862641111, the training unsup-loss is 0.08405937828763854.\n",
      "At step 1602, the training (sup)loss is 0.004378243058817589, the training unsup-loss is 0.08409636969225813.\n",
      "At step 1604, the training (sup)loss is 0.004372783902883901, the training unsup-loss is 0.08413713739109585.\n",
      "At step 1606, the training (sup)loss is 0.004375783859950445, the training unsup-loss is 0.08418325707026447.\n",
      "At step 1608, the training (sup)loss is 0.004376109797780898, the training unsup-loss is 0.08421633806976198.\n",
      "At step 1610, the training (sup)loss is 0.004370673636541419, the training unsup-loss is 0.08433319250792727.\n",
      "At step 1612, the training (sup)loss is 0.004377207422984889, the training unsup-loss is 0.08442633118533596.\n",
      "At step 1614, the training (sup)loss is 0.004379709655721355, the training unsup-loss is 0.08444830306887609.\n",
      "At step 1616, the training (sup)loss is 0.004379964509683818, the training unsup-loss is 0.08458007123992124.\n",
      "At step 1618, the training (sup)loss is 0.004380194714341234, the training unsup-loss is 0.08464966369194193.\n",
      "At step 1620, the training (sup)loss is 0.004385419968044224, the training unsup-loss is 0.08477638245353268.\n",
      "At step 1622, the training (sup)loss is 0.004389529973704887, the training unsup-loss is 0.08483839857893848.\n",
      "At step 1624, the training (sup)loss is 0.004389757166612031, the training unsup-loss is 0.08491035301887472.\n",
      "At step 1626, the training (sup)loss is 0.004394341682898382, the training unsup-loss is 0.08498793132179319.\n",
      "At step 1628, the training (sup)loss is 0.004393392688702306, the training unsup-loss is 0.08504842075002117.\n",
      "At step 1630, the training (sup)loss is 0.004397153379852695, the training unsup-loss is 0.0850762174921853.\n",
      "At step 1632, the training (sup)loss is 0.004399906562848985, the training unsup-loss is 0.08520163581238639.\n",
      "At step 1634, the training (sup)loss is 0.004401762777143768, the training unsup-loss is 0.08530607898155343.\n",
      "At step 1636, the training (sup)loss is 0.004404588989300931, the training unsup-loss is 0.08537806485538214.\n",
      "At step 1638, the training (sup)loss is 0.004407083969028787, the training unsup-loss is 0.08546122209645435.\n",
      "At step 1640, the training (sup)loss is 0.004407658274342283, the training unsup-loss is 0.08563046571401108.\n",
      "At step 1642, the training (sup)loss is 0.004408345541300821, the training unsup-loss is 0.08573857672800662.\n",
      "At step 1644, the training (sup)loss is 0.004410119441172466, the training unsup-loss is 0.08579430639134253.\n",
      "At step 1646, the training (sup)loss is 0.004410686215243211, the training unsup-loss is 0.08583404878063867.\n",
      "At step 1648, the training (sup)loss is 0.004410879821539178, the training unsup-loss is 0.08590502630256894.\n",
      "At step 1650, the training (sup)loss is 0.004411360466514121, the training unsup-loss is 0.08603952097339612.\n",
      "At step 1652, the training (sup)loss is 0.004411077408174156, the training unsup-loss is 0.08610991867694363.\n",
      "At step 1654, the training (sup)loss is 0.004411128356359097, the training unsup-loss is 0.08626427260163311.\n",
      "At step 1656, the training (sup)loss is 0.0044107449672337855, the training unsup-loss is 0.08636641043636516.\n",
      "At step 1658, the training (sup)loss is 0.004411803449496213, the training unsup-loss is 0.08643347663643115.\n",
      "At step 1660, the training (sup)loss is 0.0044121628716947085, the training unsup-loss is 0.08648353668030756.\n",
      "At step 1662, the training (sup)loss is 0.004411939565186466, the training unsup-loss is 0.08650900149341363.\n",
      "At step 1664, the training (sup)loss is 0.004411628403781483, the training unsup-loss is 0.08654519966792297.\n",
      "At step 1666, the training (sup)loss is 0.004411535611765242, the training unsup-loss is 0.08658289354985484.\n",
      "For epoch 2, the mean sup loss is: 0.0044110180409519025, and accuracy is: 1.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|| 2/2 [32:24<00:00, 972.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.840399980545044.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Start Training\n",
    "trainer = TrainerUDA(model, datamodeler, loss_dict, optimizer, report_freq=2, device=DEVICE)\n",
    "trainer.train(2, schedule_type = SchedulerType.INVERSE_SQRT, save_model_freq=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble test dataloader\n",
    "test_data = torch.load(os.path.join(PATH, f'data/{DATASET_NAME}/test_data.pt'))\n",
    "test_data = TensorDataset(\n",
    "    test_data.tensors[0],\n",
    "    test_data.tensors[1],\n",
    "    test_data.tensors[2]\n",
    ")\n",
    "test_dataloder = DataLoader(test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator = Evaluator(loss_sup, test_dataloder, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8436499834060669.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_evaluator.run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in model.named_parameters():\n",
    "    if each[1].requires_grad:\n",
    "        print(f'{each[0]} : {each[1].requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunli/anaconda3/envs/ML Algo/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').to(torch.device('mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.encode(['asdf', 'asdf'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.76334281e-02, -4.01788950e-02, -7.13511631e-02,  4.50336933e-02,\n",
       "        1.61476061e-02, -4.87910211e-02,  8.94101784e-02,  3.41040939e-02,\n",
       "       -2.44780928e-02, -5.21589257e-02,  3.36785540e-02, -6.02451935e-02,\n",
       "       -2.48572119e-02,  4.25534463e-03, -2.88347546e-02, -5.18903993e-02,\n",
       "        1.29055418e-02, -1.11941166e-01, -7.36576989e-02,  2.17653196e-02,\n",
       "       -5.77138327e-02,  3.37865651e-02,  6.34062812e-02, -4.78094025e-03,\n",
       "        2.23415568e-02, -2.57172510e-02, -5.82065880e-02, -3.65310088e-02,\n",
       "        1.24478126e-02, -1.53364465e-01,  7.69489110e-02,  5.37916534e-02,\n",
       "        6.88013136e-02,  3.24284062e-02, -2.96538621e-02,  2.27979664e-02,\n",
       "       -3.80658992e-02,  9.05014388e-03,  1.19353570e-02,  4.71523553e-02,\n",
       "       -5.87766767e-02, -9.66688469e-02,  8.36317707e-03, -2.60597691e-02,\n",
       "       -1.64373312e-02, -1.97897255e-02, -3.57994549e-02, -3.85996960e-02,\n",
       "        1.14329785e-01,  5.42615354e-02, -1.44849066e-02,  6.67404532e-02,\n",
       "       -7.33617917e-02, -3.58984284e-02, -2.46705841e-02, -8.24545883e-03,\n",
       "        1.18252542e-03, -7.49036819e-02, -6.30668402e-02, -3.66596901e-03,\n",
       "       -7.77250202e-03, -1.04598841e-02, -7.85366148e-02,  6.11487962e-02,\n",
       "        3.83357219e-02,  5.18945530e-02,  5.23003808e-04, -1.65152140e-02,\n",
       "        9.28216707e-03,  2.72326004e-02, -5.62373400e-02,  5.98575994e-02,\n",
       "        3.84824052e-02,  1.37033863e-02,  1.95686687e-02,  4.70137373e-02,\n",
       "        3.18750963e-02, -4.18895483e-02,  8.88173133e-02, -3.18489484e-02,\n",
       "        7.03925267e-02,  5.95750026e-02,  2.21595820e-02, -2.98819076e-02,\n",
       "        9.53139563e-04, -7.53159728e-03,  4.10305858e-02, -1.93817187e-02,\n",
       "       -4.19703647e-02,  8.44822824e-02, -2.91627571e-02,  5.25649190e-02,\n",
       "        9.65889543e-03,  6.26377622e-03, -9.03164968e-02, -7.86763208e-04,\n",
       "        1.18117765e-01, -4.00057212e-02,  2.51871496e-02,  1.82843715e-01,\n",
       "       -2.62559094e-02,  1.75951719e-02, -1.69244390e-02,  3.58150131e-03,\n",
       "        1.20431336e-03, -7.59788677e-02, -6.17013797e-02, -4.05850708e-02,\n",
       "        2.25872882e-02,  1.32401742e-03, -3.23000140e-02, -5.77264465e-02,\n",
       "       -8.38354602e-02, -6.36535510e-02, -3.06460075e-04,  2.39475835e-02,\n",
       "       -6.87127188e-02,  4.94073704e-03,  2.36150082e-02, -3.08293402e-02,\n",
       "       -1.31919328e-02,  1.66226143e-03, -5.27716205e-02,  5.07040396e-02,\n",
       "       -1.02245715e-02,  2.74733640e-02, -2.88649625e-03, -2.88918750e-33,\n",
       "       -2.12930422e-02,  7.86580071e-02,  3.11430860e-02, -6.33490458e-02,\n",
       "        1.72451381e-02, -3.26116830e-02, -8.10256635e-04, -1.85531918e-02,\n",
       "       -3.06927785e-02, -5.16022369e-02,  1.21001489e-02,  4.94879074e-02,\n",
       "       -4.69108075e-02, -6.05783146e-03,  4.39455472e-02, -8.23132992e-02,\n",
       "        4.87848483e-02,  4.53266539e-02, -2.23267898e-02, -3.01487744e-02,\n",
       "        2.61516869e-02,  4.53454480e-02,  1.34119298e-03,  3.98683511e-02,\n",
       "       -2.92406566e-02, -5.94983175e-02,  1.61678307e-02,  2.91787237e-02,\n",
       "        3.30375507e-02,  4.20109183e-02,  9.19766817e-03, -2.41756421e-02,\n",
       "       -2.77500451e-02, -5.97292259e-02,  3.57434712e-02, -5.33570200e-02,\n",
       "        3.79967061e-03, -1.15866341e-01, -3.43456753e-02, -5.61664924e-02,\n",
       "       -5.79463840e-02,  2.98151672e-02,  4.69694939e-03,  3.21670435e-02,\n",
       "       -6.79642335e-02,  1.52960634e-02,  3.28944996e-02, -3.03368364e-02,\n",
       "        5.23776710e-02,  3.91681679e-02, -7.48162493e-02, -1.36280227e-02,\n",
       "       -8.51683877e-03, -1.20222904e-02,  3.20312455e-02, -1.85677893e-02,\n",
       "       -2.32875515e-02, -2.41694171e-02,  4.30339761e-02,  9.70206633e-02,\n",
       "        6.23269677e-02, -1.68522783e-02, -4.88500409e-02, -1.10379923e-02,\n",
       "       -4.75852229e-02, -4.24372628e-02,  8.12296048e-02,  6.34292373e-04,\n",
       "        6.06496856e-02, -9.00798589e-02, -6.16793819e-02,  1.11895502e-02,\n",
       "        1.30804360e-01,  2.40149442e-03, -1.66160967e-02,  6.10160567e-02,\n",
       "       -1.98975541e-02,  3.73504348e-02,  1.59893613e-02, -4.60575707e-02,\n",
       "       -1.45254448e-01,  3.71329561e-02, -4.34755199e-02,  2.83520855e-02,\n",
       "        3.78932916e-02,  2.63934955e-03,  5.55793159e-02, -3.18178311e-02,\n",
       "        1.34664157e-03, -6.08819425e-02, -7.20447823e-02, -7.74393156e-02,\n",
       "        4.26362529e-02, -8.15832242e-02,  2.27298401e-02,  1.11976049e-33,\n",
       "       -7.27115199e-02, -2.70923022e-02, -5.84720597e-02,  6.00168034e-02,\n",
       "        4.80067655e-02,  7.72483647e-02,  1.13433458e-01,  9.21229348e-02,\n",
       "       -4.64288630e-02,  4.68763113e-02,  1.59652457e-02,  9.91837904e-02,\n",
       "        1.50097478e-02, -4.59331460e-02,  3.88580114e-02, -3.58401909e-02,\n",
       "        1.31714880e-01,  5.06434180e-02,  1.62039772e-02,  2.66544633e-02,\n",
       "        3.58741358e-02,  1.30838742e-02, -2.36942447e-04,  8.49396735e-02,\n",
       "        1.14128338e-02,  6.89172298e-02,  2.43668593e-02, -2.59091295e-02,\n",
       "       -1.53219938e-01,  4.03913185e-02,  1.01524919e-01,  4.22509387e-02,\n",
       "       -3.03309504e-02,  5.54581322e-02, -4.64858534e-03, -3.56645919e-02,\n",
       "        5.80418296e-02,  1.55594507e-02, -1.27285510e-01, -3.35095897e-02,\n",
       "        4.97484617e-02,  3.49158905e-02,  1.02859316e-02,  2.85600536e-02,\n",
       "       -7.14214332e-03, -1.91261992e-02, -8.29252303e-02,  4.85694446e-02,\n",
       "        1.48302361e-01,  9.90626682e-03, -7.37567768e-02,  1.47132715e-02,\n",
       "       -1.69009157e-02, -3.41276675e-02, -6.31474629e-02, -3.82963638e-03,\n",
       "        7.07159787e-02, -4.00310643e-02, -3.76702696e-02,  4.45828326e-02,\n",
       "       -1.01332273e-02, -5.63298725e-03,  1.47530055e-02,  2.43507209e-03,\n",
       "       -2.44359002e-02, -4.70070764e-02, -2.97210515e-02, -6.33151010e-02,\n",
       "        6.31156098e-03, -2.03171540e-02,  1.90467797e-02,  7.60836247e-03,\n",
       "       -2.51981467e-02,  6.94333436e-03, -2.05165800e-02, -1.01691805e-01,\n",
       "       -3.91574018e-03,  1.42708691e-02, -2.29378585e-02,  6.38321415e-02,\n",
       "       -4.07550856e-02, -2.81243324e-02, -2.00394094e-02,  6.34590313e-02,\n",
       "       -2.40301993e-02, -5.54335229e-02,  1.02847323e-01, -1.52475359e-02,\n",
       "       -6.46785721e-02,  4.19331249e-03, -1.85946859e-02,  1.94756147e-02,\n",
       "        1.10970009e-02,  7.80489743e-02,  4.48446348e-02, -1.43427643e-08,\n",
       "       -5.45064313e-03,  8.60583857e-02,  1.28314197e-01, -5.12098595e-02,\n",
       "        9.58329067e-02,  5.00173047e-02, -4.20291051e-02, -7.44064078e-02,\n",
       "       -1.11022089e-02, -3.03444155e-02,  2.80454638e-04, -3.76816280e-02,\n",
       "        6.19324297e-02,  3.37122157e-02,  1.29424753e-02, -4.23733704e-02,\n",
       "       -2.12623123e-02,  8.05753320e-02, -3.00602987e-02, -1.84156504e-02,\n",
       "       -2.37076860e-02,  1.31404763e-02, -2.46943142e-02,  1.96314957e-02,\n",
       "        1.80863496e-03,  6.60807565e-02,  2.40664221e-02,  7.79147297e-02,\n",
       "        8.05415362e-02,  1.95280463e-02,  6.22306811e-03,  2.41707657e-02,\n",
       "        1.60235132e-03,  1.68060437e-02, -3.62939830e-03, -1.89795271e-02,\n",
       "        5.60260713e-02, -1.63256228e-02, -2.23115627e-02,  1.63672164e-01,\n",
       "        7.76675995e-03, -5.20603545e-02,  4.82051298e-02, -1.88104510e-02,\n",
       "       -6.29775878e-03,  1.05641805e-01,  2.07918580e-03,  1.61278844e-02,\n",
       "       -4.93252240e-02, -2.32344959e-03,  5.47921751e-04, -2.02208702e-02,\n",
       "       -4.98713404e-02,  4.38915379e-02,  2.88259741e-02,  1.38928033e-02,\n",
       "       -5.08415103e-02,  4.86970600e-03, -6.63787723e-02,  3.11264284e-02,\n",
       "        8.95943567e-02, -7.57562667e-02,  3.10762506e-02,  7.35144839e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(axis=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
